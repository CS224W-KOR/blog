<h1 id="lecture-173">Lecture 17.3</h1>

<h2 id="lecture-173---cluster-gcn-scaling-up-gnns"><strong>**Lecture 17.3 - Cluster GCN: Scaling up GNNs</strong>**</h2>

<p><strong>Issues with Neighbor Sampling</strong></p>

<p>17.2에서 말했듯이 Neighbor Sampling 방식으로 mini-batch를 구성하는데는 여러 문제점이 있습니다.</p>

<ul>
  <li><strong>Computation graph의 크기가 GNN layer의 갯수에 대해 지수적으로 증가</strong>하여 계산 효율성이 크게 떨어지게 됩니다.</li>
  <li><strong>샘플링 된 이웃 노드들이 많이 겹치는 경우, redundant한 계산이 많이 일어나기 때문에 효율적이지 않습니다.</strong> 아래 그림에서 보다시피 노드 C와 D는 두 computation graph에 모두 포함되어 여러번 불필요하게 연산에 참여하게 됩니다.</li>
</ul>

<p><img src="Lecture%2017%203%2009498e1e5d1a4c63959b85feb5b36a52/Untitled.png" alt="Untitled" /></p>

<p><strong>Insight from Full-batch GNN</strong></p>

<p>이러한 비효율성을 어떻게 해결하면 좋을까요? 해답은 full-batch GNN에서 얻을 수 있었습니다. 다시 full-batch를 떠올려보죠.</p>

<p><img src="Lecture%2017%203%2009498e1e5d1a4c63959b85feb5b36a52/Untitled%201.png" alt="Untitled" /></p>

<p>Full-batch training으로 학습할 때, 한 GNN layer에서 위 식을 통해 모든 노드의 임베딩을 동시에 업데이트 합니다. <strong>일괄적으로 $l-1$번째 layer의 노드 임베딩을 한번씩만 사용하여 $l$번째 layer의 노드 임베딩을 생성하기 때문에 Neighbor Sampling에서처럼 redundant한 연산은 일어나지 않게 됩니다.</strong> 이를 더 자세히 들여다 보면 아래와 같이 동작하게 됩니다.</p>

<p><img src="Lecture%2017%203%2009498e1e5d1a4c63959b85feb5b36a52/Untitled%202.png" alt="Untitled" /></p>

<ul>
  <li>하나의 GNN layer을 기준으로 computation graph를 만들고 자시고 할 것 없이 전체 그래프에 대해 일괄적으로 노드 임베딩을 생성합니다. 이 때 모든 엣지에 대해 양방향으로 두 번의 message passing이 일어납니다.
→ <strong>시간 복잡도 $\propto O(2*#(edges))$</strong></li>
  <li>$K$-layer GNN을 기준으로 하면,
→ <strong>시간 복잡도</strong> $\propto O(2K*#(edges))$</li>
</ul>

<p>따라서 full-batch training의 시간 복잡도는 엣지의 갯수와 GNN layer의 갯수에 선형 비례하기 때문에 이론적으론 굉장히 빨라야 합니다. 하지만, 우리가 다루는 그래프가 큰 그래프인만큼 엣지의 수가 굉~장히 크고 전체 그래프를 GPU 메모리 상에 로딩하는 것이 불가능하기 때문에 full-batch training을 사용할 수 없는 것이었죠.</p>

<p><strong>Subgraph Sampling</strong></p>

<p>그렇다면 <strong>전체 그래프 말고 더 작은 subgraph를 추출하여 subgraph에 대해 full-batch training을 적용하면 어떨까요?</strong> 즉, 이 아이디어는 곧 <strong>mini-batch를 전체 그래프에서 추출한 subgraph로 구성</strong>하여 SGD training을 하면 보다 계산 redundancy를 줄이고, 더 효율적으로 큰 그래프를 학습할 수 있다는 말과 같습니다. 그러려면 아래 그림과 같이 전체 그래프로부터 GPU에 올려질 수 있을 법한 작은 subgraph들을 잘 추출해내야 합니다.</p>

<p><img src="Lecture%2017%203%2009498e1e5d1a4c63959b85feb5b36a52/Untitled%203.png" alt="Untitled" /></p>

<aside>
💡 **Subgraph로 mini-batch를 구성하자!**

</aside>

<p>그럼 여기서, 어떻게 전체 그래프를 잘 설명할 수 있는 subgraph들을 추출할 수 있을까요? Full-batch training에서는 엣지를 따라 일어나는 message passing을 통해 노드 임베딩을 업데이트하기 때문에, <strong>좋은 subgraph란 전체 그래프의 엣지 연결 구조를 잘 드러내는 모양새여야 할 것입니다.</strong></p>

<p><img src="Lecture%2017%203%2009498e1e5d1a4c63959b85feb5b36a52/Untitled%204.png" alt="Untitled" /></p>

<p>위 그림의 왼쪽, 오른쪽 subgraph 중에 어떤 subgraph가 더 좋을까요? 왼쪽 subgraph는 원본 그래프의 커뮤니티 구조를 잘 유지하고 있어서 좋은 subgraph가 될 수 있는 반면, 오른쪽 subgraph는 원본 그래프의 중요한 엣지들을 많이 버린 구조기 때문에 안 좋은 subgraph라 볼 수 있습니다. 특히나 오른쪽 subgraph에서 생긴 고립된 노드들은 어떠한 엣지로도 연결 되어 있지 않기 때문에 노드 임베딩을 제대로 만들 수 없을 것입니다.</p>

<p><strong>Exploiting Community Structure</strong></p>

<p><img src="Lecture%2017%203%2009498e1e5d1a4c63959b85feb5b36a52/Untitled%205.png" alt="Untitled" /></p>

<p>13장에서 설명하였듯이 실생활에 쓰이는 대부분의 그래프는 위와 같이 커뮤니티 구조를 가집니다. 따라서 좋은 subgraph로 mini-batch를 구성하기 위해 <strong>원본 그래프의 국소적 엣지 연결 구조를 잘 보존하고 있는 커뮤니티가 subgraph로 사용되는 것이 좋아 보입니다.</strong></p>

<aside>
💡 **원본 그래프의 커뮤니티 구조를 subgraph로 사용하자!**

</aside>

<p><strong>Cluster-GCN</strong></p>

<p>Cluster-GCN은 mini-batch로 subgraph를 쓰자는 아이디어로부터 제안되었으며 두 단계를 통해 학습이 수행됩니다.</p>

<p><img src="Lecture%2017%203%2009498e1e5d1a4c63959b85feb5b36a52/Untitled%206.png" alt="Untitled" /></p>

<ol>
  <li><strong>Pre-processing</strong></li>
</ol>

<p><img src="Lecture%2017%203%2009498e1e5d1a4c63959b85feb5b36a52/Untitled%207.png" alt="Untitled" /></p>

<p>원본의 큰 그래프 $G$가 주어졌을 때, 아무 scalable한 community detection 알고리즘 (e.g., Louvain, METIS, BigCLAM 등)을 사용해서 위 그림과 같이 $<strong>G_i$로 커뮤니티를 분리해냅니다</strong>. 이 때, 각 커뮤니티 $G_i$가 subgraph로 사용될 것이기 때문에 커뮤니티 내부의 엣지는 보존하되 커뮤니티 사이의 엣지는 버립니다. 여기서 subgraph를 랜덤하게 뽑아서 mini-batch로 사용할 것이기 때문에 각 subgraph는 GPU에 통째로 올릴 수 있을 크기여야겠죠?</p>

<ol>
  <li><strong>Mini-batch training</strong></li>
</ol>

<p><img src="Lecture%2017%203%2009498e1e5d1a4c63959b85feb5b36a52/Untitled%208.png" alt="Untitled" /></p>

<p><strong>여러 subgraph 중 랜덤하게 뽑은 하나 (편의상 $G_c$라고 하겠습니다!) 를 mini-batch로 사용합니다.</strong> 이제 이 온전한 그래프에 대하여 <strong>full-batch training을 통해 노드 임베딩을 생성</strong>하면 됩니다. 이후로는 동일하게 downstream task에 걸맞는 mini-batch의 loss 값이 계산되고, 역전파를 통해 한번 GNN 모델의 파라미터가 업데이트 되겠네요. 계속 full-batch training의 계산상 효율성 때문에 Cluster-GCN이 제안되었다면서 교안에는 mini-batch training이라는 용어가 쓰여서 혹여나 헷갈리실까봐 덧붙여 설명드리겠습니다. 결국 subgraph를 mini-batch로 활용하여 학습을 진행하는 것이기 때문에 크게 보면 SGD, 즉 mini-batch training의 범주에 포함되는 것이 맞습니다. 다만, mini-batch 자체가 Neighbor Sampling에서 처럼 따로따로 구분된 computation graph로 구성되는 것이 아니고 하나의 온전한 그래프 모양을 갖는 subgraph이기 때문에, 한번에 노드 임베딩을 생성해버리는 full-batch training처럼 학습할 수 있는 것입니다!</p>

<p><strong>Issues with Cluster-GCN</strong></p>

<p><img src="Lecture%2017%203%2009498e1e5d1a4c63959b85feb5b36a52/Untitled%209.png" alt="Untitled" /></p>

<p><img src="Lecture%2017%203%2009498e1e5d1a4c63959b85feb5b36a52/Untitled%2010.png" alt="Untitled" /></p>

<p>하지만 Cluster-GCN에도 문제는 있습니다. <strong>그래프 community detection은 비슷한 노드들을 같은 커뮤니티에 집어넣기 때문에 추출되는 subgraph들은 원본 그래프를 대표한다고 볼 수 없을 뿐더러 커뮤니티 사이의 엣지들이 소실되며 중요한 메세지들을 잃을 수 있습니다.</strong> 13강의 Granovetter의 실험을 되짚어보면 커뮤니티 사이의 연결로부터 직업 소개와 같은 중요한 정보들이 많이 전달된다는 거 기억나시죠? 중요한 엣지들이 소실됨에 따라 subgraph 마다 마다 계산된 gradient 값의 편차가 심할 것이고, 이는 학습의 불안정성을 야기하며 수렴 속도를 더디게 합니다.</p>

<p><strong>Advanced Cluster-GCN: Overview</strong></p>

<p><img src="Lecture%2017%203%2009498e1e5d1a4c63959b85feb5b36a52/Untitled%2011.png" alt="Untitled" /></p>

<p><strong>이러한 문제는 여러 커뮤니티 (subgraph)를 하나의 mini-batch에 같이 넣음으로써 해결할 수 있습니다.</strong> 이런 방식으로 mini-batch를 구성한다면, subgraph 내에 로컬 커뮤니티 구조도 보존하면서 동시에 <strong>커뮤니티 간 엣지도 포함</strong>하게 되면서 보다 원본 그래프를 잘 대표할 수 있는 subgraph를 만들 수 있습니다. 다만 community detection시에 커뮤니티 사이즈를 좀 더 작게 함으로 여러 커뮤니티를 합쳐서 subgraph를 만들 때에도 subgraph가 GPU에 잘 올라가도록 해야 함에 주의하세요.</p>

<aside>
💡 **여러 개의 커뮤니티 구조를 합쳐서 subgraph로 사용하자!**

</aside>

<p><strong>Advanced Cluster-GCN</strong></p>

<p>Advanced Cluster-GCN의 두 단계 또한 subgraph를 생성하는 부분 이외에는 vanilla Cluster-GCN과 동일하기 때문에 간략하게만 언급하고 넘어가도록 하겠습니다.</p>

<ol>
  <li>
    <p><strong>Pre-processing</strong></p>

    <p>Community detection 알고리즘을 활용하여 원본 그래프를 여러 커뮤니티로 분리합니다. 이 때, 커뮤니티 사이즈를 작게 하여 detection을 진행하도록 합니다.</p>
  </li>
  <li>
    <p><strong>Mini-batch training</strong></p>

    <p>분리된 여러 커뮤니티 중 몇 개를 랜덤하게 골라 aggregate 함으로써 subgraph, 즉 mini-batch를 구성할 것입니다. 이 때, <strong>커뮤니티 사이의 엣지 또한 보존됨에 주목하세요</strong>!</p>
  </li>
</ol>

<p><strong>Comparison of Time Complexity</strong></p>

<p>자, 그럼 지금까지 큰 그래프를 GNN으로 학습하기 위해 배운 두 가지 방법, Neighbor Sampling과 Cluster-GCN을 시간 복잡도 측면에서 비교해보도록 하겠습니다. 두 방법에서 모두 머신러닝 모델은 $K$개의 GNN layer을 가지며, mini-batch의 사이즈는 $M$이라고 가정하겠습니다.</p>

<ul>
  <li><strong>Neighbor Sampling (Sampling factor = $H$일 때)</strong></li>
</ul>

<p><img src="Lecture%2017%203%2009498e1e5d1a4c63959b85feb5b36a52/Untitled%2012.png" alt="Untitled" /></p>

<p>이 방법의 경우 한 mini-batch가 $M$개의 computation graph들로 구성되며, 각 computation graph에서 한 노드 당 $H$개의 이웃 노드만이 샘플링되며 $H^K$의 크기를 갖게 됩니다. 따라서 전체 시간 복잡도는 $M \cdot H^K$ 입니다.</p>

<blockquote>
  <p><strong>시간 복잡도 =</strong> $O(M \cdot H^K)$</p>

</blockquote>

<ul>
  <li>
    <p><strong>Cluster-GCN</strong></p>

    <p>한 mini-batch에 대해 계산되는 시간 복잡도는 subgraph 내 엣지의 수에 비례합니다. 만약 전체 그래프에 대한 평균 차수가 $D_{avg}$이라면, mini-batch로 사용되는 subgraph 내의 노드 갯수가 $M$개이기 때문에 전체 엣지 수는 $M \cdot D_{avg}$가 됩니다. 따라서 전체 $K$ layer에 대한 시간 복잡도는 $K \cdot M \cdot D_{avg}$ 입니다.</p>

    <blockquote>
      <p><strong>시간 복잡도 =</strong> $O(K \cdot M \cdot D_{avg})$</p>

    </blockquote>
  </li>
</ul>

<p>일반적으로 시간 복잡도가 GNN layer의 갯수 $K$에 지수적으로 비례하는 Neighbor Sampling에 비해 선형 비례하는 Cluster-GCN이 계산 측면으로는 더 효율적이라고 합니다. 다만, 만약 $K$를 작게 두는 경우에 보통 현업에서는 Neighbor Sampling 방식이 더 우세하다고 하는데요, 이는 Cluster-GCN 방법이 Neighbor Sampling 방식에 비해 원본 그래프를 잘 대표할 수 있는 mini-batch를 구성하지 못하기 때문이라고 합니다. 어쨌거나 Cluster-GCN 방법은 community detection을 통해 추출된 커뮤니티 구조에 크게 의존하고 있으니까요 🙂</p>

<p><strong>Summary: Cluster-GCN</strong></p>

<aside>
💡 **1.  GNN 구조 : 그대로 사용
2.  Mini-batch 구성 방식: 커뮤니티 구조를 포함하는 subgraph로 구성
3.  학습 방식: SGD training**

</aside>

<ul>
  <li>Cluster-GCN은 먼저 community detection을 통해 원본 그래프를 여러 커뮤니티로 분리함</li>
  <li>여러 커뮤니티가 합쳐진 subgraph가 mini-batch로 사용됨</li>
  <li>온전한 그래프 모양을 띠는 mini-batch에 대해 full-batch training이 수행됨</li>
  <li>$K$가 클 때 Cluster-GCN은 Neighbor Sampling에 비해 더 효율적임</li>
  <li>하지만 일반적으로 Cluster-GCN은 편향된 gradient를 만들게 되는데, $K$가 커진다 하더라도 동일 구조의 subgraph에 대해서만 aggregation이 일어나므로 진짜 원본 그래프를 $K$-hop만큼 explore하는 효과를 낼 수 없기 때문임</li>
</ul>

<hr />
