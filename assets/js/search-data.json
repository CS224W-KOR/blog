{
  
    
        "post0": {
            "title": "Fastpages Notebook Blog Post",
            "content": "About . This notebook is a demonstration of some of capabilities of fastpages with notebooks. . With fastpages you can save your jupyter notebooks into the _notebooks folder at the root of your repository, and they will be automatically be converted to Jekyll compliant blog posts! . Front Matter . The first cell in your Jupyter Notebook or markdown blog post contains front matter. Front matter is metadata that can turn on/off options in your Notebook. It is formatted like this: . # &quot;My Title&quot; &gt; &quot;Awesome summary&quot; - toc:true- branch: master - badges: true - comments: true - author: Hamel Husain &amp; Jeremy Howard - categories: [fastpages, jupyter] . Setting toc: true will automatically generate a table of contents | Setting badges: true will automatically include GitHub and Google Colab links to your notebook. | Setting comments: true will enable commenting on your blog post, powered by utterances. | . The title and description need to be enclosed in double quotes only if they include special characters such as a colon. More details and options for front matter can be viewed on the front matter section of the README. . Markdown Shortcuts . A #hide comment at the top of any code cell will hide both the input and output of that cell in your blog post. . A #hide_input comment at the top of any code cell will only hide the input of that cell. . The comment #hide_input was used to hide the code that produced this. . put a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it: . import pandas as pd import altair as alt . . put a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it: . cars = &#39;https://vega.github.io/vega-datasets/data/cars.json&#39; movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; sp500 = &#39;https://vega.github.io/vega-datasets/data/sp500.csv&#39; stocks = &#39;https://vega.github.io/vega-datasets/data/stocks.csv&#39; flights = &#39;https://vega.github.io/vega-datasets/data/flights-5k.json&#39; . . place a #collapse-output flag at the top of any cell if you want to put the output under a collapsable element that is closed by default, but give the reader the option to open it: . print(&#39;The comment #collapse-output was used to collapse the output of this cell by default but you can expand it.&#39;) . The comment #collapse-output was used to collapse the output of this cell by default but you can expand it. . . Interactive Charts With Altair . Charts made with Altair remain interactive. Example charts taken from this repo, specifically this notebook. . Example 1: DropDown . # use specific hard-wired values as the initial selected values selection = alt.selection_single( name=&#39;Select&#39;, fields=[&#39;Major_Genre&#39;, &#39;MPAA_Rating&#39;], init={&#39;Major_Genre&#39;: &#39;Drama&#39;, &#39;MPAA_Rating&#39;: &#39;R&#39;}, bind={&#39;Major_Genre&#39;: alt.binding_select(options=genres), &#39;MPAA_Rating&#39;: alt.binding_radio(options=mpaa)} ) # scatter plot, modify opacity based on selection alt.Chart(df).mark_circle().add_selection( selection ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=&#39;IMDB_Rating:Q&#39;, tooltip=&#39;Title:N&#39;, opacity=alt.condition(selection, alt.value(0.75), alt.value(0.05)) ) . Example 2: Tooltips . alt.Chart(df).mark_circle().add_selection( alt.selection_interval(bind=&#39;scales&#39;, encodings=[&#39;x&#39;]) ).encode( alt.X(&#39;Rotten_Tomatoes_Rating&#39;, type=&#39;quantitative&#39;), alt.Y(&#39;IMDB_Rating&#39;, type=&#39;quantitative&#39;, axis=alt.Axis(minExtent=30)), # y=alt.Y(&#39;IMDB_Rating:Q&#39;, ), # use min extent to stabilize axis title placement tooltip=[&#39;Title:N&#39;, &#39;Release_Date:N&#39;, &#39;IMDB_Rating:Q&#39;, &#39;Rotten_Tomatoes_Rating:Q&#39;] ).properties( width=500, height=400 ) . Example 3: More Tooltips . label = alt.selection_single( encodings=[&#39;x&#39;], # limit selection to x-axis value on=&#39;mouseover&#39;, # select on mouseover events nearest=True, # select data point nearest the cursor empty=&#39;none&#39; # empty selection includes no data points ) # define our base line chart of stock prices base = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)), alt.Color(&#39;symbol:N&#39;) ) alt.layer( base, # base line chart # add a rule mark to serve as a guide line alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), # add circle marks for selected time points, hide unselected points base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), # add white stroked text to provide a legible background for labels base.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=&#39;price:Q&#39; ).transform_filter(label), # add text labels for stock prices base.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=&#39;price:Q&#39; ).transform_filter(label), data=stocks ).properties( width=500, height=400 ) . Data Tables . You can display tables per the usual way in your blog: . df[[&#39;Title&#39;, &#39;Worldwide_Gross&#39;, &#39;Production_Budget&#39;, &#39;Distributor&#39;, &#39;MPAA_Rating&#39;, &#39;IMDB_Rating&#39;, &#39;Rotten_Tomatoes_Rating&#39;]].head() . Title Worldwide_Gross Production_Budget Distributor MPAA_Rating IMDB_Rating Rotten_Tomatoes_Rating . 0 The Land Girls | 146083.0 | 8000000.0 | Gramercy | R | 6.1 | NaN | . 1 First Love, Last Rites | 10876.0 | 300000.0 | Strand | R | 6.9 | NaN | . 2 I Married a Strange Person | 203134.0 | 250000.0 | Lionsgate | None | 6.8 | NaN | . 3 Let&#39;s Talk About Sex | 373615.0 | 300000.0 | Fine Line | None | NaN | 13.0 | . 4 Slam | 1087521.0 | 1000000.0 | Trimark | R | 3.4 | 62.0 | . Images . Local Images . You can reference local images and they will be copied and rendered on your blog automatically. You can include these with the following markdown syntax: . ![](my_icons/fastai_logo.png) . . Remote Images . Remote images can be included with the following markdown syntax: . ![](https://image.flaticon.com/icons/svg/36/36686.svg) . . Animated Gifs . Animated Gifs work, too! . ![](https://upload.wikimedia.org/wikipedia/commons/7/71/ChessPawnSpecialMoves.gif) . . Captions . You can include captions with markdown images like this: . ![](https://www.fast.ai/images/fastai_paper/show_batch.png &quot;Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/&quot;) . . Other Elements . GitHub Flavored Emojis . Typing I give this post two :+1:! will render this: . I give this post two :+1:! . Tweetcards . Typing &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this: Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 . Youtube Videos . Typing &gt; youtube: https://youtu.be/XfoYk_Z5AkI will render this: . Boxes / Callouts . Typing &gt; Warning: There will be no second warning! will render this: . Warning: There will be no second warning! . Typing &gt; Important: Pay attention! It&#39;s important. will render this: . Important: Pay attention! It&#8217;s important. . Typing &gt; Tip: This is my tip. will render this: . Tip: This is my tip. . Typing &gt; Note: Take note of this. will render this: . Note: Take note of this. . Typing &gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs: . Note: A doc link to an example website: fast.ai should also work fine. . Footnotes . You can have footnotes in notebooks, however the syntax is different compared to markdown documents. This guide provides more detail about this syntax, which looks like this: . For example, here is a footnote {% fn 1 %}. And another {% fn 2 %} {{ &#39;This is the footnote.&#39; | fndetail: 1 }} {{ &#39;This is the other footnote. You can even have a [link](www.github.com)!&#39; | fndetail: 2 }} . For example, here is a footnote 1. . And another 2 . 1. This is the footnote.â†© . 2. This is the other footnote. You can even have a link!â†© .",
            "url": "https://cs224w-kor.github.io/blog/jupyter/2022/09/06/test.html",
            "relUrl": "/jupyter/2022/09/06/test.html",
            "date": " â€¢ Sep 6, 2022"
        }
        
    
  
    
        ,"post1": {
            "title": "Lecture 12.3 - Finding Frequent Subgraphs",
            "content": ". Lecture 12 . Lecture 12.1 - Fast Neural Subgraph Matching &amp; Counting | Lecture 12.2 - Neural Subgraph Matching | Lecture 12.3 - Finding Frequent Subgraphs | . . Lecture 12.3 - Finding Frequent Subgraphs . Problem . . Enumerating all size-k connected subgraphs | Counting #(occurrences of each subgraph type) | ê°€ì¥ ë¹ˆë„ìˆ˜ê°€ ë†’ì€ size-kì˜ Motifsë¥¼ ì°¾ê¸° ìœ„í•´ì„œëŠ” ë‹¤ìŒ 2ê°€ì§€ë¥¼ í•´ê²°í•´ì•¼ í•œë‹¤. ê·¸ëŸ°ë° ì´ë ‡ê²Œ Enumeratingí•˜ê³  Countingí•˜ëŠ” ê²ƒì€ ê°€ëŠ¥í•œ ëª¨ë“  íŒ¨í„´ë“¤ì„ ì¡°í•©ì‹œì¼œì„œ Combinatorial explosionì„ ê°€ì ¸ì˜¤ê¸° ë•Œë¬¸ì— ë§¤ìš° hard computational problemì´ë‹¤. ë”°ë¼ì„œ ìš°ë¦¬ëŠ” ì´ëŸ¬í•œ ë¬¸ì œë¥¼ Representation learningì„ í†µí•´ì„œ í•´ê²°í•œë‹¤. GNNì„ ì´ìš©í•˜ì—¬ ê·¸ë˜í”„ì˜ ì„ë² ë”©ì„ ì„œë¡œ ë¹„êµí•˜ë©´ì„œ ë‘ ê·¸ë˜í”„ì˜ ê´€ê³„ë¥¼ ì°¾ì•„ì„œ í•´ê²°í•œë‹¤. . SPMinier . . ê°€ì¥ ë¹ˆë„ìˆ˜ê°€ ë†’ì€ size-kì˜ Motifsë¥¼ ì°¾ëŠ” í•˜ë‚˜ì˜ neural modelì´ ë°”ë¡œ SPMinerì´ë©°, $G_T$Â ê·¸ë˜í”„ë¥¼ ë¶„í•´í•˜ì—¬ order embedding spaceë¡œ ë³´ë‚¸ ë’¤ ì„ë² ë”© ê³µê°„ì— ë‚˜íƒ€ë‚œ ë’¤ì— ì£¼ì–´ì§„ Subgraph $G_Q$ë¥¼ ëª¨ë‘ ë¹„êµí•˜ë©° Subgraph ë¹ˆë„ ìˆ˜ë¥¼ êµ¬í•˜ëŠ” ê²ƒì´ë‹¤. ì—¬ê¸°ì„œ Subgraphì˜ ì§‘í•© = size-kì˜ Motifsì˜ í›„ë³´ . . Order embedding spaceì—ì„œëŠ” subgraphì˜ ì—¬ë¶€ë¥¼ ì‰½ê²Œ ì•Œ ìˆ˜ ìˆìœ¼ë©° ìœ„ ê·¸ë¦¼ì—ì„œ ë¶‰ì€ìƒ‰ ì˜ì—­ ë‚´ì˜ ëª¨ë“  ë…¸ë€ ì ë“¤ì€Â $G_Q$ë¥¼ í¬í•¨í•˜ëŠ” ëª¨ë“ Â $G_T$ì˜ neighborhoodsê°€ ëœë‹¤. . . SPMiner ëª©í‘œëŠ” k step ë§ˆë‹¤ ê°€ì¥ ë§ì€ eighborhood embeddingsë¥¼ í¬í•¨í•˜ëŠ” Motifë¥¼ ì°¾ëŠ” ê²ƒì´ê³ , í•™ìŠµì€ ë¬´ì‘ìœ„ë¡œ í•œê°œì˜ ë…¸ë“œë¥¼ ì´ˆê¸°ì˜ ê°’ìœ¼ë¡œ ì„ íƒí•œ í›„$(S = u)$ì— ê° step ë§ˆë‹¤ì˜ subgraphë¥¼ ì €ì¥í•˜ëŠ” ê³¼ì •($S$ì˜ ì´ì›ƒ ë…¸ë“œë“¤ì„ ê³¨ë¼ ì ì§„ì ìœ¼ë¡œ ëŠ˜ë ¤ Motifì˜ ì‚¬ì´ì¦ˆë¥¼ í‚¤ì›Œê°„ë‹¤ = ìŠ¤í…ì„ ì§„í–‰í•˜ë©´ì„œ motifë¥¼ ì„±ì¥ì‹œì¼œ ë” í° motifë¥¼ ì°¾ëŠ” ê²ƒì´ ëª©í‘œì´ë©°, ìœ„ì˜ ê·¸ë¦¼ì˜ ë¹¨ê°„ ì ì— ì†í•˜ëŠ” neighborhoodsì˜ ìˆ˜ë¥¼ ìµœëŒ€í™”í•˜ëŠ” ê²ƒì´ ëª©í‘œ)ìœ¼ë¡œ ì´ë£¨ì–´ì§€ê³ , ì§€ì •í•œ $k$(ì›í•˜ëŠ” mofit í¬ê¸°)ì— ë„ë‹¬í•˜ë©´ í•™ìŠµì„ ë©ˆì¶”ë©° Subgraphë¥¼ ë„ì¶œí•œë‹¤. . Summary . Subgraphs and motifs are important concepts that provide insights into the structure of graphs. (Subgraphì™€ MotifëŠ” ê·¸ë˜í”„ì˜ êµ¬ì¡°ì— ëŒ€í•œ insightsë¥¼ ì œê³µí•˜ëŠ” ì¤‘ìš”í•œ ê°œë…) . Their counts can be used as features for nodes and graphs. (ì´ë¥¼ ë…¸ë“œ ë° ê·¸ë˜í”„ì˜ ê¸°ëŠ¥ìœ¼ë¡œ ì‚¬ìš©í•  ìˆ˜ ìˆë‹¤.) | . | We covered neural approaches to prediction subgraph isomorphism relationship. (Subgraphë¥¼ ì˜ˆì¸¡í•˜ê¸° ìœ„í•´ì„œ neural apporachesë¥¼ ì ìš©í•˜ì˜€ë‹¤.) | Order embeddings have desirable properties and can be used to encode subgraph relations (Order embeddingsì˜ ì†ì„±ì„ ì‚¬ìš©í•˜ì—¬ì„œ Subgraphì˜ ê´€ê³„ë¥¼ encodeì— ì‚¬ìš©í•  ìˆ˜ ìˆë‹¤.) | Neural embedding-guided search in order embedding space can enable ML model to identify motifs much more frequent than existing methods (order embedding spaceì„ í†µí•´ì„œ ML ëª¨ë¸ì´ ê¸°ì¡´ ë°©ë²•ë³´ë‹¤ í›¨ì”¬ ë” Motifë¥¼ ì‹ë³„í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.) â€” Reference . | . CS224W: Machine Learning with Graphs 2021 Lecture 12.3 - Finding Frequent Subgraphs . Lecture 12. Frequent Subgraph Mining with GNNs . 12. Frequent Subgraph Mining with GNNs .",
            "url": "https://cs224w-kor.github.io/blog/graph%20neural%20network/gnn/graph%20convolution%20network/gcn/2022/08/17/lecture-1203.html",
            "relUrl": "/graph%20neural%20network/gnn/graph%20convolution%20network/gcn/2022/08/17/lecture-1203.html",
            "date": " â€¢ Aug 17, 2022"
        }
        
    
  
    
        ,"post2": {
            "title": "Lecture 12.2 - Neural Subgraph Matching",
            "content": ". Lecture 12 . Lecture 12.1 - Fast Neural Subgraph Matching &amp; Counting | Lecture 12.2 - Neural Subgraph Matching | Lecture 12.3 - Finding Frequent Subgraphs | . . Lecture 12.2 - Neural Subgraph Matching . Subgraph Matching . Subgraph matchingì´ë€:Â query ê·¸ë˜í”„ê°€ target ê·¸ë˜í”„ì˜ subgraph isomorphismì¸ì§€ í™•ì¸í•˜ëŠ” task . . ìš°ë¦¬ëŠ” ë‹¤ìŒê³¼ ê°™ì´ Queryì™€ Target ê·¸ë˜í”„ê°€ ì£¼ì–´ì¡Œì„ë•Œ Queryê°€ Targetì˜ subgraph ì¸ì§€ íŒë‹¨í•˜ê¸° ìœ„í•´ì„œ embedding spaceì˜ geometric shapeì„ í™œìš©í•˜ë©°, ì—¬ê¸°ì„œ embedding spaceì˜ geometric shapeì„ êµ¬í•˜ê¸° ìœ„í•´ì„œ GNNì„ í™œìš©í•©ë‹ˆë‹¤. . . Node anchorë¥¼ í™œìš©í•˜ì—¬ queryì˜ ë…¸ë“œÂ $v$ì™€ targetì˜ ë…¸ë“œÂ $u$ì˜ ì„ë² ë”©ì´ ë™ì¼í•œì§€ í™•ì¸í•˜ë©°, Queryì˜ anchor nodeê°€ k-hopì„ ê°€ì§ˆ ë•Œ k-hop ë‚´ì— ìˆëŠ” ì´ì›ƒë…¸ë“œë“¤ì˜ ì„ë² ë”© ë˜í•œ ë¹„êµí•œë‹¤. . Order Embedding Space . . ë‹¤ìŒê³¼ ê°™ì´ embedding space ì•ˆì— ${ color{red} square}$ê³¼ ${ color{green} bigcirc}$, ${ color{yellow} bigcirc}$ë¥¼ Representationí–ˆì„ ë•Œ ${ color{green} bigcirc}$ì€ ${ color{green} bigcirc}â©½ leqslantâ©½{ color{red} square}$ì˜ ê´€ê³„ë¥¼ ê°€ì§€ê¸° ë•Œë¬¸ì— Targetì˜ Subgraphë¼ê³  í•  ìˆ˜ ìˆë‹¤. . ê·¸ëŸ¬ë‚˜ Query2ëŠ” ${ color{yellow} bigcirc}â‹  npreceqâ‹ { color{red} square}$ì´ê¸° ë•Œë¬¸ì— Targetê³¼ ë‹¤ë¥¸ ê·¸ë˜í”„ë¼ê³  í•  ìˆ˜ ìˆë‹¤. . . Subgraph isomorphism relationship ì€ 3ì¢…ë¥˜ë¡œ ë‹¤ìŒê³¼ ê°™ì´ embedding spaceì— Representation ëœë‹¤. . TransitivityëŠ” ${ color{yellow} square}$ $ preccurlyeq$ ${ color{green} square}$, ${ color{green} square}$ $ preccurlyeq$ ${ color{red} square}$ ì¸ ê²½ìš° ${ color{yellow} square}$ $ preccurlyeq$ ${ color{red} square}$ì˜ ê´€ê³„ë¥¼ ê°€ì§€ëŠ” í˜•íƒœë¡œ ${ color{yellow} square}$ ì€ ${ color{green} square}$ì˜ Subgraphì´ê³  ${ color{green} square}$ì€ ${ color{red} square}$ì˜ Subgraphì´ê³  ${ color{yellow} square}$ ì€ ${ color{red} square}$ì˜ Subgraphì¸ ê´€ê³„ì´ë‹¤. . Anti-symmetryëŠ” ${ color{yellow} square}$ == ${ color{green} square}$ ì˜ ê´€ê³„ë¥¼ ê°€ì§€ëŠ” í˜•íƒœë¡œ ${ color{yellow} square}$ ê³¼ ${ color{green} square}$ì´ ì„œë¡œ ë™ì¼í•œ ê·¸ë˜í”„ì¸ ê´€ê³„ì´ë‹¤. . Closure under intersectionì€ ${ color{yellow} square}$ ì´ ${ color{red} square}$ê³¼ ${ color{green} square}$ì˜ ë¶€ë¶„ì ì¸ Subgraphì¸ ê´€ê³„ì´ë©°, ìŒìˆ˜ë¥¼ ê°€ì§€ëŠ” ì„ë² ë”©ì€ ì—†ìœ¼ë©°, ì—¬ê¸°ì„œ ${ color{yellow} square}$ ìœ íš¨í•œ ê°’ì„ ê°€ì§„ë‹¤. . Loss Function . . Subgraph ì†ì„±ì´ ìˆœì„œ ì„ë² ë”© ê³µê°„ì—ì„œ ë³´ì¡´ë˜ë„ë¡ ìˆœì„œ ì œì•½ ì¡°ê±´ì„ ì§€ì •í•˜ë©°, ì´ë¥¼ ìœ„í•´ì„œ max-margin lossë¥¼ ì‚¬ìš©í•˜ê²Œë˜ë©° ì´ë¥¼ GNNì— í•™ìŠµí•˜ì—¬ embedding space êµ¬í•©ë‹ˆë‹¤. í•´ë‹¹ loss functionì€ ì•„ë˜ì™€ ê°™ìŠµë‹ˆë‹¤. . E(Gq,Gt)=âˆ‘i=1D(maxâ¡(0,zq[i]âˆ’zt[i]))2E left(G_{q}, G_{t} right)= sum_{i=1}^{D} left( max left(0, z_{q}[i]-z_{t}[i] right) right)^{2}E(Gqâ€‹,Gtâ€‹)=i=1âˆ‘Dâ€‹(max(0,zqâ€‹[i]âˆ’ztâ€‹[i]))2 . ìœ„ì˜ ìˆ˜ì‹ì„ ê·¸ë˜í”„ $G_{q}$ì™€ $G_{t}$ ì‚¬ì´ì˜ â€œmarginâ€ë¡œ ì •ì˜í•©ë‹ˆë‹¤. . $E left(G_{q}, G_{t} right)=0$ (ì™¼ìª½ ê·¸ë˜í”„)ì´ë©´ $G_{q}$ ëŠ” $G_{t}$ ì˜ Subgraphê°€ ì´ê³  $E left(G_{q}, G_{t} right)&gt;0$ (ì˜¤ë¥¸ìª½ ê·¸ë˜í”„)ì´ë©´ $G_{q}$ ëŠ” $G_{t}$ ì˜ Subgraphê°€ ì•„ë‹˜ì„ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤. . Training . í•™ìŠµ ë°ì´í„°ì…‹ $ left(G_{q}, G_{t} right)$ëŠ” $G_{t}$ ì˜ subgraphì¸ $G_{q}$ê°€ ë°˜, ê·¸ë ‡ì§€ ì•Šì€ ê²ƒì´ ë°˜ì´ ë˜ë„ë¡ êµ¬ì„±í•´ì•¼ í•œë‹¤. | Positive sampleì— ëŒ€í•´ì„œëŠ” $E left(G_{q}, G_{t} right)$ ë¥¼ ìµœì†Œí™”í•˜ë„ë¡ negative smapleì— ëŒ€í•´ì„œëŠ” $ max left(0, alpha-E left(G_{q}, G_{t} right) right)$ ë¥¼ ìµœì†Œí™”í•˜ë„ë¡ í•™ìŠµí•˜ëŠ”ë° ì´ëŠ” ëª¨ë¸ì´ ì„ë² ë”©ì„ ë„ˆë¬´ ë©€ë¦¬ ì´ë™ì‹œí‚¤ëŠ” ê²ƒì„ ë°©ì§€í•˜ê¸° ìœ„í•¨ì´ë‹¤. For positive examples: $G_{q}$ê°€ $G_{t}$ì˜ subgraphì¼ ë•Œ $E left(G_{q}, G_{t} right)$ ìµœì†Œí™” | For negative examples: $ max left(0, alpha-E left(G_{q}, G_{t} right) right)$ ìµœì†Œí™” | . | ë°ì´í„°ì…‹ $G$ ë¡œë¶€í„° í•™ìŠµì„ ìœ„í•œ $G_{T}$ ì™€ $G_{Q}$ ë¥¼ ìƒ˜í”Œë§í•˜ëŠ” ê³¼ì •ì´ í•„ìš”í•˜ë‹¤. ë§¤ë²ˆ ë°˜ë³µí•  ë•Œë§ˆë‹¤ ìƒˆë¡œìš´ training pairsì„ ìƒ˜í”Œë§í•œë‹¤. | ì´ì : ë°˜ë³µí•  ë•Œë§ˆë‹¤ ëª¨ë¸ì€ ë‹¤ë¥¸ Subgraph exampleë¥¼ ë³¼ ìˆ˜ ìˆë‹¤. | ìƒ˜í”Œë§ë˜ëŠ” Subgraphê°€ ê¸°í•˜ê¸‰ìˆ˜ì ìœ¼ë¡œ ë§ê¸° ë•Œë¬¸ì— ì„±ëŠ¥ì€ í–¥ìƒë˜ê³ , ê³¼ì í•©ì´ ë°©ì§€ëœë‹¤. | . | $G_{T}$ëŠ” ë¬´ì‘ìœ„ë¡œ anchor ë…¸ë“œ $v$ ë¥¼ ë½‘ì€ ë’¤ ê±°ë¦¬ê°€ $K$ ì¸ ëª¨ë“  ë…¸ë“œë¥¼ í¬í•¨ì‹œì¼œ ë§Œë“ ë‹¤. | Positive example $G_{Q}$ëŠ” BFS ìƒ˜í”Œë§ì„ ê±°ì¹œë‹¤. ì¼ë°˜ì ìœ¼ë¡œ ë°ì´í„° ì§‘í•©ì˜ í¬ê¸°ì— ë”°ë¼ 3-5ë¥¼ ì‚¬ìš©í•œë‹¤. | ëŸ°íƒ€ì„ê³¼ ì„±ëŠ¥ì„ ì ˆì¶œí•˜ëŠ” í•˜ì´í¼íŒŒë¼ë¯¸í„° | . | . . $S=v, V= phi$ ë¡œ ì´ˆê¸°í™”í•œë‹¤. | ë§¤ ìŠ¤í…ë§ˆë‹¤ $S$ ì˜ ëª¨ë“  ì´ì›ƒ ë…¸ë“œ ì§‘í•© $N(S)$ì˜ $10 %$ ë¥¼ ìƒ˜í”Œë§í•˜ì—¬ $S$ ë¡œ ì—…ë°ì´íŠ¸í•˜ë©° ë‚˜ë¨¸ì§€ ë…¸ë“œë“¤ì€ $V$ ë¡œ ì—…ë°ì´íŠ¸ í•œë‹¤. | $K$ ìŠ¤í…ì„ ê±°ì¹˜ë©´ $G_{Q}$ë¥¼ ì–»ê²Œ ëœë‹¤. | Negative exampleì€ $G_{Q}$ë¡œë¶€í„° ë…¸ë“œ/ì—£ì§€ë“¤ì€ ì œê±°í•˜ê±°ë‚˜ ì¶”ê°€í•˜ì—¬ ë§Œë“ ë‹¤. . Summary . Neural subgraph matching uses a machine learning based approach to learn the NP-hard problem of subgraph isomorphism (NP-hard ë¬¸ì œì¸ Subgraph matchingë¥¼ í•´ê²°í•˜ê¸° ìœ„í•´ì„œ ML ê¸°ë°˜ì˜ ì ‘ê·¼ë²•ì„ ì‚¬ìš©) Given query and target graph, it embeds both graphs into an order embedding space (ì¿¼ë¦¬ì™€ ê·¸ë˜í”„ê°€ ì£¼ì–´ì§€ë©´ ëª¨ë‘ order embedding spaceì— í¬í•¨ì‹œí‚¨ë‹¤.) | Using these embeddings, it then computes $E left(G_{q}, G_{t} right)$ to determine whether query is a subgraph of the target (ì´ëŸ¬í•œ ì„ë² ë”©ì„ ì‚¬ìš©í•˜ì—¬ $E left(G_{q}, G_{t} right)$ì„ ê³„ì‚°í•˜ì—¬ Subgraphì¸ì§€ í™•ì¸í•œë‹¤.) | . | Embedding graphs within an order embedding space allows subgraph isomorphism to be efficiently represented and tested by the relative positions of graph embeddings (order embedding spaceë¥¼ í†µí•´ Subgraphë¥¼ íš¨ìœ¨ì ìœ¼ë¡œ í‘œí˜„í•˜ì˜€ìœ¼ë©°, graph embeddingì˜ ìƒëŒ€ì  ìœ„ì¹˜ì— ì˜í•´ í…ŒìŠ¤íŠ¸ ëœë‹¤.) â€” Reference . | . CS224W: Machine Learning with Graphs 2021 Lecture 12.2 - Neural Subgraph Matching . Lecture 12. Frequent Subgraph Mining with GNNs . 12. Frequent Subgraph Mining with GNNs .",
            "url": "https://cs224w-kor.github.io/blog/graph%20neural%20network/gnn/graph%20convolution%20network/gcn/2022/08/17/lecture-1202.html",
            "relUrl": "/graph%20neural%20network/gnn/graph%20convolution%20network/gcn/2022/08/17/lecture-1202.html",
            "date": " â€¢ Aug 17, 2022"
        }
        
    
  
    
        ,"post3": {
            "title": "Lecture 12.1 - Fast Neural Subgraph Matching & Counting",
            "content": ". Lecture 12 . Lecture 12.1 - Fast Neural Subgraph Matching &amp; Counting | Lecture 12.2 - Neural Subgraph Matching | Lecture 12.3 - Finding Frequent Subgraphs | . . Lecture 12.1 - Fast Neural Subgraph Matching &amp; Counting . Subgraphsì™€ MotifsëŠ” ê·¸ë˜í”„ì˜ êµ¬ì¡°ì  insightsë¥¼ ì œê³µí•´ì£¼ëŠ” ì¤‘ìš”í•œ conceptsì´ê¸° ë•Œë¬¸ì—, Subgraphsì™€ Motifsì˜ ê°œìˆ˜ë¥¼ ì…ˆìœ¼ë¡œì¨ ìš°ë¦¬ëŠ” ë…¸ë“œë‚˜ ê·¸ë˜í”„ì˜ featuresë¡œ ì‚¬ìš©í•  ìˆ˜ ìˆë‹¤. . ë”°ë¼ì„œ ìš°ë¦¬ëŠ” Subgraphsì™€ motifsì˜ ê°œìˆ˜ë¥¼ ì„¸ëŠ” ë°©ë²•ì„ ì´ë²ˆ ëª©ì°¨ì— ë‹¤ë£° ê²ƒì´ë©°, Motifsë¥¼ Subgraphsì— í¬í•¨ë˜ê±°ë‚˜ ë™ì¼í•œ ê°œë…ìœ¼ë¡œ ì´í•´í•˜ì‹œë©´ ë©ë‹ˆë‹¤. . Subgraphs . . SubgraphëŠ” ë„¤íŠ¸ì›Œí¬ì˜ êµ¬ì„± ìš”ì†Œì´ë©°, ì´ë¥¼ í†µí•´ì„œ ë„¤íŠ¸ì›Œí¬ë¥¼ êµ¬ì„±í•  ìˆ˜ ìˆê¸° ë•Œë¬¸ì— building blocks of networksì´ë©°, Subgraphë¥¼ ë ˆê³  ë¸”ëŸ­ìœ¼ë¡œ ìƒê°í•œë‹¤ë©´, ë ˆê³ ë“¤ì´ ëª¨ì—¬ì„œ í° ë ˆê³ ë¥¼ í˜•ì„±í•˜ë“¯ì´ ë„¤íŠ¸ì›Œí¬ë¥¼ êµ¬ì„±í•˜ê²Œ ë©ë‹ˆë‹¤. ì´ë ‡ê²Œ ë ˆê³  ë¸”ëŸ­ê³¼ ê°™ì´ ë§ì€ ì˜ì—­ì—ì„œ ë°˜ë³µë˜ëŠ” êµ¬ì¡°ì˜ êµ¬ì„±ìš”ì†Œë¥¼ í†µí•˜ì—¬ ê·¸ë˜í”„ì˜ ê¸°ëŠ¥ ë° ë™ì‘ì„ ê²°ì •í•©ë‹ˆë‹¤. . ì´ëŸ¬í•œ Subgraph êµ¬ì„± ê¸°ì¤€ì— ìˆì–´ 2ê°€ì§€ ë°©ë²•ì´ ìˆëŠ”ë°, ì²«ë²ˆì§¸ëŠ” ë…¸ë“œë¥¼ ì¤‘ì‹¬ìœ¼ë¡œ Subsetì„ êµ¬ì„±í•˜ë©´ â€œNode-induced subgraphâ€ì´ë©°, â€œinduced subgraphâ€ë¼ê³  ë¶€ë¥´ê¸°ë„ í•©ë‹ˆë‹¤. . ë‘ë²ˆì§¸ë¡œ ì—£ì§€ë¥¼ ì¤‘ì‹¬ìœ¼ë¡œ êµ¬ì„±í•˜ë©´ â€œEdge-induced subgraphâ€ë¼ê³  í•˜ë©°, â€œnon-induced subgraphâ€ or just â€œsubgraphâ€ë¼ê³  ë¶€ë¥´ê¸°ë„ í•©ë‹ˆë‹¤. . ì´ëŸ¬í•œ ë‘ ê°€ì§€ì˜ êµ¬ì„± ê¸°ì¤€ì€ ë„ë©”ì¸ì— ë”°ë¼ ë‘ ë°©ì‹ ì¤‘ í•˜ë‚˜ê°€ ì„ íƒë˜ì–´ ì‚¬ìš©ë˜ë©° ì˜ˆë¥¼ ë“¤ì–´ chemistryëŠ” ë…¸ë“œê°€ ì¤‘ìš”í•˜ë¯€ë¡œ node-inducedë¥¼ knowledge graphì—ì„œëŠ” ì—£ì§€ê°€ ì¤‘ìš”í•˜ë¯€ë¡œ edge-inducedë¥¼ í™œìš©í•©ë‹ˆë‹¤. . Graphì—ì„œëŠ” ë‘ ê°œì˜ ê·¸ë˜í”„ê°€ ì„œë¡œ ê°™ì€ ì§€ë¥¼ ì•„ëŠ” ê²ƒì´ ì¤‘ìš”í•œë°, ì´ëŸ¬í•œ ë¬¸ì œë¥¼ Graph isomorphism problem ë¼ê³  í•˜ë©°, ì´ ë¬¸ì œëŠ” NP-hardì…ë‹ˆë‹¤. . . ì˜ˆë¥¼ ë“¤ì–´ì„œ ìœ„ì™€ ê°™ì€ ê·¸ë˜í”„ê°€ ì£¼ì–´ì¡Œì„ë•Œ, ìš°ë¦¬ëŠ” ë‘ ê°œì˜ ê·¸ë˜í”„ê°€ ì™„ì „íˆ ë‹¤ë¥´ë‹¤ê³  ë§í•  ìˆ˜ ìˆì„ê¹Œìš”? ìš°ë¦¬ëŠ” ëŒ€ë¶€ë¶„ $G_2$Â ì•ˆ ì— $G_1$Â ì´ í¬í•¨ë˜ì–´ ìˆë‹¤ê³  ë§í•  ê²ƒì´ë©°, ì´ëŸ¬í•œ ê·¸ë˜í”„ì˜ isomorphismì„ ì¡°ê¸ˆ ë” í™•ì‹¤í•˜ê²Œ êµ¬ë¶„í•˜ê¸° ìœ„í•´ì„œëŠ” Nodeì™€ Edgeë¥¼ ê°€ì§€ê³  ì„œë¡œ ë‘ ê·¸ë˜í”„ë¥¼ ë¹„êµí•´ë³´ëŠ” ê²ƒì…ë‹ˆë‹¤. . . ìœ„ ì²˜ëŸ¼ ë‘ ê·¸ë˜í”„ëŠ” ì‹œê°ì ìœ¼ë¡œ ëª¨ë‘ ë‹¤ë¥¸ ê·¸ë˜í”„ ì²˜ëŸ¼ ë³´ì´ì§€ë§Œ ì²«ë²ˆì§¸ ê·¸ë˜í”„ì˜ ê²½ìš°ì—ëŠ” ê° ë…¸ë“œì™€ ì—°ê²°ëœ ì—£ì§€ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ë¹„êµí•´ë³¸ë‹¤ë©´ ì„œë¡œ ê°™ì€ ê·¸ë˜í”„ì´ë‹¤. ë‘ë²ˆì§¸ ê·¸ë˜í”„ì˜ ê²½ìš° ê° ë…¸ë“œì— ì—°ê²°ëœ ì—£ì§€ê°€ ë‹¤ë¥´ë‹ˆ ë‹¤ë¥¸ ê·¸ë˜í”„ì´ë‹¤. ì´ì²˜ëŸ¼ Nodeì™€ Edgeë¥¼ ê°€ì§€ê³  ì„œë¡œ ë‘ ê·¸ë˜í”„ë¥¼ ë¹„êµí•˜ë©´ ë‘ ê·¸ë˜í”„ì˜ isomorphism íŒë‹¨í•  ìˆ˜ ìˆë‹¤. . . $G_1$ê³¼ $G_2$ì˜ nodeë¥¼ ê°ê° match ì‹œí‚¤ë©´ ìš°ë¦¬ëŠ” $G_1$ê³¼ $G_2$Â ì‚¬ì´ì— ë™ì¼í•œ subgraphë¥¼ ê°€ì§„ë‹¤ëŠ” ê²ƒì„ ì•Œ ìˆ˜ ìˆë‹¤. ë”°ë¼ì„œ ìš°ë¦¬ëŠ” $G_1$ê³¼ $G_2$ëŠ” ë‹¤ë¥¸ ê·¸ë˜í”„ì§€ë§Œ $G_1$ì€ $G_2$ì˜ subgraph ë¼ê³  í•  ìˆ˜ ìˆë‹¤. . . ì™¼ìª½ ì´ë¯¸ì§€ì²˜ëŸ¼ ê·¸ë˜í”„ì˜ í¬ê¸°ê°€ ê°™ë”ë¼ë„ ì—£ì§€ì˜ ìˆ˜ ë° ë°©í–¥ì— ë”°ë¼ ë‹¤ì–‘í•œ non-isomorphic ê·¸ë˜í”„ê°€ ì¡´ì¬í•©ë‹ˆë‹¤. ì™¼ìª½ ê·¸ë¦¼ì€ ê·¸ë˜í”„ì˜ í¬ê¸°ê°€ 4ì¸ ëª¨ë“  ë¹„ë™í˜•, ì—°ê²°ëœ ë¹„ë°©í–¥ ê·¸ë˜í”„ì˜ ì˜ˆì‹œì´ë©°, ì˜¤ë¥¸ìª½ì€, í¬ê¸°ê°€ 3ì¸ ëª¨ë“  ë¹„ë™í˜• ì—°ê²° ë°©í–¥ ê·¸ë˜í”„ì˜ ì˜ˆì‹œì…ë‹ˆë‹¤. . Network Motifs . . Network motifsëŠ”Â â€œrecurring, significant patterns of interconnectionsâ€ë¼ê³  ì •ì˜í•˜ë©° Network Motifsë¥¼ ì •ì˜í•˜ê¸° ìœ„í•´ì„œëŠ” 3ê°€ì§€ ê°œë…ì„ ì•Œì•„ì•¼ í•©ë‹ˆë‹¤. . 1. Pattern: ì°¾ê³ ì í•˜ëŠ” node-induced subgraphë¡œ motifì´ë¼ê³ ë„ ë¶€ë¥´ë©°, ê·¸ë¦¼ì— Induced subgraph of interest ì™€ ê°™ì€ ìœ ë„ë  ìˆ˜ ìˆëŠ” subgraphì˜ íŒ¨í„´ì´ë‹¤. . 2. Recurring: patternì´ ì „ì²´ ê·¸ë˜í”„ì—ì„œ ë‚˜íƒ€ë‚˜ëŠ” ë¹ˆë„ë¥¼ ì˜ë¯¸í•œë‹¤. . 3. Significant: real graphì™€ randomìœ¼ë¡œ ìƒì„±ëœ graphë¥¼ ë¹„êµë¥¼ í†µí•´ ëœë¤í•˜ê²Œ ìƒì„±ëœ ê·¸ë˜í”„ì—ì„œ ê¸°ëŒ€ë³´ë‹¤ ë¹ˆë²ˆí•˜ê²Œ ìˆëŠ”ê°€ë¥¼ ì˜ë¯¸í•œë‹¤. . . Motifê°€ í•„ìš”í•œ ì´ìœ ëŠ” ì´ëŸ¬í•œ Motifì„ í†µí•´Â ê·¸ë˜í”„ì˜ ì‘ë™ë°©ì‹ì„ ì´í•´í•  ìˆ˜ ìˆìœ¼ë©° ì˜ˆì¸¡í•˜ëŠ”ë° ë„ì›€ì´ ë˜ë©°, ì˜ˆë¥¼ ë“¤ì–´, feed-forward loopëŠ” noiseë¥¼ ì œê±°í•˜ëŠ” ìš©ë„ë¡œ ë‡Œì˜ ë‰´ëŸ° ë„¤íŠ¸ì›Œí¬ì—ì„œ í”íˆ ë³¼ ìˆ˜ ìˆìœ¼ë©°, parallel loopsëŠ” ë¨¹ì´ì‚¬ìŠ¬ ê´€ê³„ì—ì„œ ë³¼ ìˆ˜ ìˆìœ¼ë©°, Single-input moduleëŠ” ìœ ì „ì ì œì–´ ë„¤íŠ¸ì›Œí¬ì—ì„œ í”íˆ ë³¼ ìˆ˜ ìˆë‹¤ê³  í•©ë‹ˆë‹¤. . Subgraph Frequency . . ì™¼ìª½ ì´ë¯¸ì§€ì²˜ëŸ¼ Graph-levelì—ì„œëŠ” ê±°ëŒ€ ê·¸ë˜í”„Â $G_T$ì— í¬í•¨ë˜ëŠ” ì‘ì€ ê·¸ë˜í”„Â $G_Q$ì˜ ê°œìˆ˜ë¡œ ë¹ˆë„ë¥¼ ì •ì˜í•  ìˆ˜ ìˆìœ¼ë©°, ì˜¤ë¥¸ìª½ ì´ë¯¸ì§€ì²˜ëŸ¼ Node-levelì—ì„œëŠ”Â $G_Q$ì˜ anchor ë…¸ë“œë¥¼Â $v$ë¼ í•  ë•ŒÂ $G_Q$ì™€ isomorphicgí•œÂ $G_T$ì˜Â subgraphë“¤ ì¤‘Â $u$ë¡œ ë§µí•‘ë˜ëŠ” ë…¸ë“œÂ $v$ì˜ ê°œìˆ˜ë¥¼ ë¹ˆë„ë¡œ ì •ì˜í•˜ë©°, ì´ëŸ¬í•œ ë°©ì‹ì€ outlierì— ë” robustí•˜ë‹¤ê³  í•©ë‹ˆë‹¤. . . ë°ì´í„° ì…‹ì— ì—¬ëŸ¬ ê°œì˜ ê·¸ë˜í”„ê°€ í¬í•¨ë˜ì–´ ìˆê³  ë°ì´í„° ì„¸íŠ¸ì—ì„œ í•˜ìœ„ ê·¸ë˜í”„ì˜ ë¹ˆë„ë¥¼ ê³„ì‚°í•˜ë ¤ëŠ” ê²½ìš°ì—ëŠ” ë°ì´í„° ì…‹ì´ ì—¬ëŸ¬ê°œì˜ ê·¸ë˜í”„ë¥¼ ê°€ì§„ë‹¤ë©´ ì—°ê²°ë˜ì§€ ì•Šì€ ë¶€ë¶„ì„ ê°€ì§„ í•˜ë‚˜ì˜ ê±°ëŒ€ ê·¸ë˜í”„Â $G_T$ë¡œ ë³´ê³  ë¹ˆë„ë¥¼ ê³„ì‚°í•œë‹¤ê³  í•©ë‹ˆë‹¤. . Motif Significance . . Motif SignificanceëŠ” ëœë¤ìœ¼ë¡œ ìƒì„±ëœ networkì™€ ë¹„êµí•˜ê³ ì í•˜ëŠ” network(real)ì—ì„œ ì£¼ì–´ì§„ motifì˜ ë°œìƒ ë¹ˆë„ë¥¼ ë¹„êµí•˜ì—¬ ê³„ì‚°í•œë‹¤. . . Random Graphsë¥¼ ì •ì˜í•˜ëŠ” ë°©ë²•ì€ ìœ„ì˜ ê·¸ë¦¼ ì²˜ëŸ¼ 3ê°€ì§€ë¡œ ì •ì˜ê°€ëŠ¥í•˜ë©°, ì²«ë²ˆì§¸ ë°©ë²•ì€ ErdÅ‘sâ€“RÃ©nyi random graphsë¼ê³  í•˜ë©°, nê°œì˜ ë…¸ë“œë¥¼ ì •í•˜ê³  ê° ë…¸ë“œë¥¼ ì—°ê²°í•˜ëŠ” ì—£ì§€ë¥¼ ìƒì„±í•  í™•ë¥  pë¥¼ ì •í•˜ì—¬ random graphë¥¼ ìƒì„±í•˜ëŠ” ë°©ë²•ì´ë‹¤. . ë˜ ë‹¤ë¥¸ ë°©ë²•ì€ Configuration modelë¼ê³  í•˜ë©°, Configuration modelì€ ì£¼ì–´ì§„ degree sequenceë¥¼ ì‚¬ìš©í•˜ì—¬ random graphë¥¼ ìƒì„±í•˜ëŠ” ë°©ë²•ì´ë‹¤. í•˜ë‚˜ì˜ ë…¸ë“œê°€ ì£¼ì–´ì§€ë©´ ê° ë…¸ë“œëŠ” degree(=spokes)ê°€ ì¡´ì¬í•œë‹¤. ì´ ê° ë…¸ë“œì— ì¡´ì¬í•˜ëŠ” edgeì˜ ê°œìˆ˜, ì¦‰ degreeë¥¼ í•˜ë‚˜ì˜ ì‘ì€ nodeë¡œ ìƒê°í•˜ì—¬ ê° ë…¸ë“œë¥¼ ë¬´ì‘ìœ„ë¡œ ì—°ê²°í•œë‹¤. ì´ ì—°ê²°ì„ ì´ 1ê°œ ì´ìƒ ì¡´ì¬í•˜ë©´ ë§ˆì§€ë§‰ê³¼ ê°™ì´ ê·¸ë˜í”„ì˜ Nodeë¥¼ ì—°ê²°í•˜ì—¬ random graphë¥¼ ìƒì„±í•œë‹¤. . ë˜ ë‹¤ë¥¸ ë°©ë²•ì€ Switchingì´ë¼ í•˜ë©°, Switchingì€ ê·¸ë¦¼ê³¼ ê°™ì´ A-&gt;B / C-&gt;D ë¡œ ì—°ê²°ë˜ëŠ” edgeë¥¼ ë¬´ì‘ìœ„ë¡œ ì„ íƒí•œ í›„, ë‘ edgeì˜ endpointë¥¼ ì„œë¡œ êµí™˜í•˜ì—¬ A-&gt;D / C-&gt;B ë¡œ ì—°ê²°ë˜ëŠ” edgeë¡œ ë°”ê¾¸ì–´ random graphë¥¼ ìƒì„±í•˜ëŠ” ë°©ë²•ì´ë‹¤. ë”°ë¼ì„œ Switchingì€ randomìœ¼ë¡œ ìƒì„±ëœ ê·¸ë˜í”„ì™€ ë¹„êµë  ê·¸ë˜í”„ì˜ node degreeê°€ ì„œë¡œ ê°™ë‹¤ëŠ” íŠ¹ì§•ì„ ê°€ì§„ë‹¤. Configuration modelì€ node degreeê°€ ë‹¬ë¼ì§ˆ ìˆ˜ ìˆë‹¤. . Motif Significantë¥¼ ê³„ì‚°í•˜ëŠ” ê³¼ì •ì€ ì´ 3 Stepìœ¼ë¡œ ì´ë£¨ì–´ ì§„ë‹¤. . ì§€ê¸ˆê¹Œì§€ëŠ” 1, 2ë‹¨ê³„ë¥¼ ë‹¤ë¤˜ê³  ì´ì œ ê°ê°ì˜ ëª¨í‹°ë¸Œê°€ í†µê³„ì ìœ¼ë¡œ ì–¼ë§ˆë‚˜ ìœ ì˜í•œì§€ì— ëŒ€í•˜ì—¬ ì¸¡ì •í•˜ëŠ” ë°©ë²•ì¸ 3ë‹¨ê³„ë¥¼ ë°°ìš¸ ê²ƒì„ . 1ë‹¨ê³„: ì£¼ì–´ì§„ ê·¸ë˜í”„ $ left(G^{ text {real}} right)$ì˜ ëª¨í‹°í”„ ìˆ˜ | 2ë‹¨ê³„: ìœ ì‚¬í•œ í†µê³„(ì˜ˆ: ë…¸ë“œ ìˆ˜, ì—ì§€ ìˆ˜, ì •ë„ ìˆœì„œ)ë¥¼ ê°€ì§„ ëœë¤ ê·¸ë˜í”„ ìƒì„± ë° ëœë¤ ê·¸ë˜í”„ì—ì„œ ëª¨í‹°ë¸Œ ìˆ˜ ê³„ì‚° | 3ë‹¨ê³„: í†µê³„ì  ì¸¡ì •ì„ ì‚¬ìš©í•˜ì—¬ ê° ëª¨í‹°í”„ê°€ ì–¼ë§ˆë‚˜ ìœ ì˜í•œì§€ í‰ê°€í•©ë‹ˆë‹¤. | Z-ì ìˆ˜ ì‚¬ìš© | . Z-score for Statistical Significance . $Z_{i}$ëŠ” ëª¨í‹°í”„ $i$ì˜ í†µê³„ì  ì¤‘ìš”ì„±ì„ í¬ì°© : . Zi=(NirealÂ âˆ’NË‰irandÂ )/stdâ¡(NirandÂ )Z_{i}= left(N_{i}^{ text {real }}- bar{N}{i}^{ text {rand }} right) / operatorname{std} left(N{i}^{ text {rand }} right)Ziâ€‹=(NirealÂ â€‹âˆ’NË‰irandÂ )/std(NirandÂ ) . $N_{i}^{ text {real }}$ì€ graph $G^{ text {real }}$ì—ì„œ motif $i$ì˜ ìˆ˜ | $ bar{N}_{i}^{ text {rand }}$ random graphë“¤ì˜ motifÂ $i$ì˜ ìˆ˜ì˜ í‰ê· ì´ë‹¤. | . Network significance profile (SP): . SPi=Zi/âˆ‘jZj2S P_{i}=Z_{i} / sqrt{ sum_{j} Z_{j}^{2}}SPiâ€‹=Ziâ€‹/jâˆ‘â€‹Zj2â€‹ . â€‹ . $SP_i$ëŠ” ì •ê·œí™”ëœ Z-ì ìˆ˜ì˜ ë²¡í„°ì´ë‹¤. | $SP_i$ì˜ ì°¨ì›ì€ motifsì˜ ìˆ˜ê°€ ëœë‹¤. | SP emphasizes relative significance of subgraphs: í¬ê¸°ê°€ ë‹¤ë¥¸ ë„¤íŠ¸ì›Œí¬ë¥¼ ë¹„êµí•˜ëŠ” ë° ì¤‘ìš” | ì¼ë°˜ì ìœ¼ë¡œ í° ê·¸ë˜í”„ì¼ ìˆ˜ë¡ Z-scoreì˜ ê°’ì´ ì»¤ì§„ë‹¤. | . | . . Motif SignificantëŠ” Z-scoreì™€ ë¹„ìŠ·í•œ ê°œë…ì´ë‹¤. ëœë¤ìœ¼ë¡œ ìƒì„±ëœ networkì˜ motifì˜ ë°œìƒ ë¹ˆë„ì˜ í‰ê· ê³¼ í‘œì¤€í¸ì°¨ë¥¼ ì‚¬ìš©í•˜ê³  ì—¬ê¸°ì„œ ëœë¤ìœ¼ë¡œ ìƒì„±ëœ networkë¥¼ ëª¨ì§‘ë‹¨ìœ¼ë¡œ ì‚¬ìš©í•˜ì—¬ Z-scoreë¥¼ êµ¬í•˜ë©°, ìš°ë¦¬ëŠ” Network significance profileë¥¼ ì •ê·œí™”ëœ Z-scoreì˜ ë²¡í„°ë¡œ êµ¬í•  ìˆ˜ ìˆë‹¤. ì¼ë°˜ì ìœ¼ë¡œ í° ê·¸ë˜í”„ì¼ ìˆ˜ë¡ Z-scoreì˜ ê°’ì´ ì»¤ì§„ë‹¤. . ìš°ë¦¬ëŠ” Z-scoreë¥¼ í†µí•´ì„œ ê° Subgraphì— Significance(ìŒìˆ˜ ê°’ì€ í‘œí˜„ì´ ë¶€ì¡±í•¨, ì–‘ìˆ˜ ê°’ì€ ê³¼í‘œí˜„)ë¥¼ ë¶„ë¥˜í•  ìˆ˜ ìˆìœ¼ë©°, ì´ë¥¼ í†µí•´ network significance profile(ëª¨ë“  í•˜ìœ„ ê·¸ë˜í”„ ìœ í˜•ì— ëŒ€í•œ ê°’ì´ ìˆëŠ” í˜•ìƒ ë²¡í„°)ì„ ë§Œë“¤ ìˆ˜ ìˆìœ¼ë©°, ë‹¤ìŒìœ¼ë¡œëŠ” ì—¬ëŸ¬ ê·¸ë˜í”„ì˜ í”„ë¡œíŒŒì¼ì„ ëœë¤ ê·¸ë˜í”„ì™€ ë¹„êµí•©ë‹ˆë‹¤. . . ê·œì œë§(ìœ ì „ì ê·œì œ) | ë‰´ëŸ° ë„¤íŠ¸ì›Œí¬(ì‹œëƒ…ìŠ¤ ì—°ê²°) | ì–¸ì–´ ë„¤íŠ¸ì›Œí¬(ë‹¨ì–´ ì¸ì ‘) | ì›”ë“œ ì™€ì´ë“œ ì›¹(í˜ì´ì§€ ê°„ í•˜ì´í¼ë§í¬) | ì†Œì…œ ë„¤íŠ¸ì›Œí¬(ìš°ì •) | . ë¹„êµë¥¼ í†µí•´ ì„œë¡œ ë‹¤ë¥¸ ë„ë©”ì¸ì˜ networkëŠ” ì„œë¡œ ë‹¤ë¥¸ network significance profileë¥¼ ê°€ì§„ë‹¤ëŠ” ê²ƒì„ ì•Œ ìˆ˜ ìˆê³  ì„œë¡œ ê°™ì€ ë„ë©”ì¸ì˜ networkëŠ” ì„œë¡œ ê°™ì€ network significance profileì„ ê°€ì§„ë‹¤ëŠ” ê²ƒì„ í™•ì¸í•  ìˆ˜ ìˆë‹¤. ë”°ë¼ì„œ ìš°ë¦¬ëŠ” network significance profileì„ í†µí•´ì„œ í•´ë‹¹ ë„ë©”ì¸ì˜ ê³ ìœ í•œ íŠ¹ì„±ì— ëŒ€í•œ insightë¥¼ ì–»ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤. . Summary . Subgraphs and motifs are the building blocks of graphs (motifsëŠ” graphsì˜ ì¼ë¶€ë¶„) Subgraph isomorphism and counting are NP-hard ( isomorphism and countingì€ NP-hard ë‹¨ê³„ì˜ ì–´ë ¤ì›€ì„ ê°™ëŠ”ë‹¤.) | . | Understanding which motifs are frequent or significant in a dataset gives insight into the unique characteristics of that domain (motifsì˜ significantë¥¼ í†µí•´ì„œ ìš°ë¦¬ëŠ” í•´ë‹¹ ë„ë©”ì¸ì˜ ê³ ìœ í•œ íŠ¹ì„±ì— ëŒ€í•œ insightë¥¼ ì–»ì„ ìˆ˜ ìˆìŒ) | Use random graphs as null model to evaluate the significance of motif via Z-score (random graphsëŠ” motifì˜ significanceë¥¼ í‰ê°€í•˜ê¸° ìœ„í•œ null modelë¡œì¨ ì‚¬ìš©ë¨) | . . Reference . CS224W: Machine Learning with Graphs 2021 Lecture 12.1-Fast Neural Subgraph Matching &amp; Counting . Lecture 12. Frequent Subgraph Mining with GNNs . 12. Frequent Subgraph Mining with GNNs .",
            "url": "https://cs224w-kor.github.io/blog/graph%20neural%20network/gnn/graph%20convolution%20network/gcn/2022/08/17/lecture-1201.html",
            "relUrl": "/graph%20neural%20network/gnn/graph%20convolution%20network/gcn/2022/08/17/lecture-1201.html",
            "date": " â€¢ Aug 17, 2022"
        }
        
    
  
    
        ,"post4": {
            "title": "Lecture 11.3 - Query2box Reasoning over KGs",
            "content": ". Lecture 11. Reasoning in Knowledge Graphs using Embeddings . Lecture 11.1 - Reasoning in Knowledge Graphs | Lecture 11.2 - Answering Predictive Queries | Lecture 11.3 - Query2box: Reasoning over KGs | . . Motivation . 11.2ì˜ ë§ˆì§€ë§‰ ë¶€ë¶„ì—ì„œ ë‹¤ìŒê³¼ ê°™ì€ í•´ê²°í•´ì•¼ í•  ì§ˆë¬¸ë“¤ì´ ìˆì—ˆìŠµë‹ˆë‹¤. . ì¤‘ê°„ ë…¸ë“œë“¤ì€ entitiesì˜ ì§‘í•©(set)ì´ ë í…ë° ì–´ë–»ê²Œ ë‚˜íƒ€ë‚¼ ìˆ˜ ìˆì„ê²ƒì¸ê°€? | latent spaceì—ì„œ êµì§‘í•©(intersection) ì—°ì‚°(operation)ì„ ì–´ë–»ê²Œ ë‚˜íƒ€ë‚¼ ìˆ˜ ìˆì„ ê²ƒì¸ê°€? | Box Embeddings . latent spaceì— queryë¥¼ hyper-rectangles(boxes)ë¡œ ë‚˜íƒ€ë‚¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë°•ìŠ¤ëŠ” ì¤‘ì‹¬ì ê³¼ í¬ê¸°ë¥¼ ë‚˜íƒ€ë‚´ëŠ” offsetì˜ ê°’ìœ¼ë¡œ ì •ì˜í•˜ê³  ë‚˜íƒ€ë‚¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë”°ë¼ì„œ ì´ë¥¼ ìˆ˜ì‹ìœ¼ë¡œ ë‚˜íƒ€ë‚´ë©´ ì•„ë˜ì™€ ê°™ìŠµë‹ˆë‹¤. . . ì„ë² ë”© ê³µê°„ì—ì„œ ë‚˜íƒ€ë‚´ë³´ë©´ ì•„ë˜ ê·¸ë¦¼ê³¼ ê°™ì„ ê²ƒ ì…ë‹ˆë‹¤. ì´ë ‡ê²Œ boxí˜•ì‹ì˜ í‘œí˜„ì´ ì¢‹ì€ ì ì€ ì•ì„œ motivationì—ì„œ ì‚´í´ë³´ì•˜ë˜ 2ê°€ì§€ ê³ ë¯¼ì— ëŒ€í•œ ì¢‹ì€ í•´ê²°ì±…ì´ ë˜ê¸° ë•Œë¬¸ì…ë‹ˆë‹¤. (1) í•´ë‹¹ queryì— ì†í•˜ëŠ” nodeë“¤ì˜ setì„ ì†ì‰½ê²Œ boxì•ˆì— í‘œí˜„í•˜ë©´ ë˜ëŠ” ê²ƒì´ê³  (2) intersection ë˜í•œ ê° queryë¥¼ ë‚˜íƒ€ë‚´ëŠ” box areaì˜ êµì§‘í•© ë¶€ë¶„ìœ¼ë¡œ ë‚˜íƒ€ë‚¼ ìˆ˜ ìˆê¸° ë•Œë¬¸ì…ë‹ˆë‹¤. . . queryë¥¼ boxë¡œ ë‚˜íƒ€ë‚´ê¸°ë¡œ í•œ ë‹¤ìŒ ì´ì œ ë‹¤ë¥¸ ë¶€ë¶„ë“¤ì€ ìë™ìœ¼ë¡œ ë‹¤ìŒê³¼ ê°™ì´ embedding spaceì— í‘œí˜„ë©ë‹ˆë‹¤. . Entity embedding: entityëŠ” zero-volume box(í•˜ë‚˜ì˜ ì )ë¡œ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤. íŒŒë¼ë¯¸í„° ìˆ˜ëŠ” $d|V|$ ì…ë‹ˆë‹¤. | Relation embedding: í•˜ë‚˜ì˜ boxì—ì„œ ë‹¤ë¥¸ ìƒˆë¡œìš´ boxë¡œ ë§¤ì¹­í•˜ëŠ” ê²ƒìœ¼ë¡œ í‘œí˜„ë©ë‹ˆë‹¤. íŒŒë¼ë¯¸í„° ìˆ˜ëŠ” $2d|R|$ ì…ë‹ˆë‹¤. | Intersection operator $f$: ì¸í’‹ì€ boxë“¤ì´ê³  ì•„ì›ƒí’‹ìœ¼ë¡œëŠ” í•˜ë‚˜ì˜ boxê°€ ë‚˜ì˜µë‹ˆë‹¤. ì¸í’‹ìœ¼ë¡œ ë“¤ì–´ê°€ëŠ” boxë“¤ì˜ êµì§‘í•© ë¶€ë¶„ì„ êµ¬í•˜ëŠ” ëª¨ë¸ì…ë‹ˆë‹¤. | . Projection Operator . í•˜ë‚˜ì˜ query ì˜ì—­ì—ì„œ relationì„ í†µí•´ ë‹¤ìŒì˜ query ì˜ì—­ìœ¼ë¡œ ì´ë™í•˜ëŠ” ê²ƒì„ ë‹¤ìŒê³¼ ê°™ì´ Projection Operatorë¥¼ ì´ìš©í•´ì„œ í‘œí˜„í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ìƒˆë¡œìš´ query $qâ€™$ì˜ centerì™€ offset ëª¨ë‘ projection operatorë¥¼ í†µí•´ ì´ì „ query $q$ì™€ relation $r$ë¡œë¶€í„° êµ¬í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. . . ì´ë¥¼ ì•ì„œ conjunctive queryì—ì„œ intersectionì„ êµ¬í•˜ê¸° ì „ê¹Œì§€ì˜ query plan ê³¼ì •ì„ embedding spaceì—ì„œ ë‚˜íƒ€ë‚´ë©´ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤. ESR2ì™€ Short of Breathì—ì„œ ê°ê° ì¶œë°œí•˜ì—¬ relationê³¼ì •(projection ì—°ì‚°)ì„ í†µí•´ query boxê°€ embedding spaceì—ì„œ ì›€ì§ì´ëŠ” ê²ƒì„ ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤. . . Intersection Operator . query boxë“¤ì˜ êµì§‘í•© ë¶€ë¶„ì€ Intersection operatorë¥¼ í†µí•´ì„œ êµ¬í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì´ë¥¼ embedding spaceì—ì„œì˜ ê·¸ë¦¼ìœ¼ë¡œ ë‚˜íƒ€ë‚´ë©´ ì•„ë˜ì™€ ê°™ìŠµë‹ˆë‹¤. Intersection operatorëŠ” ì•ì„œ ì‚´í´ë´¤ë˜ ê²ƒì²˜ëŸ¼ ì¸í’‹ìœ¼ë¡œëŠ” ì—¬ëŸ¬ê°œì˜ boxë“¤ì„ ë°›ê³  ì•„ì›ƒí’‹ìœ¼ë¡œëŠ” í•˜ë‚˜ì˜ boxë¥¼ ì¶œë ¥í•©ë‹ˆë‹¤. ë”°ë¼ì„œ ì•„ì›ƒí’‹ì¸ ìƒˆë¡œìš´ boxì˜ center pointì™€ offsetì„ ì–´ë–»ê²Œ êµ¬í• ì§€ ì„¤ê³„í•´ì•¼ í•©ë‹ˆë‹¤. . . Center point | embedding spaceì—ì„œ ì—¬ëŸ¬ê°œì˜ boxë“¤ì˜ intersectionì´ ë˜ëŠ” ë¶€ë¶„ì˜ ì¤‘ì‹¬ì ì€ boxë“¤ì˜ center pointë“¤ê³¼ ê°€ê¹ë‹¤ëŠ” insightë¥¼ í™œìš©í•©ë‹ˆë‹¤. ë”°ë¼ì„œ ì—¬ëŸ¬ê°œ boxë“¤ì˜ center pointë“¤ë¡œ ë§Œë“¤ì–´ì§€ëŠ” ê²½ê³„ ë°•ìŠ¤(ì•„ë˜ ê·¸ë¦¼ì—ì„œ ë¶„í™ìƒ‰ box) ì•ˆì— intersection boxì˜ center pointê°€ ìˆì„ ê²ƒì…ë‹ˆë‹¤. . . ë”°ë¼ì„œ ìˆ˜ì‹ì ìœ¼ë¡œ ìƒˆë¡œìš´ intersection boxì˜ ì¤‘ì‹¬ì ì€ ì¸í’‹ì´ ë˜ëŠ” boxë“¤ì˜ ì¤‘ì‹¬ì ë“¤ì˜ ìœ„ì¹˜ë¥¼ weighted sumì„ í•˜ì—¬ êµ¬í•©ë‹ˆë‹¤. ì´ë•Œ weight $ mathbf{w}i$ëŠ” Neural networkì¸ $f{cen}$ì„ í†µí•´ trainingë˜ëŠ” trainableí•œ ê°’ì´ë©° ê° center pointë“¤ì— ëŒ€í•œ self-attention scoreë¥¼ ë‚˜íƒ€ë‚¸ë‹¤ê³  ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì¦‰, ì•„ë˜ì˜ ìˆ˜ì‹ê³¼ ê°™ì´ boxë“¤ì˜ center pointì¸ $Cen(q_i)$ê°€ NN $f_{cen}$ì„ í†µí•´ ë‚˜ì˜¨ ì•„ì›ƒí’‹ì„ softmaxë¥¼ í†µí•´ êµ¬í•œ ê°’ì´ weight $ mathbf{w}_i$ê°€ ë˜ëŠ” ê²ƒì…ë‹ˆë‹¤. . . Offset (Box size) | ë‹¹ì—°íˆ intersectionì´ ë˜ëŠ” ì˜ì—­ì€ ì¸í’‹ì´ ë˜ëŠ” boxë“¤ì˜ í¬ê¸°ë³´ë‹¤ ì‘ì•„ì§ˆ ê²ƒ ì…ë‹ˆë‹¤.(shrinking) intersectionì˜ offsetì€ ì•„ë˜ì™€ ê°™ì€ ìˆ˜ì‹ìœ¼ë¡œ í‘œí˜„ë˜ëŠ”ë°, min()ì—ì„œ ì•„ì›ƒí’‹ì˜ ë°•ìŠ¤ê°€ ì¤„ì–´ë“œëŠ” ê²ƒì„ ë³´ì¥í•˜ë©° Neural Network $f_{off}$ì— boxë“¤ì˜ offsetë“¤ì„ ë„£ì–´ì„œ ë‚˜ì˜¨ ê°’ì—ë‹¤ê°€ sigmoid í•¨ìˆ˜ë¥¼ í†µê³¼í•˜ì—¬ ë‚˜ì˜¨ 0~1 ì‚¬ì´ì˜ ê°’ì„ ê³±í•˜ì—¬ intersection boxì˜ offsetì„ ê²°ì •í•˜ê²Œ ë©ë‹ˆë‹¤. . . ì´ë ‡ê²Œ intersection operatorê¹Œì§€ êµ¬í•˜ëŠ” ë²•ì„ ë°°ì› ìœ¼ë‹ˆ ì•ì„œ ì§„í–‰í•œ conjunctive query planì„ embedding spaceì—ì„œì˜ ì—°ì‚° ê³¼ì •ìœ¼ë¡œ ë‚˜íƒ€ë‚´ë©´ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤. ìµœì¢…ì ìœ¼ë¡œ ë¹—ê¸ˆì³ì§„ intersectionì´ answer ì˜ì—­ì´ ë©ë‹ˆë‹¤. . . Entity-to-Box Distance . Embedding ê³µê°„ì—ì„œ ì•ìœ¼ë¡œ ì—¬ëŸ¬ ì—°ì‚°ê³¼ ê³„ì‚°ì„ í•˜ê¸° ìœ„í•´ ê±°ë¦¬ë¥¼ ì •ì˜í•  ìˆ˜ ìˆì–´ì•¼ í•  ê²ƒ ì…ë‹ˆë‹¤. Entity(í•˜ë‚˜ì˜ ì )ê³¼ Box(query)ì‚¬ì´ì˜ ê±°ë¦¬ë¥¼ ì–´ë–»ê²Œ êµ¬í•˜ëŠ” ê²ƒì´ ì¢‹ì„ê¹Œìš”? . ìš°ì„  entityëŠ” zero-volumeì˜ ì ì´ë¯€ë¡œ í•˜ë‚˜ì˜ ìœ„ì¹˜ë¡œ ë‚˜íƒ€ë‚¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì´ì™€ ê°™ì€ ë§¥ë½ìœ¼ë¡œ boxëŠ” ì¤‘ì‹¬ì ì„ ëŒ€í‘œ ìœ„ì¹˜ë¡œ ë‚˜íƒ€ë‚¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤. í•˜ì§€ë§Œ boxëŠ” ì ì´ ì•„ë‹Œ í•˜ë‚˜ì˜ ì˜ì—­ì´ë¯€ë¡œ ë°•ìŠ¤ ì•ˆê³¼ ë°–ì„ êµ¬ë¶„í•  ìˆ˜ ìˆê³  ë”°ë¼ì„œ ì´ë¥¼ êµ¬ë¶„í•˜ì—¬ ì -ë°•ìŠ¤ ì‚¬ì´ì˜ ê±°ë¦¬ $d_{box}$ë¥¼ ê³„ì‚°í•˜ê²Œ ë©ë‹ˆë‹¤. box ì•ˆì— ìˆëŠ” ê²½ìš°, ê±°ë¦¬ $d_{in}$ì— 0~1ì‚¬ì´ì˜ ê°’ì¸ weight $ alpha$ë¥¼ ê³±í•´ì£¼ì–´ ê³„ì‚°í•©ë‹ˆë‹¤. ìˆ˜ì‹ìœ¼ë¡œ ë‚˜íƒ€ë‚´ë©´ ì•„ë˜ì™€ ê°™ìŠµë‹ˆë‹¤. . . . ì´ë ‡ê²Œ ì •ì˜í•œ ì -ë°•ìŠ¤ ì‚¬ì´ì˜ ê±°ë¦¬ $d_{box}$ë¥¼ ê¸°ë°˜ìœ¼ë¡œ score function $f_q(v)$ëŠ” ë‹¤ìŒê³¼ ê°™ì´ ì •ì˜í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ê±°ë¦¬ê°€ ë©€ìˆ˜ë¡ bad scoreì´ì–´ì•¼ í•˜ë¯€ë¡œ $-$ë¥¼ ë¶™ì—¬ ìŒìˆ˜ë¡œ ë§Œë“¤ì–´ì¤ë‹ˆë‹¤. . . Embedding AND-OR Queries . Union operationì¤‘ í•˜ë‚˜ì¸ or ì— ëŒ€í•´ ìƒê°í•´ë´…ì‹œë‹¤. ì˜ˆë¥¼ ë“¤ì–´ ìœ ë°©ì•” ë˜ëŠ” íì•”ì„ ì¹˜ë£Œí•  ìˆ˜ ìˆëŠ” ì•½ë¬¼ì€ ë¬´ì—‡ì¼ê¹Œ?ì™€ ê°™ì€ ì§ˆë¬¸ì— í•„ìš”í•œ ì—°ì‚°ì¼ ê²ƒ ì…ë‹ˆë‹¤. Conjunctive queryì™€ disjunctionì„ ê²°í•©í•œ ê²ƒì„ Existential Positive First-order (EPFO) queryë¼ê³ ë„ í•©ë‹ˆë‹¤. ì—¬ê¸°ì—ì„œëŠ” ì•ìœ¼ë¡œ AND-OR queryë¼ê³  ì§€ì¹­í•˜ê² ìŠµë‹ˆë‹¤. ì—¬ê¸°ì„œ ê³ ë¯¼í•´ì•¼ í•  ë¶€ë¶„ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤. . disjunction operatorë¥¼ ì–´ë–»ê²Œ ì„¤ê³„í•  ê²ƒì¸ê°€? | ì €ì°¨ì›ì˜ vector spaceì—ì„œ AND-OR queryë“¤ì„ ì–´ë–»ê²Œ ë‚˜íƒ€ë‚¼ ê²ƒì¸ê°€? | ìš°ì„  2ë²ˆì§¸ ê³ ë¯¼ì— ëŒ€í•œ í•´ê²°ì±…ì€ ì—†ìŠµë‹ˆë‹¤. ì„ì˜ì˜ queryë“¤ì˜ í•©ì§‘í•©(union)ì„ ë‚˜íƒ€ë‚´ê¸° ìœ„í•´ì„œëŠ” ê³ ì°¨ì›ì˜ embeddingì´ í•„ìš”í•˜ê¸° ë•Œë¬¸ì…ë‹ˆë‹¤. ì•„ë˜ì˜ ì˜ˆì‹œë¥¼ ë³´ë©´ì„œ ì´í•´í•´ë³´ê² ìŠµë‹ˆë‹¤. . . ì´ 3ê°œì˜ queryê°€ ì£¼ì–´ì ¸ ìˆê³  ê° queryì— í•´ë‹¹í•˜ëŠ” answer nodeë“¤ $v_1$, $v_2$, $v_3$ì´ ìˆë‹¤ê³  í•´ë´…ì‹œë‹¤. answer nodeë“¤ì€ ì„œë¡œ ì¤‘ë³µë˜ì§€(overlapping) ì•ŠìŠµë‹ˆë‹¤. . . ê·¸ëŸ¬ë©´ ìš°ì„  ê° queryë¥¼ boxë¡œ ë‚˜íƒ€ë‚´ì–´ answerì— ë§ë„ë¡ ê·¸ë ¤ë³´ë©´ ë‹¤ìŒê³¼ ê°™ì„ ê²ƒì…ë‹ˆë‹¤. . . ë‹¤ìŒìœ¼ë¡œ queryì˜ í•©ì§‘í•© ì—°ì‚° vì— ëŒ€í•œ query boxë¥¼ ê·¸ë ¤ë³´ë©´ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤. ì•„ì§ê¹Œì§€ëŠ” answer(positive)ì™€ answerê°€ ì•„ë‹Œ(negative) entityë“¤ì„ êµ¬ë¶„í•˜ëŠ” ì˜ì—­ì„ ê·¸ë¦¬ëŠ” ê²ƒì´ ê°€ëŠ¥í•©ë‹ˆë‹¤. . . í•˜ì§€ë§Œ ë‹¤ìŒê³¼ ê°™ì´ entityê°€ 4ê°œì¸ ê²½ìš°ê°€ ëœë‹¤ë©´ answerì™€ answerê°€ ì•„ë‹Œ ê²ƒë“¤ì„ êµ¬ë¶„í•˜ëŠ” box embeddingì„ êµ¬í•  ìˆ˜ ì—†ê²Œ ë©ë‹ˆë‹¤. ì´ì²˜ëŸ¼ non-overlapping answerë“¤ì„ ê°€ì§€ê³  ìˆëŠ” queryë“¤ì˜ ê°¯ìˆ˜ê°€ ì¦ê°€í•  ìˆ˜ë¡ ì´ë“¤ì˜ OR ì—°ì‚°ìë¥¼ ë‹¤ë£° ìˆ˜ ìˆëŠ” ê³µê°„ì˜ ì°¨ì›ë„ ë†’ì•„ì§€ê²Œ ë˜ì–´ ì €ì°¨ì›ì—ì„œì˜ AND-OR queryë¥¼ ë‚˜íƒ€ë‚¼ ìˆ˜ ì—†ëŠ” ê²ƒì…ë‹ˆë‹¤. . . í•˜ì§€ë§Œ ì €ì°¨ì›ì˜ AND-OR ì—°ì‚°ìë¥¼ ë‚˜íƒ€ë‚¼ ìˆ˜ ì—†ë‹¤ê³  í•´ì„œ ì´ë¥¼ ì•„ì˜ˆ í•´ê²°í•  ë°©ë²•ì´ ì—†ëŠ” ê²ƒì€ ì•„ë‹™ë‹ˆë‹¤. ì—¬ê¸°ì„œ í•µì‹¬ ì•„ì´ë””ì–´ëŠ” ë§¨ ë§ˆì§€ë§‰ stepì˜ unionë§Œ ì œì™¸í•˜ê³  ë‹¤ë¥¸ union ì—°ì‚°ë“¤ì€ ë‹¤ í’€ì–´ì„œ í‘œí˜„í•´ì£¼ë©´ ëœë‹¤ëŠ” ê²ƒì…ë‹ˆë‹¤. ì•„ë˜ì˜ ê·¸ë¦¼ì—ì„œ ì²˜ëŸ¼ ì• stepì— ìˆë˜ unionì„ í’€ì–´ì„œ ë” ì—¬ëŸ¬ê°œì˜ relationìœ¼ë¡œ í‘œí˜„í•´ì¤€ ë‹¤ìŒ ë§¨ ë§ˆì§€ë§‰ ë‹¨ê³„ì—ì„œ unionì„ ë„£ì–´ì£¼ì–´ ë™ì¹˜ì¸ ìƒí™©ì„ ë§Œë“¤ì–´ì¤ë‹ˆë‹¤. . . Disjunctive Normal Form . AND-OR queryëŠ” ë™ì¹˜ì¸ Disjunctive Normal form, DNFë¡œ ì¹˜í™˜ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤. . . Entityì™€ DNF $q=q_{1} vee q_{2} vee cdots vee q_{m}$ ì‚¬ì´ì˜ ê±°ë¦¬ëŠ” ë‹¤ìŒê³¼ ê°™ì´ ì •ì˜ ë©ë‹ˆë‹¤. . . query qì˜ AND-OR embedding ê³¼ì • query $q$ë¥¼ ë™ì¹˜ì¸ DNF $q=q_{1} vee q_{2} vee cdots vee q_{m}$ë¡œ ë§Œë“ ë‹¤. | $q_1$~ $q_m$ ëª¨ë‘ embedding í•œë‹¤. | (box) distance $d_{box}( mathbf{q}_i, mathbf{v})$ë¥¼ ê³„ì‚°í•œë‹¤. | 3ë²ˆì—ì„œ êµ¬í•œ distanceë“¤ ì¤‘ì—ì„œ ìµœì†Ÿê°’ minì„ êµ¬í•œë‹¤. | final scoreì¸ $f_q(v)=-d_{box}( mathbf{q}, mathbf{v})$ë¥¼ êµ¬í•œë‹¤. | | . Training . í•™ìŠµìœ¼ë¡œ trainingí•  ìˆ˜ ìˆëŠ” ë¶€ë¶„ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤. . íŒŒë¼ë¯¸í„° ìˆ˜ê°€ $d|V|$ ì¸ Entity embedding | íŒŒë¼ë¯¸í„° ìˆ˜ê°€ $2d|R|$ ì¸ Relation embedding | Intersection operator $f$ | . í•™ìŠµ ê³¼ì • ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤. . training graphì¸ $G_{train}$ì—ì„œ í•™ìŠµí•  query $ mathbf{q}$ë¥¼ ìƒ˜í”Œë§ í•©ë‹ˆë‹¤. ì´ë•Œ ì •ë‹µì´ ë˜ëŠ” $v in llbracket q rrbracket_{G_{train}}$ê³¼ ì •ë‹µì´ ì•„ë‹Œ negative sampleì¸ $vâ€™ notin llbracket q rrbracket_{G_{train}}$ ëª¨ë‘ ìƒ˜í”Œë§ í•©ë‹ˆë‹¤. | ìƒ˜í”Œë§ ëœ query $ mathbf{q}$ë¥¼ embedding í•©ë‹ˆë‹¤. | score functionì„ ì´ìš©í•˜ì—¬ $f_q(v)$ì™€ $f_q(vâ€™)$ì„ ê³„ì‚°í•©ë‹ˆë‹¤. | ì•„ë˜ì™€ ê°™ì´ ì •ì˜ëœ loss function $l$ì„ ì´ìš©í•˜ì—¬ positive sampleì˜ scoreëŠ” ìµœëŒ€í™”, negative sampleì˜ scoreëŠ” ìµœì†Œí™”í•˜ëŠ” ë°©í–¥ìœ¼ë¡œ í•™ìŠµí•©ë‹ˆë‹¤. | . Query Generation from Templates . training setì—ëŠ” ë‹¤ìŒê³¼ ê°™ì´ ë‹¤ì–‘í•œ multiple query templateë“¤ì„ ìƒì„±í•´ì„œ ë„£ì–´ì¤„ ìˆ˜ ìˆìŠµë‹ˆë‹¤. . . answer nodeì—ì„œ ì‹œì‘í•´ì„œ backwardí•˜ëŠ” ë°©ì‹ìœ¼ë¡œ training sampleë“¤ì„ ë§Œë“¤ì–´ ë‚¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤. . . ìœ„ì™€ ê°™ì€ KGì—ì„œ answer nodeì¸ Fulvestrantë¥¼ í•˜ë‚˜ ì„ íƒí•©ë‹ˆë‹¤. ì´ ë…¸ë“œì—ì„œë¶€í„° TreatedBy, Assoc relationì„ í†µí•´ backwardë¥¼ í•˜ê²Œ ë˜ë©´ [Anchor1, (Relation1, Realtion2)] templateì¸ [ESR2, (Assoc, TreatedBy)]ì˜ sampleì„ í•˜ë‚˜ ì–»ê²Œ ë˜ëŠ” ê²ƒì…ë‹ˆë‹¤. ì´ì™€ ê°™ì€ ë§¤ì»¤ë‹ˆì¦˜ìœ¼ë¡œ CausedBy relationìœ¼ë¡œ backwardí•˜ê²Œ ë˜ë©´ [Anchor1, (Relation1)] templateì˜ sample í•˜ë‚˜ë¥¼ ì–»ê²Œ ë©ë‹ˆë‹¤. . . ì´ëŸ¬í•œ backwardë¥¼ í†µí•´ positive sampleì¸ $ llbracket q rrbracket_{G}$ë¥¼ ì–»ê²Œ ë˜ê³ , negative sampleì€ KGì—ì„œ random í•˜ê²Œ ìƒ˜í”Œë§í•˜ê²Œ ë§Œë“¤ì–´ì§€ê²Œ ë˜ëŠ”ë° ì´ë•Œ randomí•˜ê²Œ ë§Œë“¤ì–´ì§„ negative sampleì´ positive sampleê³¼ ê²¹ì¹˜ì§€ ì•ŠëŠ”ì§€ ì£¼ì˜í•  í•„ìš”ê°€ ìˆìŠµë‹ˆë‹¤. . Visualization Embedding Space . ì‹¤ì œë¡œ box embeddingì´ ë¬´ì—‡ì„ ë°°ìš°ëŠ”ì§€ í™•ì¸í•´ë³´ê¸° ìœ„í•´ t-SNEë¥¼ ì´ìš©í•˜ì—¬ ì‹œê°í™” í•´ë´…ì‹œë‹¤. (ê³ ì°¨ì›ì—ì„œ box embeddingì´ ì´ë£¨ì–´ì§€ê³  2ì°¨ì›ì˜ t-sneë¡œ ë‚˜íƒ€ë‚´ëŠ” ê²ƒì´ë¯€ë¡œ box ì˜ì—­ìœ¼ë¡œ ë‚˜íƒ€ë‚´ì–´ì§€ì§€ ì•ŠìŠµë‹ˆë‹¤.) . . 11ê°• ì •ë¦¬ . í° KGì—ì„œì˜ predictive queryì˜ answerë¥¼ ì°¾ëŠ” ë°©ë²•ì— ëŒ€í•´ ì•Œì•„ë³´ì•˜ìŠµë‹ˆë‹¤. | í•µì‹¬ì€ queryë¥¼ embeddingí•¨ìœ¼ë¡œì¨ latent spaceì—ì„œì˜ íƒìƒ‰ì„ ê°€ëŠ¥í•˜ê²Œ í–ˆë‹¤ëŠ” ì ì…ë‹ˆë‹¤. embeddingëœ queryë“¤ì„ ê°€ì§€ê³  í•  ìˆ˜ ìˆëŠ” ë‹¤ì–‘í•œ operatorë“¤ì— ëŒ€í•´ ì•Œì•„ë³´ì•˜ìŠµë‹ˆë‹¤. | query box embeddingì„ í†µí•´ embedding spaceì—ì„œ answerë¥¼ êµ¬í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. | . | . . Original Lecture Video : CS224W: Machine Learning with Graphs 2021 Lecture 11.3 - Query2box: Reasoning over KGs .",
            "url": "https://cs224w-kor.github.io/blog/reasoning/knowledge%20graph/box%20embedding/projection%20operator/intersection%20operator/distance/query%20generation/score%20function/t-sne/2022/08/17/lecture-1103.html",
            "relUrl": "/reasoning/knowledge%20graph/box%20embedding/projection%20operator/intersection%20operator/distance/query%20generation/score%20function/t-sne/2022/08/17/lecture-1103.html",
            "date": " â€¢ Aug 17, 2022"
        }
        
    
  
    
        ,"post5": {
            "title": "Lecture 11.2 - Answering Predictive Queries",
            "content": ". Lecture 11. Reasoning in Knowledge Graphs using Embeddings . Lecture 11.1 - Reasoning in Knowledge Graphs | Lecture 11.2 - Answering Predictive Queries | Lecture 11.3 - Query2box: Reasoning over KGs | . . Traversing KG in Vector Space . í•µì‹¬ ì•„ì´ë””ì–´ëŠ” Queryë¥¼ embeddingí•˜ëŠ” ê²ƒì…ë‹ˆë‹¤. ì´ì „ì— ë°°ìš´ TransEë¥¼ ë‹¤ì‹œ ë³µìŠµí•´ë³´ë©´ $ mathbf{h}$ì—ì„œ $ mathbf{t}$ë¡œ ê°€ê¸° ìœ„í•´ $ mathbf{r}$ë¥¼ ê²½ìœ í•˜ì—¬ ê°ˆ ë•Œì˜ score function $f_r(h, t)$ëŠ” ë‹¤ìŒê³¼ ê°™ì´ ë‚˜íƒ€ëƒˆì—ˆìŠµë‹ˆë‹¤. . . . ì´ê²ƒì„ Query Embedding ê´€ì ìœ¼ë¡œ ë‹¤ì‹œ ìƒê°í•´ë³´ë©´, query embedding $ mathbf{q} = mathbf{h} + mathbf{r}$ë¡œ ë³¼ ìˆ˜ ìˆê³  ì´ë•Œì˜ ëª©í‘œëŠ” query embedding $ mathbf{q}$ë¥¼ answer embedingì¸ $ mathbf{t}$ì™€ ê°€ê¹ê²Œ ë§Œë“œëŠ” ê²ƒì…ë‹ˆë‹¤. ë”°ë¼ì„œ queryë¥¼ ì„ë² ë”© í•œë‹¤ëŠ” ê²ƒì€ TransEë¥¼ multi-hop reasoningìœ¼ë¡œ ì¼ë°˜í™” ì‹œí‚¤ëŠ” ê²ƒê³¼ ê°™ìŠµë‹ˆë‹¤. Queryì˜ relationë“¤ì´ ì—¬ëŸ¬ê°œ ì ìš©ë˜ëŠ” ê³¼ì •ì€ embedding spaceì—ì„œ vector addition ê³¼ì •ì´ë©° ì´ëŠ” Knowledge Graphì—ì„œì˜ entityë§ˆë‹¤ ë…ë¦½ì ìœ¼ë¡œ ì¼ì–´ë‚˜ëŠ” ê³¼ì •ì…ë‹ˆë‹¤. . . . . ğŸ’¡ - ***TransE***ëŠ” **composition relationì„ ë‹¤ë£° ìˆ˜ ìˆì—ˆê¸° ë•Œë¬¸ì—** path queryì˜ multi-hopë¥¼ **latent spaceì—ì„œì˜ relation additionìœ¼ë¡œ** í‘œí˜„í•  ìˆ˜ ìˆë„ë¡ í–ˆìŠµë‹ˆë‹¤. - ë°˜ë©´, *TransR, DistMult, ComplEx*ëŠ” composition relationì„ ì²˜ë¦¬í•  ìˆ˜ ì—†ê¸° ë•Œë¬¸ì— path queryë¥¼ ë‹¤ë£° ìˆ˜ ìˆë„ë¡ í™•ì¥í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. Conjunctive Queries . . ì§€ê¸ˆê¹Œì§€ One-hop queryì™€ ì´ë¥¼ multi-hopìœ¼ë¡œ í™•ì¥í•œ Path Queryê¹Œì§€ ì•Œì•„ë³´ì•˜ìŠµë‹ˆë‹¤. ê·¸ë ‡ë‹¤ë©´ ë§ˆì§€ë§‰ìœ¼ë¡œ ë‚¨ì€ ë” ë³µì¡í•œ queryë“¤ ê°„ì˜ ê²°í•©(conjunction) ì—°ì‚°ë„ ê°€ëŠ¥í• ê¹Œìš”? ê° queryì˜ anchor nodeì—ì„œ ì¶œë°œí•˜ì—¬ traversingì„ í•˜ê³  ë‚œ ë’¤, ê° traversingì˜ ê²°ê³¼ì¸ nodeì˜ êµì§‘í•©(intersection)ì„ êµ¬í•˜ë©´ ì •ë‹µì„ êµ¬í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. . . í•˜ì§€ë§Œ ì—¬ê¸°ì„œë„ missing connectionì´ ìˆë‹¤ë©´ í•´ë‹¹ connectionê³¼ ê´€ë ¨ëœ ì •ë‹µì„ ë†“ì¹  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì´ëŸ° ê²½ìš° implicití•˜ê²Œ connectionì„ ì¶”ë¡ í•´ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë§Œì•½ (ESR2, Assoc, Breast Cancer)ì´ missingë˜ì—ˆë‹¤ë©´ (1) ESR2ê°€ BRCA1ê³¼ ESR1 ëª¨ë‘ì™€ interaction ê´€ê³„ë¥¼ ê°€ì§€ê³  ìˆê³  (2)BRCA1ê³¼ ESR1 ê°ê°ì€ Breast cancerê³¼ association ê´€ê³„ë¥¼ ê°€ì§€ê³  ìˆê¸° ë•Œë¬¸ì— (ESR2, Assoc, Breast Cancer)ì„ ê°€ì§ˆ ìˆ˜ ìˆìŒì„ ì¶”ë¡ í•´ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤. . . Traversing KG in Vector Space . í•˜ì§€ë§Œ KG traversingì„ ë²¡í„° ê³µê°„ì—ì„œ ë‚˜íƒ€ë‚¼ ë•Œ ëª‡ê°€ì§€ ë” ê³ ë¯¼í•´ë³¼ ì ë“¤ì´ ìˆìŠµë‹ˆë‹¤. ì´ì— ëŒ€í•œ ë‹µì€ ë‹¤ìŒ section 11.3ì—ì„œ ì°¾ì•„ë³´ê² ìŠµë‹ˆë‹¤. . conjunctive queryì—ì„œ ë‚˜íƒ€ë‚˜ëŠ” ì¤‘ê°„ê³¼ì •ì˜ ë…¸ë“œ(intermediate node)ë“¤ì€ ì§‘í•©(set)ìœ¼ë¡œ ë‚˜íƒ€ë‚˜ê²Œ ë˜ê³  ì´ë¥¼ ì–´ë–»ê²Œ í‘œí˜„í•  ê²ƒì¸ì§€ | latent spaceì—ì„œì˜ Intersectionì„ ì–´ë–»ê²Œ ì •ì˜í•˜ê³  ì–´ë–¤ ì—°ì‚°ê³¼ì •ìœ¼ë¡œ ë‚˜íƒ€ë‚¼ ê²ƒì¸ì§€ | . . Original Lecture Video : CS224W: Machine Learning with Graphs 2021 Lecture 11.2 - Answering Predictive Queries .",
            "url": "https://cs224w-kor.github.io/blog/reasoning/knowledge%20graph/traversing/query%20embedding/transe/conjunctive%20query/2022/08/17/lecture-1102.html",
            "relUrl": "/reasoning/knowledge%20graph/traversing/query%20embedding/transe/conjunctive%20query/2022/08/17/lecture-1102.html",
            "date": " â€¢ Aug 17, 2022"
        }
        
    
  
    
        ,"post6": {
            "title": "Lecture 11.1 - Reasoning in Knowledge Graphs",
            "content": ". Lecture 11. Reasoning in Knowledge Graphs using Embeddings . Lecture 11.1 - Reasoning in Knowledge Graphs | Lecture 11.2 - Answering Predictive Queries | Lecture 11.3 - Query2box: Reasoning over KGs | . . ë³µìŠµ: Knowledge Graph Completion Task . ê±°ëŒ€í•œ Knowledge Graphê°€ ì£¼ì–´ì¡Œì„ ë•Œ ì–´ë–»ê²Œ KGë¥¼ ì™„ì„±(missing partë¥¼ ì°¾ëŠ” ê²ƒ)ì‹œí‚¬ ìˆ˜ ìˆì„ê¹Œìš”? ì´ì— ëŒ€í•œ í•´ê²°ë°©ë²•ì€ headì™€ relationì´ ì£¼ì–´ì¡Œì„ë•Œ tailsë¥¼ ì˜ˆì¸¡í•˜ëŠ” ê²ƒìœ¼ë¡œ ë°”ê¿” ìƒê°í•´ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤. . . ì•„ë˜ì˜ ë¬¸í•™ Knowledge Graphì˜ ì˜ˆì‹œì—ì„œ â€œJ.K. Rowlingâ€ë¼ëŠ” headingê³¼ â€œgenreâ€ë¼ëŠ” relationì´ ì£¼ì–´ì¡Œì„ ë•Œ, â€œScience Fictionâ€ì´ë¼ëŠ” tailì„ ì˜ˆì¸¡í•˜ëŠ” taskë¥¼ ìƒê°í•´ ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤. . . Reasoning over Knowledge Graphs . KGì—ì„œì˜ ì¶”ë¡ (Reasoning)ì€ ë‹¤ìŒê³¼ ê°™ì€ ëª©í‘œë¡œ ìƒê°í•´ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤. . Goal: KGì—ì„œ multi-hop(=ê·¸ë˜í”„ ìƒì—ì„œ ì—¬ëŸ¬ë²ˆ traverse) ì¶”ë¡ í•˜ê¸° | . ê²°êµ­ multi-hop queryë¥¼ ì˜ˆì¸¡í•˜ëŠ” ê²ƒì´ë¼ê³  ìƒê°í•  ìˆ˜ ìˆëŠ”ë°, ê·¸ë ‡ë‹¤ë©´ ë¶ˆì™„ì „í•˜ê³ (incomplete) ê±°ëŒ€í•œ(massive)í•œ KGì—ì„œ ì–´ë–»ê²Œ multi-hop reasoningì„ í•  ìˆ˜ ìˆì„ê¹Œìš”? ë¬¸ì œë¥¼ ì •ì˜í•˜ê¸° ìœ„í•´ ìš°ì„  query íƒ€ì…ë“¤ì— ëŒ€í•´ ì•Œì•„ë³´ê² ìŠµë‹ˆë‹¤. query íƒ€ì…ì—ëŠ” ì•„ë˜ì˜ ê·¸ë¦¼ê³¼ ê°™ 3ê°€ì§€, one-hop, path(multi-hop), conjuntive queryê°€ ìˆìŠµë‹ˆë‹¤. . . ì•„ë˜ì˜ KGì—ì„œ ê° query íƒ€ì…ì˜ ì˜ˆì‹œ ì§ˆë¬¸ê³¼ ì´ì— ë”°ë¥¸ query í‘œê¸°ë¥¼ ì‚´í´ë³´ê² ìŠµë‹ˆë‹¤. . . Query íƒ€ì… ì˜ˆì‹œ(ìì—°ì–´ ì§ˆë¬¸) ì˜ˆì‹œ(Query) . One-hop Queries | What adverse event is caused by Fulvestrant? Fì— ì˜í•´ ì¼ì–´ë‚˜ëŠ” ë¶€ì‘ìš©ì€? | [e:Fulvestrant, (r:Causes)] | . Path Queries | What protein is associated with the adverse event caused by Fulvestrant? Fì— ì˜í•´ ì¼ì–´ë‚˜ëŠ” ë¶€ì‘ìš©ê³¼ ê´€ë ¨ëœ ë‹¨ë°±ì§ˆì€ ë¬´ì—‡ì¸ê°€? | [e:Fulvestrant, (r:Causes, r:Assoc)] | . Conjunctive Queries | What is the drug that treats breast cancer and caused headaches? ìœ ë°©ì•”ì„ ì¹˜ë£Œí•˜ëŠ”ë° ì“°ì´ì§€ë§Œ ë‘í†µì„ ì¼ìœ¼í‚¬ ìˆ˜ ìˆëŠ” ì•½ë¬¼ì€ ë¬´ì—‡ì¸ê°€? | ([e:BreastCancer, (r:TreatedBy)], [e:Migraine, (r:CausedBy)]) | . *e: entity, r:relation . Predictive One-hop Queries â†’ Path Queries . ì‚¬ì‹¤ KG completion ë¬¸ì œëŠ” One-hop queryë¬¸ì œë“¤ë¡œ ë‚˜ëˆ„ì–´ì„œ ìƒê°í•´ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤. One-hopë³´ë‹¤ ë” ë³µì¡í•´ì§€ë©´ One-hopì„ ë” ì—¬ëŸ¬ê°œ ë¶™ì¸ë‹¤ê³  ìƒê°í•˜ë©´ ë˜ê¸° ë•Œë¬¸ì…ë‹ˆë‹¤. . . ê·¸ë˜ì„œ ê²°êµ­ N-hop queryëŠ” ë‹¤ìŒê³¼ ê°™ì´ â€œachorâ€ì¸ $v_a$ë¥¼ ì‹œì‘ì ìœ¼ë¡œ í•˜ëŠ” ì—¬ëŸ¬ê°œì˜ relationë“¤ì´ ì´ì–´ì§€ëŠ” $(r_1, r_2, â€¦, r_n)$ìœ¼ë¡œ ë‚˜íƒ€ë‚¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì´ë•Œ í•´ë‹¹ KG completionì— ë§ëŠ” ì •ë‹µ queryëŠ” ì—¬ëŸ¬ê°œì¼ ìˆ˜ ìˆê¸° ë•Œë¬¸ì— $ llbracket q rrbracket_{G}$ë¡œ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤. . . . ì˜ˆì‹œë¥¼ ë“¤ì–´ Notationì„ ì‚´í´ë³´ê² ìŠµë‹ˆë‹¤. . ì•„ë˜ì˜ KGì—ì„œ Fulvestrantì— ì˜í•´ ì¼ì–´ë‚˜ëŠ” ë¶€ì‘ìš©ê³¼ ê´€ë ¨ëœ ë‹¨ë°±ì§ˆì€ ì–´ë–¤ ê²ƒë“¤ì´ ìˆì„ê¹Œìš”? . . ì´ taskì—ì„œ anchor nodeëŠ” Fulvestrantê°€ ë˜ê³ , 2ë²ˆì˜ relationë“¤ì„ ê±°ì³ì„œ ë‹µë³€ì„ í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. Notationê³¼ ë§¤ì¹­í•˜ì—¬ ì •ë¦¬í•´ë³´ë©´ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤. . $v_a$â†’ e:Fulvestrant | $(r_1, r_2)$â†’ (r: Causes, r:Assoc) | Query: [e:Fulvestrant, (r:Causes, r:Assoc)] | . . Queryê°€ ì •ë¦¬ë˜ì—ˆë‹¤ë©´ ì´ì œ queryì— ë§ëŠ” ì •ë‹µ pathë¥¼ ì°¾ì•„ì•¼ í•  ê²ƒ ì…ë‹ˆë‹¤. ì–´ë–»ê²Œ KGì—ì„œ ì •ë‹µ pathë“¤ì„ ì°¾ì•„ë³¼ ìˆ˜ ìˆì„ê¹Œìš”? . ê°„ë‹¨í•˜ê²Œ KGì—ì„œ queryì˜ relation ìˆœì„œì— ë§ì¶° traverseë¥¼ í•˜ë©´ ë©ë‹ˆë‹¤. Fulvestrantì—ì„œ ì‹œì‘í•˜ëŠ” (1) Causes relationì„ ê°€ì§€ê³  ìˆëŠ” ëª¨ë“  ë‹¤ìŒ ë…¸ë“œë“¤ì„ ì°¾ìŠµë‹ˆë‹¤. ê·¸ ë‹¤ìŒìœ¼ë¡œ í•´ë‹¹ ë…¸ë“œì—ì„œ (2) Association relationì„ ê°€ì§€ê³  ìˆëŠ” ëª¨ë“  ë…¸ë“œë“¤ì„ ì°¾ì•„ ì •ë‹µì„ ì°¾ìœ¼ë©´ ë©ë‹ˆë‹¤. . . Predictive Queries with LIMITATION . ë‹¨ìˆœí•˜ê²Œ traversingí•˜ë©´ì„œ queryì— ì‘ë‹µí•˜ëŠ” ê²ƒìœ¼ë¡œ í•´ê²°í•˜ë©´ ì¢‹ê² ì§€ë§Œ ë¶ˆí–‰íˆë„ KGëŠ” incompleteí•œ ê²½ìš°ê°€ ë§ìŠµë‹ˆë‹¤. ì¦‰, ê´€ë ¨ëœ ì§€ì‹ì´ ì¶©ë¶„í•˜ì§€ ëª»í•´ì„œ ê·¸ë˜í”„ì— ë‚˜íƒ€ë‚´ì§€ ëª»í–ˆì„ ìˆ˜ë„ ìˆê³  ëª¨ë“  ì‚¬ì‹¤ë“¤ì„ ìƒë‹¹í•œ ì‹œê°„ê³¼ ë¹„ìš©ì„ ë“¤ì—¬ í‘œí˜„í•˜ê¸°ê°€ ì‚¬ì‹¤ìƒ ê±°ì˜ ë¶ˆê°€ëŠ¥ì— ê°€ê¹ê¸° ë•Œë¬¸ì— ëŒ€ë‹¤ìˆ˜ì˜ relationë“¤ì´ missingë˜ì–´ ìˆëŠ” incomplete KGë¥¼ ë°›ì•„ë“¤ì¼ ìˆ˜ ë°–ì— ì—†ìŠµë‹ˆë‹¤. ë”°ë¼ì„œ ì´ëŸ° ë¶ˆì™„ì „í•¨ë•Œë¬¸ì— ëª¨ë“  answerë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´ ì•ì„  ì˜ˆì‹œì—ì„œ Fulvestrantâ†”Short of Breath ì˜ relationì´ missingë˜ì–´ ìˆë‹¤ë©´ BIRC2 ë¥¼ ì°¾ì„ ìˆ˜ ì—†ì„ ê²ƒì…ë‹ˆë‹¤. . . ê·¸ë ‡ë‹¤ë©´ (probabilistic) completeí•œ KGë¥¼ ë§Œë“œëŠ” ê²ƒì„ ìš°ì„ ì ìœ¼ë¡œ í•´ì•¼ í• ê¹Œìš”? . í•˜ì§€ë§Œ completeí•œ KGëŠ” ë§¤ìš° ì—°ê²°ì„±ì´ ì•½í•œ ê´€ê³„ì¡°ì°¨ë¡œ 0ì´ ì•„ë‹Œ ë§¤ìš° ì‘ì€ ê°’(í™•ë¥ )ìœ¼ë¡œë¼ë„ í‘œí˜„ì´ ë  ê²ƒì´ê¸° ë•Œë¬¸ì— ë§¤ìš° denseí•  ê²ƒì…ë‹ˆë‹¤. ì´ë ‡ê²Œ denseí•œ KGì—ì„œ traversingì˜ ì‹œê°„ë³µì¡ë„ëŠ” pathì˜ ê¸¸ì´ë¥¼ $L$ì´ë¼ê³  í–ˆì„ ë•Œ $d^L_{max}$ê°€ ë  ê²ƒì´ê¸° ì§€ìˆ˜ì ìœ¼ë¡œ ì¦ê°€í•  ê²ƒì´ê¸° ë•Œë¬¸ì— êµ¬í•˜ê¸° ì–´ë ¤ì›Œì§‘ë‹ˆë‹¤. . . ë”°ë¼ì„œ incompleteí•œ KGì—ì„œ path-based queriesë¥¼ êµ¬í•˜ëŠ” taskì¸ Predictive queriesë¥¼ ì •ì˜í•˜ê³  ë¬¸ì œë¥¼ í’€ê²Œ ë©ë‹ˆë‹¤. ì´ taskëŠ” link prediction taskê°€ ì¢€ ë” ì¼ë°˜í™”ëœ ê²ƒì´ë¼ê³  ìƒê°í•  ìˆ˜ ìˆìœ¼ë©° one-step predictionì„ multi-step predictionìœ¼ë¡œ í™•ì¥í•˜ëŠ” ë¶€ë¶„ì´ ì¤‘ìš”í•œ í¬ì¸íŠ¸ì…ë‹ˆë‹¤. . . Original Lecture Video : CS224W: Machine Learning with Graphs 2021 Lecture 11.1 - Reasoning in Knowledge Graphs .",
            "url": "https://cs224w-kor.github.io/blog/reasoning/knowledge%20graph/one-hop%20query/path%20query/conjunctive%20query/predictive%20query/2022/08/17/lecture-1101.html",
            "relUrl": "/reasoning/knowledge%20graph/one-hop%20query/path%20query/conjunctive%20query/predictive%20query/2022/08/17/lecture-1101.html",
            "date": " â€¢ Aug 17, 2022"
        }
        
    
  
    
        ,"post7": {
            "title": "Lecture 6.3 - Deep Learning for Graphs",
            "content": ". Lecture 6. Graph Neural Networks (1) GNN Model . Lecture 6.1 - Introduction to Graph Neural Networks | Lecture 6.2 - Basics of Deep Learning | Lecture 6.3 - Deep Learning for Graphs | . . Lecture 6.3 - Deep Learning for Graphs . ì´ì œ ë³¸ê²©ì ìœ¼ë¡œ ì´ë²ˆ ê°•ì˜ì˜ ë©”ì¸ ì£¼ì œì˜€ë˜ ë”¥ëŸ¬ë‹ì„ í™œìš©í•œ ê·¸ë˜í”„ ë°ì´í„° ì„ë² ë”© ë°©ë²•ì— ëŒ€í•´ ê³µë¶€í•´ ë´…ì‹œë‹¤. ì°¸ê³ ë¡œ Deep Encoderì€ ê·¸ë˜í”„ ë‰´ëŸ´ ë„¤íŠ¸ì›Œí¬(GNN)ìœ¼ë¡œë„ ë¶€ë¥´ê¸° ë•Œë¬¸ì— í˜¼ì¬í•´ì„œ ì‚¬ìš©í•˜ë”ë¼ë„ í—·ê°ˆë¦¬ì§€ ì•Šìœ¼ì‹œê¸¸ ë°”ëë‹ˆë‹¤. . Setup . ë“¤ì–´ê°€ê¸°ì— ì•ì„œ ë°˜ë³µì ìœ¼ë¡œ í™œìš©í•  notationì— ëŒ€í•´ ê°„ëµíˆ ì„¤ëª…í•˜ê³  ì‹œì‘í•˜ê² ìŠµë‹ˆë‹¤. ì•ìœ¼ë¡œ ì•„ë˜ ê¸°í˜¸ë¥¼ ì­‰ ì‚¬ìš©í•˜ì—¬ ì„¤ëª…ì„ í•  ì˜ˆì •ì´ë‹ˆ ì˜ ì•Œì•„ë‘ì‹œê¸° ë°”ëë‹ˆë‹¤. . $V$: ë…¸ë“œ ì§‘í•© | $A$: ì¸ì ‘ í–‰ë ¬ (Adjacency matrix) | $X in mathbb{R}^{m times |V|}$: ë…¸ë“œ features | $v$: ë…¸ë“œ ì§‘í•©ì— í¬í•¨ëœ í•œ ë…¸ë“œ, $N(v)$: $v$ì˜ ì´ì›ƒ ë…¸ë“œ | . ê·¸ë˜í”„ êµ¬ì¡°ëŠ” ì—£ì§€ì˜ ë°©í–¥ì„± ë° ê°€ì¤‘ ì—¬ë¶€ì— ë”°ë¼ ì—¬ëŸ¬ ì¢…ë¥˜ë¡œ ë¶„ë¥˜í•  ìˆ˜ ìˆì§€ë§Œ, ì´í•´ë¥¼ ìœ„í•´ ì—¬ê¸°ì„œëŠ” ê°€ì¥ ê°„ë‹¨í•œ undirected &amp; unweighted ê·¸ë˜í”„ë¡œ ì„¤ëª…í•˜ê² ìŠµë‹ˆë‹¤. ë”°ë¼ì„œ ì¸ì ‘ í–‰ë ¬ì€ 0ê³¼ 1ë¡œ ì´ë£¨ì–´ì§„, ëŒ€ê° ë°©í–¥ìœ¼ë¡œ symmetricí•œ í–‰ë ¬ì´ë¼ê³  ìƒê°í•˜ì‹¤ ìˆ˜ ìˆìŠµë‹ˆë‹¤. . ë˜í•œ, ì´ì „ì˜ Shallow Encoderê³¼ëŠ” ë‹¬ë¦¬ ì´ì œ ë…¸ë“œ featureë„ í•¨ê»˜ ê³ ë ¤í•˜ì—¬ ë…¸ë“œ ì„ë² ë”©ì„ í•™ìŠµí•œë‹¤ëŠ” ì ì— ìœ ì˜í•˜ì‹œê¸° ë°”ëë‹ˆë‹¤. . ì‚¬ì‹¤ ëŒ€ë¶€ë¶„ì˜ ê·¸ë˜í”„ ë°ì´í„°ì…‹ì€ ë…¸ë“œ featureì„ í¬í•¨í•˜ê³  ìˆì§€ë§Œ, ë§Œì— í•˜ë‚˜ ì—†ëŠ” ê²½ìš°ë¼ë©´ ë‹¤ìŒê³¼ ê°™ì€ ë²¡í„°/ê°’ì„ ë…¸ë“œ featureë¡œ ì‚¬ìš©í•˜ê¸°ë„ í•©ë‹ˆë‹¤. ë…¸ë“œì˜ one-hot ì¸ì½”ë”© ë²¡í„° | ìƒìˆ˜ ë²¡í„° [1, 1, â€¦, 1] | ë…¸ë“œ ì°¨ìˆ˜(degree) | . | . Naive Approach . ë”¥ëŸ¬ë‹ ëª¨ë¸ì„ í™œìš©í•˜ì—¬ ê·¸ë˜í”„ ë° ë…¸ë“œë¥¼ ì„ë² ë”© í•˜ê¸° ìœ„í•´ ê°€ì¥ ì‰½ê²Œ ìƒê°í•  ìˆ˜ ìˆëŠ” ë°©ë²•ì€ ë‹¨ìˆœí•˜ê²Œ ê·¸ë˜í”„ì˜ êµ¬ì¡°ì ì¸ íŠ¹ì„±ì„ ë‚˜íƒ€ë‚´ëŠ” ì¸ì ‘ í–‰ë ¬ê³¼ ë…¸ë“œ feature í–‰ë ¬ì„ ê·¸ëƒ¥ ì´ì–´ ë¶™ì—¬ì„œ ë”¥ëŸ¬ë‹ ëª¨ë¸ì— ë˜ì ¸ì£¼ëŠ” ê²ƒì…ë‹ˆë‹¤. . . ìœ„ì™€ ê°™ì€ undirected ê·¸ë˜í”„ì˜ ê° ë…¸ë“œê°€ 2ì°¨ì›ì˜ featureë¥¼ ê°ê° ê°€ì§€ê³  ìˆë‹¤ë©´, ë‹¨ìˆœíˆ ë‘ í–‰ë ¬ì„ ì´ì–´ ë¶™ì—¬ì„œ ë§Œë“  7ì°¨ì›ì˜ ë²¡í„°ë¥¼ ë‰´ëŸ´ ë„¤íŠ¸ì›Œí¬ì— ì „ë‹¬í•˜ë©´ ê° ë…¸ë“œë¥¼ ê°„ë‹¨íˆ ì„ë² ë”© í•  ìˆ˜ ìˆì„ ê²ƒì…ë‹ˆë‹¤. í•˜ì§€ë§Œ ì´ëŸ¬í•œ ë°©ë²•ì—ëŠ” ë‹¤ìŒê³¼ ê°™ì€ ë¬¸ì œê°€ ì¡´ì¬í•©ë‹ˆë‹¤. . $O(|V|)$ íŒŒë¼ë¯¸í„°ê°€ í•„ìš”í•¨ | . ë…¸ë“œ featureì´ $d$ì°¨ì›ì´ë¼ê³  ê°€ì •í•˜ë©´, ê° ë…¸ë“œê°€ ë‰´ëŸ´ ë„¤íŠ¸ì›Œí¬ì— ì…ë ¥ë˜ëŠ” ì°¨ì›ì´ $|V|+d$ ì´ê² ì£ ? ë”°ë¼ì„œ ê·¸ë˜í”„ì˜ ë…¸ë“œ ê°¯ìˆ˜ì— ë¹„ë¡€í•˜ì—¬ ëª¨ë¸ì˜ íŒŒë¼ë¯¸í„°ê°€ ì¦ê°€í•©ë‹ˆë‹¤. . ë‹¤ë¥¸ ì‚¬ì´ì¦ˆì˜ ê·¸ë˜í”„ì—ëŠ” ì ìš©í•  ìˆ˜ ì—†ìŒ ìœ„ì™€ ê°™ì€ ê·¸ë˜í”„ì— ëŒ€í•´ ë‰´ëŸ´ ë„¤íŠ¸ì›Œí¬ë¥¼ ê¸°ê» í•™ìŠµì‹œì¼œ ë†“ì•˜ëŠ”ë°, 100ê°œì˜ ë…¸ë“œë¡œ êµ¬ì„±ëœ ìƒˆë¡œìš´ ê·¸ë˜í”„ê°€ ì¸í’‹ìœ¼ë¡œ ë“¤ì–´ì˜¨ë‹¤ë©´, ì¸í’‹ ì°¨ì›ì´ ë§ì§€ ì•Šì•„ í•™ìŠµì‹œí‚¨ ì„ë² ë”© ëª¨ë¸ì„ í™œìš©í•  ìˆ˜ ì—†ì„ ê²ƒì…ë‹ˆë‹¤. | ë…¸ë“œ ìˆœì„œê°€ ë°”ë€Œë©´ ë™ì¼ ë…¸ë“œì˜ ì„ë² ë”©ì´ ë‹¬ë¼ì§ˆ ìˆ˜ ìˆìŒ ìœ„ ê·¸ë˜í”„ì—ì„œ ë…¸ë“œ ìˆœì„œë¥¼ Aâ†’Bâ†’Câ†’Dâ†’Eì—ì„œ Bâ†’Eâ†’Aâ†’Câ†’D ë“±ìœ¼ë¡œ ë°”ê¾¼ë‹¤ë©´, ì´ì— ë”°ë¼ ì¸ì ‘ í–‰ë ¬ë„ ë°”ë€Œê²Œ ë©ë‹ˆë‹¤. ì´ë ‡ê²Œ ëœë‹¤ë©´ ê°™ì€ A ë…¸ë“œë¥¼ ì„ë² ë”© í•˜ê¸° ìœ„í•´ ì¸í’‹ìœ¼ë¡œ í™œìš©ë˜ëŠ” 7ì°¨ì›ì˜ ë²¡í„°ê°€ ë‹¬ë¼ì§€ê¸° ë•Œë¬¸ì— ì„ë² ë”© ê²°ê³¼ ê°’ë„ ë‹¬ë¼ì§ˆ ê²ƒì…ë‹ˆë‹¤. | . From Images to Graphs . ê·¸ë ‡ë‹¤ë©´ ê¸°ì¡´ CNN ëª¨ë¸ì—ì„œ ì•„ì´ë””ì–´ë¥¼ ì°¨ìš©í•´ë³´ëŠ” ê±´ ì–´ë–¨ê¹Œìš”? . . . ì´ë¯¸ì§€ë¥¼ ë‹¤ë£¨ëŠ” CNNì€ ìœ„ì™€ ê°™ì´ ê³ ì •ëœ ì‚¬ì´ì¦ˆì˜ convolution í•„í„°ë¥¼ ì‚¬ìš©í•˜ì—¬ ì´ë¯¸ì§€ë¥¼ ì£¼ìš± í›‘ìŠµë‹ˆë‹¤. ì—¬ê¸°ì„œì˜ convolution í•„í„°ëŠ” ë¶‰ì€ ì›ìœ¼ë¡œ í‘œì‹œëœ íƒ€ê²Ÿ í”½ì…€ì˜ ì£¼ìœ„ í”½ì…€ ì •ë³´ë¥¼ ì¶•ì•½í•˜ëŠ” ì—­í• ì„ í•©ë‹ˆë‹¤. ì‚¬ì‹¤ ì´ë¯¸ì§€ê°€ íŠ¹ìˆ˜í•œ í˜•íƒœì˜ ê·¸ë˜í”„ë¡œ í•´ì„ë  ìˆ˜ ìˆìŒì„ ìƒê°í•´ë³´ë©´, ê·¸ë˜í”„ ë°ì´í„°ì—ì„œë„ íƒ€ê²Ÿ ë…¸ë“œì˜ ì„ë² ë”©ì„ ë§Œë“¤ê¸° ìœ„í•´ ì£¼ë³€ ë…¸ë“œì˜ ì •ë³´ë¥¼ ì‚¬ìš©í•œë‹¤ëŠ” ì•„ì´ë””ì–´ëŠ” ë‚˜ì˜ì§€ ì•Šì•„ ë³´ì…ë‹ˆë‹¤. . í•˜ì§€ë§Œ ê·¸ë˜í”„ì—ì„œëŠ” CNNì—ì„œì™€ ê°™ì´ ê³ ì •ëœ í¬ê¸°ì˜ í•„í„°(?)ë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì–´ë–¤ ë…¸ë“œëŠ” í•œë‘ê°œì˜ ì´ì›ƒ ë…¸ë“œë¥¼ ê°€ì§€ì§€ë§Œ ë˜ ì–´ë–¤ ë…¸ë“œëŠ” ìˆ˜ë°±ìˆ˜ì²œê°œì˜ ì´ì›ƒ ë…¸ë“œë¥¼ ê°€ì§ˆ ìˆ˜ ìˆê¸° ë•Œë¬¸ì´ì£ . . . ê·¸ë ‡ë‹¤ë©´ ê·¸ë˜í”„ë¥¼ ì„ë² ë”© í•  ë•Œ íƒ€ê²Ÿ ë…¸ë“œì˜ ì´ì›ƒ ë…¸ë“œì—ì„œ ì •ë³´ë¥¼ ì „ë‹¬ë°›ì•„ ì´ë¥¼ í™œìš©í•˜ì—¬ íƒ€ê²Ÿ ë…¸ë“œì˜ ì„ë² ë”©ì„ ì—…ë°ì´íŠ¸ í•˜ë˜, íƒ€ê²Ÿ ë…¸ë“œë§ˆë‹¤ ì´ì›ƒ ë…¸ë“œì˜ ê°¯ìˆ˜ê°€ ë‹¤ë¥¼ ìˆ˜ ìˆëŠ” ì ì„ ê³ ë ¤í•˜ì—¬ ê°ê¸° ë‹¤ë¥¸ computation graphë¥¼ ê°–ë„ë¡ í•˜ëŠ” ê²ƒì´ ì¢‹ê² ìŠµë‹ˆë‹¤! . ë‹¤ìŒê³¼ ê°™ì€ ì•„ì´ë””ì–´ë¥¼ ê·¼ê°„ìœ¼ë¡œ Graph Convolutional Networkê°€ ë“±ì¥í•˜ê²Œ ë˜ì—ˆìŠµë‹ˆë‹¤. ë‚¨ì€ ê°•ì˜ì—ì„œëŠ” ì´ GCNì„ ë””í…Œì¼í•˜ê²Œ ì„¤ëª…í•˜ê³  ìˆìŠµë‹ˆë‹¤. . Idea: Aggregate Neighbors . ì£¼ìš” Idea: ì´ì›ƒ ë…¸ë“œ ì •ë³´ë¥¼ ê°€ì§€ê³  íƒ€ê²Ÿ ë…¸ë“œ ì„ë² ë”©ì„ ìƒì„±í•˜ì! . . ì™¼ìª½ê³¼ ê°™ì€ ê·¸ë˜í”„ì—ì„œ ìš°ë¦¬ê°€ ì„ë² ë”©í•˜ê³  ì‹¶ì€ íƒ€ê²Ÿ ë…¸ë“œê°€ ë…¸ë€ìƒ‰ì˜ A ë…¸ë“œë¼ê³  ìƒê°í•´ ë´…ì‹œë‹¤. ê·¸ë ‡ë‹¤ë©´ A ë…¸ë“œì˜ ì´ì›ƒ ë…¸ë“œ, ê·¸ë¦¬ê³  ê·¸ ì´ì›ƒ ë…¸ë“œë“¤ì˜ ì´ì›ƒ ë…¸ë“œë¥¼ ê°€ì§€ê³  ì˜¤ë¥¸ìª½ê³¼ ê°™ì€ computation graphê°€ ìƒê¹ë‹ˆë‹¤. . íƒ€ê²Ÿ ë…¸ë“œ AëŠ” ì§ì† ì´ì›ƒì¸ ë…¸ë“œ B, C, Dë¡œë¶€í„° ë©”ì‹œì§€ë¥¼ ì „ë‹¬ ë°›ê³ , ëª¨ë“  ë©”ì‹œì§€ë¥¼ í•©ì¹œ í›„ ì–´ë– í•œ ë³€í™˜ì„ ê±°ì³ ë³¸ì¸ì˜ ì„ë² ë”©ìœ¼ë¡œ í™œìš©í•©ë‹ˆë‹¤. ìš°ì¸¡ ê·¸ë¦¼ì—ì„œ íšŒìƒ‰ ë°•ìŠ¤ë¡œ í‘œì‹œëœ ë‰´ëŸ´ ë„¤íŠ¸ì›Œí¬ê°€ ë°”ë¡œ ì´ëŸ¬í•œ 1) ë©”ì‹œì§€ ë³€í™˜, 2) ì´ì›ƒ ë…¸ë“œë¡œë¶€í„° ì˜¨ ë©”ì‹œì§€ë¥¼ í†µí•©í•˜ëŠ” ë‘ ê³¼ì •ì„ ìˆ˜í–‰í•˜ê²Œ ë©ë‹ˆë‹¤. ì´ ë‰´ëŸ´ ë„¤íŠ¸ì›Œí¬ ë‚´ì˜ ëª¨ë¸ íŒŒë¼ë¯¸í„°ê°€ ìµœì¢…ì ì¸ ìš°ë¦¬ì˜ í•™ìŠµ ëŒ€ìƒì´ ë˜ëŠ” ê²ƒì…ë‹ˆë‹¤. . ì—¬ê¸°ì„œ ë˜ ëˆˆì—¬ê²¨ ë³´ì•„ì•¼ í•  ì ì´ ìˆìŠµë‹ˆë‹¤. ì—¬ì§€ê» ìš°ë¦¬ëŠ” ë…¸ë€ìƒ‰ A ë…¸ë“œë¥¼ íƒ€ê²Ÿ ë…¸ë“œë¡œ í•œ computation graphë§Œ ë³´ì•˜ëŠ”ë°, ê·¸ë ‡ë‹¤ë©´ B, C, D ë“± ë‹¤ë¥¸ íƒ€ê²Ÿ ë…¸ë“œì— ëŒ€í•´ì„œë„ ë™ì¼í•œ computation graphë¥¼ ê°€ì§ˆê¹Œìš”? . . ì•„ë‹™ë‹ˆë‹¤. ë…¸ë“œë§ˆë‹¤ ì´ì›ƒ ë…¸ë“œì˜ ê°¯ìˆ˜ì™€ ì¢…ë¥˜ê°€ ë‹¤ë¥´ê¸° ë•Œë¬¸ì— ë‹¹ì—°íˆ ë…¸ë“œë§ˆë‹¤ ì„œë¡œ ë‹¤ë¥¸ computation graphë¥¼ ê°€ì§€ê²Œ ë©ë‹ˆë‹¤. . Deep Model: Many Layers . . Layerì˜ ìˆ˜ . Deep Encoderì€ ì—¬ëŸ¬ layerë¡œ êµ¬ì„±í•  ìˆ˜ ìˆëŠ”ë°, í•œ layerì´ ì§ì† ì´ì›ƒ ë…¸ë“œì— ëŒ€í•œ ì •ë³´ë¥¼ aggregateí•˜ëŠ” ê²ƒì´ê¸° ë•Œë¬¸ì— layerì„ ë‘ ê°œ ìŒ“ëŠ”ë‹¤ë©´ ì§ì† ì´ì›ƒ ë…¸ë“œì— ëŒ€í•œ ì´ì›ƒ ë…¸ë“œ, ì¦‰ íƒ€ê²Ÿ ë…¸ë“œë¡œë¶€í„° 2-hop ë–¨ì–´ì§„ ë…¸ë“œì˜ ì •ë³´ê¹Œì§€ í™œìš©í•˜ê² ë‹¤ëŠ” ì˜ë¯¸ë¡œ í•´ì„í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ìœ„ ê·¸ë¦¼ì—ì„œë„ ì˜ ë‚˜íƒ€ë‚˜ ìˆëŠ”ë°, 2ê°œì˜ layerë¡œ êµ¬ì„±ëœ ëª¨ë¸ì„ ì‚¬ìš©í•˜ê¸° ë•Œë¬¸ì— íƒ€ê²Ÿ ë…¸ë“œ Aë¡œë¶€í„° 2-hop ë–¨ì–´ì§„ ë…¸ë“œ E, Fì˜ ì •ë³´ë„ ì„ë² ë”© ìƒì„±ì— í™œìš©ë˜ëŠ” ê²ƒì„ ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤. . ë…¸ë“œ ì„ë² ë”© ì´ˆê¸°í™” . ë˜í•œ, ë³´í†µ Layer-0ì—ì„œ ìµœì´ˆ ë…¸ë“œ ì„ë² ë”©ìœ¼ë¡œ ë…¸ë“œ featureì„ ì‚¬ìš©í•©ë‹ˆë‹¤. ëª¨ë“  ë…¸ë“œ ì„ë² ë”©ì€ layerì„ ê±°ì¹ ìˆ˜ë¡ ì´ì›ƒ ë…¸ë“œì˜ ì •ë³´ë¥¼ ë³€í™˜í•˜ê³  í•©ì¹œ í›„ ì—…ë°ì´íŠ¸ ë©ë‹ˆë‹¤. ê²°êµ­ ëª¨ë“  layerì„ ê±°ì¹˜ê³  ë‚˜ë©´ ìµœì¢… ë…¸ë“œ ì„ë² ë”©ì´ ìƒì„±ë˜ì–´ ìš°ë¦¬ê°€ downstream taskë¥¼ ìœ„í•´ ì‚¬ìš©í•˜ê²Œ ë˜ëŠ” ê²ƒì´ì£ . . Aggregator í•¨ìˆ˜ . ì—¬ê¸°ì„œ íƒ€ê²Ÿ ë…¸ë“œ Aê°€ ì´ì›ƒ ë…¸ë“œ B, C, Dì˜ ë©”ì‹œì§€ë¥¼ aggregation í•  ë•Œ, ë…¸ë“œ B, C, Dì˜ ìˆœì„œì™€ ê´€ê³„ ì—†ì´ aggregateëœ ë©”ì‹œì§€ëŠ” ë™ì¼í•´ì•¼ í•©ë‹ˆë‹¤. ì¦‰, ë©”ì‹œì§€ë¥¼ aggregateí•˜ëŠ” í•¨ìˆ˜ëŠ” permutation-invariantí•œ ì†ì„±ì„ ê°€ì ¸ì•¼ í•œë‹¤ëŠ” ë§ì…ë‹ˆë‹¤. ì¼ë°˜ì ì¸ GNNì€ ì£¼ë¡œ í•©, í‰ê· , ë§¥ìŠ¤ í’€ë§ë“±ì˜ aggregatorë¥¼ í™œìš©í•©ë‹ˆë‹¤. . The Math: Deep Encoder . . ì, ì´ì œ ê°€ì¥ ê¸°ë³¸ì ì¸ GNN í˜•íƒœë¥¼ ì •ì˜í•˜ê³  ì´ë¥¼ ìˆ˜ì‹ìœ¼ë¡œ ë‚˜íƒ€ë‚´ì–´ ì•Œê³ ë¦¬ì¦˜ì˜ ì›ë¦¬ë¥¼ ìì„¸íˆ ë“¤ì—¬ë‹¤ë³´ëŠ” ì‹œê°„ì„ ê°–ê² ìŠµë‹ˆë‹¤. ìš°ë¦¬ì˜ GNNì€ íƒ€ê²Ÿ ë…¸ë“œì˜ ì´ì›ƒ ë…¸ë“œ ì„ë² ë”©ì„ ì „ë‹¬ë°›ì•„ ì´ë¥¼ í‰ê· ëƒ„ìœ¼ë¡œì¨ ë©”ì‹œì§€ë¥¼ aggregate í•©ë‹ˆë‹¤. ê·¸ í›„, ë‰´ëŸ´ ë„¤íŠ¸ì›Œí¬ë¥¼ í†µí•´ ì–´ë– í•œ ë³€í™˜ì„ ê±°ì¹˜ê³  ì´ë¥¼ í™œìš©í•˜ì—¬ íƒ€ê²Ÿ ë…¸ë“œ ì„ë² ë”©ì„ ì—…ë°ì´íŠ¸ í•©ë‹ˆë‹¤. ì´ë¥¼ ìˆ˜ì‹ìœ¼ë¡œ ë‚˜íƒ€ë‚´ë©´ ì•„ë˜ì™€ ê°™ìŠµë‹ˆë‹¤. . . ì‹ì´ ì²˜ìŒì—” ë˜ê²Œ ë³µì¡í•´ ë³´ì´ì§€ë§Œ, í•˜ë‚˜ì”© ëœ¯ì–´ë³´ë©´ ì‚¬ì‹¤ ì•„ì£¼ ê°„ë‹¨í•©ë‹ˆë‹¤. ì‹ ì „ë°˜ì— ë‚˜íƒ€ë‚˜ëŠ” $h_{v}^{(l)}$ ì€ $l$ë²ˆì§¸ layerì—ì„œ ë…¸ë“œ $v$ì˜ ì„ë² ë”©ì„ ë‚˜íƒ€ë‚¸ë‹¤ê³  ë³´ì‹œë©´ ë©ë‹ˆë‹¤. . ì´ˆë¡ìƒ‰ ìˆ˜ì‹ ë¸”ëŸ­ : ì²« ë…¸ë“œ ì„ë² ë”©ì„ ë…¸ë“œ featureë¡œ ì´ˆê¸°í™”í•©ë‹ˆë‹¤. | íŒŒë€ìƒ‰ ìˆ˜ì‹ ë¸”ëŸ­ : $l+1$ë²ˆì§¸ layerì—ì„œì˜ ë…¸ë“œ ì„ë² ë”©ì„ ë§Œë“¤ê¸° ìœ„í•´, ë¨¼ì € íƒ€ê²Ÿ ë…¸ë“œ $v$ì˜ ì´ì›ƒ ë…¸ë“œì— ëŒ€í•´ $l$ ë²ˆì§¸ layerì—ì„œì˜ ë…¸ë“œ ì„ë² ë”© í‰ê· ì„ êµ¬í•©ë‹ˆë‹¤. ê·¸ í›„, ì´ì›ƒ ë…¸ë“œì˜ í‰ê·  ì„ë² ë”©ì— ì–´ë– í•œ transformation $W_{l}$ ì„ ê°€í•©ë‹ˆë‹¤. | ë¹¨ê°„ìƒ‰ ìˆ˜ì‹ ë¸”ëŸ­ : íƒ€ê²Ÿ ë…¸ë“œì˜ ì„ë² ë”©ì„ ì—…ë°ì´íŠ¸í•  ë•Œ ì´ì›ƒ ë…¸ë“œ ë¿ ì•„ë‹ˆë¼, ì´ì „ layerì—ì„œ ê°–ê³  ìˆë˜ ìê¸° ìì‹ ì˜ ì„ë² ë”©ë„ í™œìš©í•©ë‹ˆë‹¤. ê°™ì€ ë°©ë²•ìœ¼ë¡œ $h_{v}^{(l)}$ì— ì–´ë– í•œ transformation $B_{l}$ ì„ ê°€í•©ë‹ˆë‹¤. | ë…¸ë€ìƒ‰ ìˆ˜ì‹ ë¸”ëŸ­ : ìµœì¢…ì ìœ¼ë¡œ ë¹„ì„ í˜• í•¨ìˆ˜ë¥¼ ì ìš©í•´ì„œ $l+1$ë²ˆì§¸ layerì—ì„œì˜ íƒ€ê²Ÿ ë…¸ë“œ ì„ë² ë”©ì„ êµ¬í•©ë‹ˆë‹¤. | ë³´ë¼ìƒ‰ ìˆ˜ì‹ ë¸”ëŸ­ : ë…¸ë“œ ì„ë² ë”© ì—…ë°ì´íŠ¸ ê³¼ì •ì„ $L$ë²ˆ ë°˜ë³µí•©ë‹ˆë‹¤. ì´ ë•Œ ìµœì¢…ì ìœ¼ë¡œ í˜•ì„±ëœ ë…¸ë“œ ì„ë² ë”©ì€ ë³¸ì¸ìœ¼ë¡œë¶€í„° L-hop ë–¨ì–´ì§„ ë…¸ë“œì˜ ì •ë³´ê¹Œì§€ í™œìš©í•˜ì—¬ ë§Œë“  ê²ƒì…ë‹ˆë‹¤. | Matrix Formulation . ì´ì „ì— Random Walkë¥¼ í–‰ë ¬ í˜•íƒœë¡œ í‘œí˜„í–ˆë“¯ì´, ë²¡í„°ì‹ìœ¼ë¡œ ë‹¤ë¤˜ë˜ GNNë„ í–‰ë ¬ì‹ìœ¼ë¡œ ë‹¤ì‹œ í‘œí˜„í•´ë³´ê² ìŠµë‹ˆë‹¤. . ëª¨ë“  ë…¸ë“œ ì„ë² ë”© ë²¡í„°ë¥¼ í•œë° ëª¨ì•„ ë…¸ë“œ ì„ë² ë”© í–‰ë ¬ì„ ë§Œë“ ë‹¤ë©´, ì´ëŠ” ì•„ë˜ì™€ ê°™ì´ í‘œí˜„í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. . $H^{L} = {[h_{1}^{(l)} â€¦ h_{|V|}^{(l)}]}^T$ . ê·¸ë ‡ë‹¤ë©´ ì´ì›ƒ ë…¸ë“œì˜ ì„ë² ë”©ì„ í•©ì‚°í•˜ëŠ” ê³¼ì •ì€ ê·¸ë˜í”„ì˜ ì¸ì ‘ í–‰ë ¬ì„ ì‚¬ìš©í•˜ì—¬ ì•„ë˜ì™€ ê°™ì´ ê°„ë‹¨í•˜ê²Œ ë‚˜íƒ€ë‚¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤. . $ sum_{u in N_{v}} h_{u}^{(l)} = A_{v:}H^{(l)}$ . ë§Œì•½ ëŒ€ê° í–‰ë ¬ì„ ì´ë ‡ê²Œ ì •ì˜í•œë‹¤ë©´, ì´ ëŒ€ê° í–‰ë ¬ì˜ ì—­í–‰ë ¬ì€ ë‹¤ìŒê³¼ ê°™ê¸° ë•Œë¬¸ì—, . $D_{v,v} = Deg(v) = |N(v)|$ , $D_{v,v}^{-1} = 1/|N(v)|$ . ì´ë¥¼ í™œìš©í•˜ë©´ ì´ì›ƒ ë…¸ë“œì˜ ì„ë² ë”©ì„ í‰ê· ë‚´ëŠ” ì—°ì‚°ì„ í–‰ë ¬ì‹ìœ¼ë¡œ ê°„ë‹¨í•˜ê²Œ í‘œí˜„í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. . . ë”°ë¼ì„œ ìµœì¢…ì ìœ¼ë¡œ ë…¸ë“œ ì„ë² ë”© ì—…ë°ì´íŠ¸ í•¨ìˆ˜ë¥¼ í–‰ë ¬ì‹ìœ¼ë¡œ ë‚˜íƒ€ë‚´ë©´ ì•„ë˜ì™€ ê°™ìŠµë‹ˆë‹¤. . . ë²¡í„°ì‹ìœ¼ë¡œ ì´í•´í•˜ê¸°ë„ ì–´ë ¤ì› ëŠ”ë° ì™œ êµ³ì´ ì‚¬ì„œ í–‰ë ¬ì‹ìœ¼ë¡œ ë³€í™˜í•˜ëƒê³ ìš”? ì‚¬ì‹¤ í–‰ë ¬ì‹ì´ ê°€ì§€ëŠ” êµ¬í˜„ìƒì˜ ì´ì ì´ ìˆê¸° ë•Œë¬¸ì…ë‹ˆë‹¤. í–‰ë ¬ì‹ì„ ì‚¬ìš©í•œë‹¤ë©´ ê° ë…¸ë“œì— ëŒ€í•œ ì„ë² ë”©ì„ ë”°ë¡œ ë”°ë¡œ ì—…ë°ì´íŠ¸ í•˜ì§€ ì•Šê³  í•˜ë‚˜ì˜ í–‰ë ¬ë¡œì¨ í•œë²ˆì— ì—…ë°ì´íŠ¸ í•  ìˆ˜ ìˆìœ¼ë©°, ì´ ê³¼ì •ì—ì„œ $ tilde{A}$ê°€ í¬ì†Œ í–‰ë ¬ì´ê¸° ë•Œë¬¸ì— ë³´ë‹¤ ë” íš¨ìœ¨ì ì¸ í–‰ë ¬ ì—°ì‚°ì´ ê°€ëŠ¥í•˜ê¸° ë•Œë¬¸ì— êµ¬í˜„í•  ë•ŒëŠ” í–‰ë ¬ì‹ì´ ë” ì„ í˜¸ë©ë‹ˆë‹¤. . How to train a GNN . ì˜¤ëŠ˜ ê°•ì˜ì˜ ë§ˆì§€ë§‰ ë¶€ë¶„ìœ¼ë¡œ ì´ë ‡ê²Œ êµ¬ì„±í•œ GNNì„ ì–´ë–»ê²Œ í•™ìŠµì‹œì¼œì•¼ í•˜ëŠ”ì§€ ì•Œì•„ë³´ê² ìŠµë‹ˆë‹¤. ì‹œì‘í•˜ê¸°ì— ì•ì„œ í•™ìŠµì˜ ëŒ€ìƒì´ ë˜ëŠ” íŒŒë¼ë¯¸í„°ê°€ ë¬´ì—‡ì¸ì§€ ì§šê³  ë„˜ì–´ê°€ë³´ì£ . . . ë‹¤ìŒ ì‹ì—ì„œ í•™ìŠµë˜ëŠ” íŒŒë¼ë¯¸í„°ëŠ” $W_{l}$ì™€ $B_{l}$ì…ë‹ˆë‹¤. ë”¸ë¦° subscriptë¥¼ ë´ë„ ì•Œ ìˆ˜ ìˆë“¯ì´, ë‘ íŒŒë¼ë¯¸í„° í–‰ë ¬ì€ ëª¨ë‘ layerë§ˆë‹¤ ë”°ë¡œ ì¡´ì¬í•˜ë©°, í•œ layer ë‚´ì—ì„œëŠ” ê³µìœ ë©ë‹ˆë‹¤. ì´ë¥¼ ê°„ë‹¨í•˜ê²Œ ê·¸ë¦¼ìœ¼ë¡œ í‘œí˜„í•˜ë©´ ì•„ë˜ì™€ ê°™ìŠµë‹ˆë‹¤. . . GNNì„ í•™ìŠµí•˜ëŠ” ë°©ë²•ì€ ì—¬ëŠ ë”¥ëŸ¬ë‹ í•™ìŠµê³¼ ë§ˆì°¬ê°€ì§€ë¡œ í¬ê²Œ ì§€ë„ í•™ìŠµ, ë¹„ì§€ë„ í•™ìŠµ ì„¸íŒ…ìœ¼ë¡œ ë‚˜ë‰©ë‹ˆë‹¤. ì´ë¥¼ í•˜ë‚˜ì”© ì‚´í´ë³´ë„ë¡ í•©ì‹œë‹¤. . ì§€ë„ í•™ìŠµ ì„¸íŒ… ë…¸ë“œ label $y$ê°€ ì¡´ì¬í•˜ëŠ” ìƒí™© | ì •ë‹µ ë…¸ë“œ label $y$ë¥¼ í™œìš©í•˜ì—¬ $min_{ theta} mathcal{L}(y,f(z_v))$ë¥¼ í’‚, ì´ ë•Œ taskì— ë§ê²Œ L2 loss í˜¹ì€ Cross entropy loss ë“±ì„ loss í•¨ìˆ˜ë¥¼ ì‚¬ìš©í•¨ | ì˜ˆì‹œ) ê° ë…¸ë“œê°€ safeí•œì§€ í˜¹ì€ toxicí•œì§€ ë¶„ë¥˜í•˜ëŠ” node classification â†’ ë¶„ë¥˜ ë¬¸ì œì´ë¯€ë¡œ cross-entropy lossë¥¼ ì‚¬ìš©í•˜ê³ , ë…¸ë“œì˜ ì •ë‹µ í´ë˜ìŠ¤ labelì„ í™œìš©í•˜ì—¬ ì§ì ‘ ëª¨ë¸ í•™ìŠµ ê°€ëŠ¥ . ğŸ’¡ ì•„ë˜ loss ì‹ì—ì„œ $ sigma(z_v^T theta)$ëŠ” í•™ìŠµëœ ë…¸ë“œ ì„ë² ë”© $z_v^T$ë¥¼ ê°€ì§€ê³  ëª¨ë¸ì´ ì˜ˆì¸¡í•œ ë…¸ë“œ $v$ì˜ í´ë˜ìŠ¤ í™•ë¥ ì„ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤. | . . . | ë¹„ì§€ë„ í•™ìŠµ ì„¸íŒ… ë…¸ë“œ labelì´ ì¡´ì¬í•˜ì§€ ì•ŠëŠ” ìƒí™© | 3ê°•ì—ì„œ ê³µë¶€í–ˆë˜ ê·¸ë˜í”„ ìƒ ë…¸ë“œ similarityë¥¼ supervisionìœ¼ë¡œ í™œìš© . . ì„ì˜ì˜ supervision ì‹œê·¸ë„ì„ ë§Œë“¤ì–´ ë¹„ì§€ë„ í•™ìŠµ ì„¸íŒ…ì„ ì§€ë„ í•™ìŠµ ì„¸íŒ…ìœ¼ë¡œ ë°”ê¾¸ê¸° ìœ„í•´ì„œ ì›ë³¸ ê·¸ë˜í”„ì—ì„œì˜ ë…¸ë“œ similarityë¥¼ ë°”íƒ•ìœ¼ë¡œ labelì„ ì§€ì •í•´ì¤ë‹ˆë‹¤. ì—¬ê¸°ì„œ ë…¸ë“œ similarityëŠ” 3ê°•ì—ì„œ ë‹¤ë¤˜ë˜ Random Walk ë“±ì„ í™œìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. . ê°„ë‹¨í•˜ê²Œ DeepWalkë¡œ ë…¸ë“œ similarityë¥¼ ì •ì˜í•˜ëŠ” ê²½ìš°ë¥¼ ìƒê°í•´ë³¼ê¹Œìš”? ë§Œì•½ ë‘ ë…¸ë“œ $u$ì™€ $v$ê°€ ëœë¤ ì›Œí¬ ìƒì—ì„œ co-occurí•œë‹¤ë©´ ë‘ ë…¸ë“œëŠ” â€˜similarâ€™í•˜ë‹¤ê³  ë§í•  ìˆ˜ ìˆìœ¼ë©°, $y_{u,v} = 1$ë¡œ ì„ì˜ì˜ labelì„ ë¶™ì…ë‹ˆë‹¤. ë˜í•œ ì—¬ê¸°ì„œ $DEC(z_u,z_v)$ëŠ” í•™ìŠµëœ ë‘ ë…¸ë“œì˜ ì„ë² ë”©ì„ ë‚´ì í•¨ìœ¼ë¡œì¨ ì„ë² ë”© ê³µê°„ì—ì„œì˜ ë…¸ë“œ similarityë¥¼ ì¸¡ì •í•©ë‹ˆë‹¤. Loss í•¨ìˆ˜ë¡œ cross-entropyë¥¼ ì‚¬ìš©í•¨ìœ¼ë¡œì¨ ê·¸ë˜í”„ì—ì„œ ë…¸ë“œ similarityë¥¼ ìµœëŒ€í•œ ì˜ ë³´ì¡´í•˜ë„ë¡ ë…¸ë“œ ì„ë² ë”©ì„ í•™ìŠµí•  ìˆ˜ ìˆê²Œ ë©ë‹ˆë‹¤. . ğŸ’¡ ì›ë³¸ ê·¸ë˜í”„ì—ì„œ similarí•œ ë…¸ë“œëŠ” â†’ similarí•œ ì„ë² ë”©ì„ ê°–ë„ë¡ í•©ë‹ˆë‹¤ . | . | Model Design: Overview . ì ê·¸ëŸ¼, ì˜¤ëŠ˜ ë°°ì› ë˜ ë‚´ìš©ì„ í•œë²ˆ ì­‰ í›‘ì–´ ì •ë¦¬í•˜ê³  í¬ìŠ¤íŠ¸ë¥¼ ë§ˆë¬´ë¦¬í•˜ë„ë¡ í•˜ê² ìŠµë‹ˆë‹¤. ê·¸ë˜í”„ë¥¼ ìœ„í•œ Deep Encoder, a.k.a. GNN ëª¨ë¸ì„ ë§Œë“¤ê¸° ìœ„í•´ì„œ ì•„ë˜ì™€ ê°™ì€ ë‹¨ê³„ë¥¼ ë”°ë¼ê°€ì•¼ í•©ë‹ˆë‹¤. . ì´ì›ƒ ë…¸ë“œ ì„ë² ë”©ì„ aggregateí•˜ëŠ” í•¨ìˆ˜ë¥¼ ì •í•¨ | Taskì˜ íŠ¹ì„±ì— ë§ì¶”ì–´ loss í•¨ìˆ˜ë¥¼ ì •ì˜í•¨ | . . ì—¬ëŸ¬ computation graphì— ëŒ€í•´ GNN ëª¨ë¸ì„ í•™ìŠµì‹œí‚´ | í•™ìŠµëœ ëª¨ë¸ì„ ê°–ê³  ë…¸ë“œì— ëŒ€í•œ ì„ë² ë”©ì„ ìƒì„±í•  ìˆ˜ ìˆìŒ. ì´ ë•Œ, ëª¨ë“  ë…¸ë“œì— ëŒ€í•´ ë‰´ëŸ´ ë„¤íŠ¸ì›Œí¬ì˜ íŒŒë¼ë¯¸í„°ê°€ ê³µìœ ë˜ê¸° ë•Œë¬¸ì— í•™ìŠµì— ì‚¬ìš©ë˜ì§€ ì•Šì€ ë…¸ë“œì— ëŒ€í•´ì„œë„ ì„ë² ë”©ì„ ìƒì„±í•  ìˆ˜ ìˆìŒ (Inductive Capability) | . . ğŸ’¡ Inductive Capability 1. ìƒˆë¡œìš´ ê·¸ë˜í”„: ì˜ˆë¥¼ ë“¤ì–´ ë¶„ì ê·¸ë˜í”„ì—ì„œ, í™”í•©ë¬¼ Aì— ëŒ€í•´ í•™ìŠµëœ GNN ëª¨ë¸ì´ í™”í•©ë¬¼ B ê·¸ë˜í”„ì—ì„œ ì„ë² ë”©ì„ ë§Œë“œëŠ”ë° í™œìš©ë  ìˆ˜ ìˆìŒ 2. ìƒˆë¡œìš´ ë…¸ë“œ: ì‹œê°„ì— ë”°ë¼ evolving í•˜ëŠ” ê·¸ë˜í”„ì—ì„œ ìƒˆë¡œìš´ ë…¸ë“œê°€ ì¶”ê°€ë  ë•Œë§ˆë‹¤ ë°”ë¡œë°”ë¡œ ì„ë² ë”©ì„ ìƒì„±í•  ìˆ˜ ìˆìŒ Summary . Deep Encoder (GNN)ì˜ í•µì‹¬ ì•„ì´ë””ì–´: ì´ì›ƒ ë…¸ë“œì˜ ì •ë³´ë¥¼ aggregate í•¨ìœ¼ë¡œì¨ ë…¸ë“œ ì„ë² ë”©ì„ ìƒì„±í•˜ì! | ëª¨ë¸ ë‚´ Aggregatorê³¼ Transformation í•¨ìˆ˜ë¥¼ ê°ê° ì–´ë–»ê²Œ ì •ì˜í•˜ëŠëƒì— ë”°ë¼ ëª¨ë¸ êµ¬ì¡°ê°€ ë‹¬ë¼ì§ˆ ìˆ˜ ìˆìŠµë‹ˆë‹¤. | ë‹¤ìŒ ê°•ì˜ì—ì„  GNN variantì˜ í•˜ë‚˜ì¸ GraphSAGEë¥¼ ë‹¤ë£° ê²ƒì…ë‹ˆë‹¤. | . .",
            "url": "https://cs224w-kor.github.io/blog/graph%20neural%20network/gnn/graph%20convolution%20network/gcn/2022/07/27/lecture-0603.html",
            "relUrl": "/graph%20neural%20network/gnn/graph%20convolution%20network/gcn/2022/07/27/lecture-0603.html",
            "date": " â€¢ Jul 27, 2022"
        }
        
    
  
    
        ,"post8": {
            "title": "Lecture 6.2 - Basics of Deep Learning",
            "content": ". Lecture 6. Graph Neural Networks (1) GNN Model . Lecture 6.1 - Introduction to Graph Neural Networks | Lecture 6.2 - Basics of Deep Learning | Lecture 6.3 - Deep Learning for Graphs | . . Lecture 6.2 - Basics of Deep Learning . ì´ë²ˆ íŒŒíŠ¸ëŠ” ë³¸ê²©ì ìœ¼ë¡œ ê·¸ë˜í”„ ë°ì´í„°ë¥¼ ìœ„í•œ ë”¥ëŸ¬ë‹ì„ ì„¤ëª…í•˜ê¸°ì— ì•ì„œ ë”¥ëŸ¬ë‹ì— ìµìˆ™í•˜ì§€ ì•Šì€ ì‚¬ëŒë“¤ì„ ìœ„í•´ ë”¥ëŸ¬ë‹ì˜ ê¸°ë³¸ ê°œë…ì„ ê°„ë‹¨í•˜ê²Œ ì„¤ëª…í•˜ëŠ” ë¶€ë¶„ì…ë‹ˆë‹¤. ë§ì€ ë‚´ìš©ì„ ì»¤ë²„í•˜ê¸´ í•˜ì§€ë§Œ ë”¥ëŸ¬ë‹ ì´ˆì‹¬ìë“¤ì€ CS231nê³¼ ê°™ì€ ë”¥ëŸ¬ë‹ ê°•ì˜ë¥¼ ë¨¼ì € ìˆ˜ê°•í•˜ê³  ì˜¤ì‹¬ì„ ì¶”ì²œë“œë¦½ë‹ˆë‹¤! . Machine Learning as Optimization . ë¨¼ì € ê°„ë‹¨í•œ ì§€ë„ í•™ìŠµ ë¬¸ì œë¥¼ ìƒê°í•´ ë´…ì‹œë‹¤. ì§€ë„ í•™ìŠµ(Supervised Learning)ì´ë€ ë°ì´í„°ì— ëŒ€í•œ ground truth labelì´ ì¡´ì¬í•˜ëŠ” ê²½ìš°ë¥¼ ì¼ì»«ëŠ”ë°, ë‹¤ì‹œ ë§í•´ input $x$ê°€ ì£¼ì–´ì¡Œì„ ë•Œ, label $y$ë¥¼ ì˜ˆì¸¡í•˜ëŠ” ë¬¸ì œë¼ê³  í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì´ëŸ¬í•œ taskëŠ” ì•„ë˜ì™€ ê°™ì€ ì‹ì„ í†µí•´ ìµœì í™” ë¬¸ì œë¡œ ë°”ê¾¸ì–´ í•´ê²°í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. . . ìœ„ ì‹ì„ ì´í•´í•˜ê¸° ìœ„í•´ ë¨¼ì € êµ¬ì„± ìš”ì†Œì— ëŒ€í•´ í•˜ë‚˜ì”© ì§šì–´ë³´ê² ìŠµë‹ˆë‹¤. . $ theta$ : ìš°ë¦¬ê°€ ìµœì í™”(í•™ìŠµ) í•˜ë ¤ëŠ” íŒŒë¼ë¯¸í„°ë“¤ . ìµœì¢…ì ìœ¼ë¡œ ìš°ë¦¬ê°€ í•™ìŠµí•˜ê³ ì í•˜ëŠ” íŒŒë¼ë¯¸í„° ê°’ì…ë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´ ì•ì˜ Shallow Encoderì—ì„œëŠ” í•™ìŠµìœ¼ë¡œ ê²°ì •ë˜ëŠ” $|V| times d$ ì‚¬ì´ì¦ˆì˜ ì„ë² ë”© look-up tableì´ $ theta$ì— í•´ë‹¹í•˜ê² ì£ . . $ mathcal{L}$ : Loss í•¨ìˆ˜ . Loss í•¨ìˆ˜ëŠ” ground truth label $y$ì™€ ë¨¸ì‹ ëŸ¬ë‹ ëª¨ë¸ì´ ì˜ˆì¸¡í•œ label $f(x)$ì˜ ì°¨ì´ë¥¼ ê³„ì‚°í•˜ëŠ” metric ì…ë‹ˆë‹¤. íšŒê·€(Regression) ë¬¸ì œì—ì„œ ì£¼ë¡œ ì‚¬ìš©ë˜ëŠ” L2 lossì™€ ë¶„ë¥˜(Classification) ë¬¸ì œì—ì„œ ì£¼ë¡œ ì‚¬ìš©ë˜ëŠ” Cross entropy loss ì´ì™¸ì—ë„ L1 loss, Huber loss, Hinge loss ë“± ë‹¤ì–‘í•œ loss í•¨ìˆ˜ê°€ ì¡´ì¬í•©ë‹ˆë‹¤. ëŒ€í‘œì ì¸ loss í•¨ìˆ˜ì¸ L2 lossì™€ Cross entropy lossì˜ ìˆ˜ì‹ì€ ê°ê° ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤. . L2 loss . . Cross entropy loss . . . ê²°êµ­ ìš°ë¦¬ê°€ ì›í•˜ëŠ” ê²ƒì€ ëª¨ë¸ì´ ì˜ˆì¸¡í•œ labelì´ ì •ë‹µ ground truth labelì— ê°€ê¹Œì›Œì§€ëŠ” ê²ƒì´ê¸° ë•Œë¬¸ì—, ì´ loss í•¨ìˆ˜ ê°’ì´ ì‘ìœ¼ë©´ ì‘ì„ìˆ˜ë¡ ìš°ë¦¬ì˜ ëª¨ë¸ì´ ë” ì •í™•í•œ ì˜ˆì¸¡ì„ í•œë‹¤ëŠ” ì˜ë¯¸ì…ë‹ˆë‹¤. . . ê·¸ëŸ¼ ì´ì œ ìœ„ì—ì„œ ê³µë¶€í•œ ê° êµ¬ì„± ìš”ì†Œë¥¼ ë°”íƒ•ìœ¼ë¡œ ë‹¤ì‹œ ì´ ìµœì í™” ì‹ì„ í•´ì„í•´ ë´…ì‹œë‹¤. ê²°êµ­ ìš°ë¦¬ê°€ í’€ê³ ì í•˜ëŠ” ë¨¸ì‹ ëŸ¬ë‹ ë¬¸ì œëŠ”, ì •ë‹µ label $y$ì™€ ëª¨ë¸ì´ ì˜ˆì¸¡í•œ label $f(x)$ì˜ ì°¨ì´ë¥¼ ë‚˜íƒ€ë‚´ëŠ” loss í•¨ìˆ˜ë¥¼ ìµœì†Œí™” í•˜ëŠ” ë°©í–¥ìœ¼ë¡œ ëª¨ë¸ íŒŒë¼ë¯¸í„° $ theta$ë¥¼ ìµœì í™”í•˜ëŠ” ë¬¸ì œë¡œ í•´ì„í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. . Gradient Descent . ì§€ê¸ˆê¹Œì§„ ë‘ë£¨ë­‰ìˆ í•˜ê²Œë§Œ ë³´ì˜€ë˜ ë¨¸ì‹ ëŸ¬ë‹ ë¬¸ì œë¥¼ ë™ì¼í•œ ì˜ë¯¸ì¸ ìµœì í™” ë¬¸ì œë¡œ ì¬ì •ì˜í–ˆìŠµë‹ˆë‹¤. ê·¸ë ‡ë‹¤ë©´ ì´ ìµœì í™” ë¬¸ì œë¥¼ ì–´ë–»ê²Œ í•´ê²°í•´ì•¼ í• ê¹Œìš”? . . (ì¶œì²˜: https://ieeexplore.ieee.org/abstract/document/9298092) . ìš°ë¦¬ëŠ” ì¼ë°˜ì ìœ¼ë¡œ Loss í•¨ìˆ˜ë¡œ convex function(ë³¼ë¡ í•¨ìˆ˜)ë¥¼ í™œìš©í•©ë‹ˆë‹¤. ì´ loss í•¨ìˆ˜ì˜ ê°’ì´ ì‘ì•„ì§€ëŠ” ë°©í–¥ìœ¼ë¡œ ëª¨ë¸ íŒŒë¼ë¯¸í„° $ theta$ë¥¼ ì—…ë°ì´íŠ¸ í•˜ê¸° ìœ„í•´, $ theta$ì— ëŒ€í•œ $ mathcal{L}$ì˜ ê¸°ìš¸ê¸°ë¥¼ êµ¬í•œ í›„, ê¸°ìš¸ê¸°ê°€ ì‘ì•„ì§€ëŠ” ë°©í–¥ìœ¼ë¡œ $ theta$ë¥¼ ì—…ë°ì´íŠ¸ í•´ì¤ë‹ˆë‹¤. . (ìœ„ ê·¸ë¦¼ì—ì„œ Costë¥¼ $ mathcal{L}$, Weightsë¥¼ $ theta$ë¡œ ë³´ì‹œë©´ ë©ë‹ˆë‹¤!) . . ì´ë¥¼ ë‹¤ì‹œ Gradient ë²¡í„°ë¼ëŠ” ê°œë…ìœ¼ë¡œ ì •ë¦¬í•´ì„œ ì´ì•¼ê¸° í•´ë³´ê² ìŠµë‹ˆë‹¤. Gradient ë²¡í„°ë€ ìœ„ì˜ ì‹ê³¼ ê°™ì´ $ theta$ì— ëŒ€í•œ $ mathcal{L}$ì˜ í¸ë¯¸ë¶„, ì¦‰ ê¸°ìš¸ê¸° ê°’ìœ¼ë¡œ êµ¬ì„±ëœ ë²¡í„°ë¡œì¨, ê°€ì¥ ë¹ ë¥´ê²Œ $ mathcal{L}$ì´ ì¦ê°€í•˜ëŠ” ë°©í–¥ì„ ë‚˜íƒ€ë‚´ëŠ” ë°©í–¥ ë„í•¨ìˆ˜ ë²¡í„°ì…ë‹ˆë‹¤. . ğŸ’¡ Gradient is the directional derivative in the direction of largest increase ì¼ë‹¨ Gradientë¥¼ êµ¬í–ˆìœ¼ë©´ ì´ì œ í•  ì¼ì€ ëª¨ë¸ íŒŒë¼ë¯¸í„° $ theta$ë¥¼ gradientì˜ ë°˜ëŒ€ë°©í–¥ìœ¼ë¡œ($ mathcal{L}$ì´ ì‘ì•„ì§€ëŠ” ë°©í–¥ìœ¼ë¡œ) ë°˜ë³µì ìœ¼ë¡œ ì—…ë°ì´íŠ¸ í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤. . . ìœ„ ì‹ì—ì„œ $ eta$ëŠ” learning rateë¡œ, í•œë²ˆ íŒŒë¼ë¯¸í„°ë¥¼ ì—…ë°ì´íŠ¸ ì‹œ ì–¼ë§ˆë‚˜ ë³€ê²½í•  ê²ƒì¸ì§€ ë³´í­ì„ ë‚˜íƒ€ë‚´ëŠ” ê°’ì…ë‹ˆë‹¤. ì´ëŠ” í•™ìŠµ ê³¼ì • ë™ì•ˆ ë™ì¼í•˜ê²Œ ìœ ì§€í•  ìˆ˜ë„ ìˆê³ , ëª©ì ì— ë”°ë¼ ê³„ì† ë³€í•˜ê²Œ í•  ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤. ì´ë¡ ì ìœ¼ë¡œ ëª¨ë¸ íŒŒë¼ë¯¸í„°ì˜ ì—…ë°ì´íŠ¸ëŠ” loss í•¨ìˆ˜ì˜ local minimumì— ë„ë‹¬í•˜ì—¬ gradientê°€ 0ì´ ë  ë•Œê¹Œì§€ ì§„í–‰í•˜ëŠ” ê²ƒì´ ë§ì§€ë§Œ, ì‹¤ì „ì—ì„œëŠ” ê²€ì¦ ë°ì´í„°ì…‹ì—ì„œì˜ ì„±ëŠ¥ì´ ë” ì´ìƒ ì¦ê°€í•˜ì§€ ì•ŠëŠ” ê¸°ì ì—ì„œ ëª¨ë¸ íŒŒë¼ë¯¸í„° ì—…ë°ì´íŠ¸ë¥¼ ì¤‘ë‹¨í•©ë‹ˆë‹¤. . Stochastic Gradient Descent (SGD) . Gradient Descentì˜ ë¬¸ì œì  . Gradient descent ë°©ë²•ìœ¼ë¡œ ìµœì í™” ë¬¸ì œë¥¼ í‘¸ëŠ” ê²ƒì€ ì´ë¡ ì ìœ¼ë¡  ë¬´ê²°í•˜ì§€ë§Œ í˜„ì‹¤ì ìœ¼ë¡œëŠ” ì–´ë µìŠµë‹ˆë‹¤. ì•ì„œ ì–¸ê¸‰í–ˆë“¯ì´, Gradient ë²¡í„°ë¥¼ ê³„ì‚°í•˜ê¸° ìœ„í•´ì„œëŠ” ì „ì²´ ë°ì´í„°ì— ëŒ€í•œ loss ê°’ì„ êµ¬í•´ì•¼ í•˜ê¸° ë•Œë¬¸ì— ëª‡ì‹­ì–µê°œì˜ ë°ì´í„°ë¥¼ ê°–ëŠ” ì˜¤ëŠ˜ë‚ ì˜ ë°ì´í„°ì…‹ì— ì ìš©í•˜ê¸°ì—ëŠ” ê³„ì‚°ì ìœ¼ë¡œ ë¬´ë¦¬ê°€ ìˆìŠµë‹ˆë‹¤. . . (ì¶œì²˜: https://www.slideshare.net/w0ong/ss-82372826) . ë”°ë¼ì„œ ì „ì²´ ë°ì´í„°ì…‹ì„ ì‘ì€ ë¯¸ë‹ˆ ë°°ì¹˜ë¡œ ë‚˜ëˆ„ì–´ ëª¨ë¸ íŒŒë¼ë¯¸í„°ë¥¼ ì—…ë°ì´íŠ¸í•˜ëŠ” SGD ë°©ë²•ì´ ë“±ì¥í•˜ê²Œ ë˜ì—ˆìŠµë‹ˆë‹¤. ë¯¸ë‹ˆ ë°°ì¹˜ ë§ˆë‹¤ gradientë¥¼ êµ¬í•˜ê³  ëª¨ë¸ íŒŒë¼ë¯¸í„°ë¥¼ ì—…ë°ì´íŠ¸í•˜ëŠ” ê²ƒì´ ì „ì²´ ë°ì´í„°ì…‹ì„ í™œìš©í•œ ëª¨ë¸ ì—…ë°ì´íŠ¸ ì •ë„ë¥¼ ê·¼ì‚¬í•  ìˆ˜ ìˆê¸° ë•Œë¬¸ì´ì£ . . ğŸ’¡ SGD is unbiased estimator of full gradient ì˜¤ëŠ˜ë‚ ì˜ ìµœì í™” optimizerì€ SGDì˜ ì—¬ëŸ¬ ë°œì „ëœ í˜•íƒœë¡œì¨, Adam, Adagrad, Adadelta, RMSprop ë“±ì´ ìˆìŠµë‹ˆë‹¤. . Back-propagation . ì§€ê¸ˆê¹Œì§€ ë¨¸ì‹ ëŸ¬ë‹ ë¬¸ì œë¥¼ ìµœì í™” ë¬¸ì œë¡œ ì¬ì •ì˜í•˜ê³ , ìµœì í™” ë¬¸ì œë¥¼ í’€ê¸° ìœ„í•´ gradientë¥¼ í™œìš©í•´ ëª¨ë¸ íŒŒë¼ë¯¸í„°ë¥¼ ì—…ë°ì´íŠ¸ í•˜ëŠ” ë°©ë²•ì— ëŒ€í•´ì„œ ë°°ì› ìŠµë‹ˆë‹¤. ì˜ ë”°ë¼ì˜¤ê³  ê³„ì‹ ê°€ìš”? . ì´ì œë¶€í„°ëŠ” ì‹¤ì§ˆì ìœ¼ë¡œ ë¨¸ì‹ ëŸ¬ë‹ ëª¨ë¸ì´ ì£¼ì–´ì¡Œì„ ë•Œ, gradientë¥¼ ê³„ì‚°í•˜ëŠ” ë°©ë²•ì— ëŒ€í•´ ì•Œì•„ë³´ê² ìŠµë‹ˆë‹¤. ì˜¤ëŠ˜ë‚ ì˜ ë¨¸ì‹ ëŸ¬ë‹/ë”¥ëŸ¬ë‹ ëª¨ë¸ì€ ì•„ì£¼ ë³µì¡í•œ í˜•íƒœë¥¼ ê°€ì§€ì§€ë§Œ, ì¼ë‹¨ ì´í•´ì˜ í¸ì˜ë¥¼ ë•ê¸° ìœ„í•´ ê°€ì¥ ê°„ë‹¨í•œ linear functionë¥¼ ì˜ˆì‹œë¡œ ì„¤ëª…í•˜ê² ìŠµë‹ˆë‹¤. . Case 1 . . ì²«ë²ˆì§¸ë¡œ ìš°ë¦¬ì˜ ë¨¸ì‹ ëŸ¬ë‹ ëª¨ë¸ì´ ë‹¨ìˆœí•œ ì„ í˜• ë³€í™˜ í•¨ìˆ˜ì¸ ê²½ìš°ë¥¼ ë‹¤ë¤„ë³´ê² ìŠµë‹ˆë‹¤. . . ì„ í˜• ë³€í™˜ í•¨ìˆ˜ê°€ ë²¡í„° í˜•íƒœì´ë“  í–‰ë ¬ í˜•íƒœì´ë“  ê´€ê³„ ì—†ì´ gradientëŠ” ì´ë ‡ê²Œ ê°„ë‹¨íˆ êµ¬í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. . Case 2 . . . ë‘ë²ˆì§¸ë¡œ ì¡°ê¸ˆ ë” ë³µì¡í•œ í˜•íƒœì˜ ëª¨ë¸ë¡œ í™•ì¥í•´ë³´ê² ìŠµë‹ˆë‹¤. (ë¬¼ë¡  ì•„ì§ ì—„ì²­ ë‹¨ìˆœí•œ í˜•íƒœê¸´ í•˜ì§€ë§Œ..) ì´ ê²½ìš°ì—ëŠ” $W_{1}$ê³¼ $W_{2}$ì— ëŒ€í•´ ëª¨ë‘ gradientë¥¼ êµ¬í•´ì•¼ í•©ë‹ˆë‹¤. ì—¬ê¸°ì„œ ìš°ë¦¬ê°€ ê³ ë“±í•™êµ ë•Œ ë°°ìš´ í•©ì„±í•¨ìˆ˜ ë¯¸ë¶„ì—ì„œì˜ ì—°ì‡„ë²•ì¹™ì´ ì‚¬ìš©ë©ë‹ˆë‹¤. . ğŸ’¡ Chain Rule (ì—°ì‡„ë²•ì¹™) ![Imgur](https://i.imgur.com/IqvyTZ1.png?1) . . . ì—°ì‡„ë²•ì¹™ì— ë”°ë¼ gradientê°€ $ mathcal{L}$ â†’ $f(x)$ â†’ $W_{2}$ â†’ $W_{1}$ ì„ ë”°ë¼ ì°¨ë¡€ë¡œ ê±°ê¾¸ë¡œ íë¥´ë©´ì„œ ê³„ì‚°ë©ë‹ˆë‹¤. ì´ë ‡ê²Œ ë§ë‹¨ì—ì„œë¶€í„° ì•ìª½ê¹Œì§€ gradientê°€ í˜ëŸ¬ì˜¤ê¸° ë•Œë¬¸ì— ì—­ì „íŒŒ(back-propagation)ì´ë¼ëŠ” ì´ë¦„ì´ ë¶™ì—ˆë‹¤ê³  í•©ë‹ˆë‹¤. . Non-linearity . ì§€ê¸ˆê¹Œì§€ ì˜ˆì‹œë¡œì¨ ë‹¤ë¤„ë³¸ ë‘ ì¼€ì´ìŠ¤ëŠ” ì‚¬ì‹¤ ëª¨ë‘ ì„ í˜• ëª¨ë¸ë¡œì¨, ë¹„ì„ í˜•ì ì¸ ë°ì´í„°ë¥¼ ì˜ ëª¨ë¸ë§í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë”°ë¼ì„œ ì˜¤ëŠ˜ë‚ ì˜ ë¨¸ì‹ ëŸ¬ë‹ ëª¨ë¸ì€ ë¹„ì„ í˜•ì ì¸ í™œì„± í•¨ìˆ˜(Activation function)ë¥¼ ë„ì…í•¨ìœ¼ë¡œì¨ ì´ëŸ¬í•œ ë¬¸ì œë¥¼ í•´ê²°í•©ë‹ˆë‹¤. ëŒ€í‘œì ì¸ ë¹„ì„ í˜• í•¨ìˆ˜ë¡œëŠ” ReLU, Sigmoid ë“±ì´ ìˆìŠµë‹ˆë‹¤. . . Multi-layer Perceptron (MLP) . . MLPë€ í•œ layerë§ˆë‹¤ ì„ í˜• ë³€í™˜ê³¼ ë¹„ì„ í˜• ë³€í™˜ì´ í•©ì³ì§„ ê°€ì¥ ê¸°ë³¸ì ì¸ í˜•íƒœì˜ ë¨¸ì‹ ëŸ¬ë‹ ëª¨ë¸ì´ë¼ê³  ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ìœ„ ì‹ì€ MLP í•œ layerì„ ë‚˜íƒ€ë‚´ëŠ”ë°, layer $l$ ì˜ ì¸í’‹ìœ¼ë¡œ ë“¤ì–´ì˜¨ $x^{(l)}$ì— $W_{l}$ì´ ê³±í•´ì ¸ ì„ í˜• ë³€í™˜ ëœ í›„ bias í•­ì´ ë”í•´ì§‘ë‹ˆë‹¤. ìµœì¢…ì ìœ¼ë¡œ ë¹„ì„ í˜• í•¨ìˆ˜ë¥¼ ê±°ì¹œ ì•„ì›ƒí’‹ì´ layer $l+1$ì˜ ì¸í’‹ìœ¼ë¡œ ì „ë‹¬ë©ë‹ˆë‹¤. ì´ë¥¼ ê·¸ë¦¼ìœ¼ë¡œ ë‚˜íƒ€ë‚´ë©´ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤. . . Summary . ì§€ê¸ˆê¹Œì§€ ê°„ë‹¨í•˜ê²Œ ë°°ìš´ ë”¥ëŸ¬ë‹ì˜ ê¸°ë³¸ ê°œë…ì„ ì •ë¦¬í•´ë³´ê³ , ë³¸ê²©ì ì¸ ì˜¤ëŠ˜ ê°•ì˜ì˜ ì£¼ì œë¡œ ë„˜ì–´ê°€ê² ìŠµë‹ˆë‹¤. . ë¨¸ì‹ ëŸ¬ë‹ ë¬¸ì œëŠ” ìµœì í™” ë¬¸ì œë¡œ í’€ ìˆ˜ ìˆìŠµë‹ˆë‹¤. | . . ëª¨ë¸ $f$ëŠ” ê°„ë‹¨í•œ ì„ í˜• í•¨ìˆ˜, MLP, ë˜ëŠ” ë‹¤ë¥¸ í˜•íƒœì˜ ì‹ ê²½ë§ì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤. (e.g., ì¶”í›„ì— ë‹¤ë£° GNNë„ ê°€ëŠ¥í•©ë‹ˆë‹¤) | ë¨¼ì € ì „ì²´ ë°ì´í„°ì…‹ì„ ë¯¸ë‹ˆë°°ì¹˜ë¡œ ë‚˜ëˆ„ì–´ ì¸í’‹ $x$ìœ¼ë¡œ ì‚¬ìš©í•©ë‹ˆë‹¤. | ìˆœì „íŒŒ(Forward Propagation): $x$ê°€ ì£¼ì–´ì¡Œì„ ë•Œ loss í•¨ìˆ˜ ê°’ $ mathcal{L}$ êµ¬í•˜ê¸° | ì—­ì „íŒŒ(Backward Propagation): ì—°ì‡„ë²•ì¹™ìœ¼ë¡œ gradient $ nabla_{ theta} mathcal{L}$ êµ¬í•˜ê¸° | SGDë¥¼ í™œìš©í•˜ì—¬ ë°˜ë³µì ìœ¼ë¡œ ëª¨ë¸ íŒŒë¼ë¯¸í„° $ theta$ë¥¼ ìµœì í™”í•©ë‹ˆë‹¤. | . .",
            "url": "https://cs224w-kor.github.io/blog/graph%20neural%20network/gnn/graph%20convolution%20network/gcn/2022/07/27/lecture-0602.html",
            "relUrl": "/graph%20neural%20network/gnn/graph%20convolution%20network/gcn/2022/07/27/lecture-0602.html",
            "date": " â€¢ Jul 27, 2022"
        }
        
    
  
    
        ,"post9": {
            "title": "Lecture 6.1 - Introduction to Graph Neural Networks",
            "content": ". Lecture 6. Graph Neural Networks (1) GNN Model . Lecture 6.1 - Introduction to Graph Neural Networks | Lecture 6.2 - Basics of Deep Learning | Lecture 6.3 - Deep Learning for Graphs | . . Recap: Shallow Encoders . ì´ì „ ê°•ì˜ì—ì„œ ë°°ìš´ ë‚´ìš©ì„ ë‹¤ì‹œ ë– ì˜¬ë ¤ ë´…ì‹œë‹¤. ë‹¤ì–‘í•œ downstream taskë¥¼ ë¨¸ì‹ ëŸ¬ë‹ìœ¼ë¡œ í‘¸ëŠ” ê³¼ì •ì—ì„œ ë¹„ì •í˜• ë°ì´í„°ì¸ ê·¸ë˜í”„ ì¸í’‹ì„ í™œìš©í•˜ê¸° ìœ„í•´ ê·¸ë˜í”„ë¥¼ ì„ë² ë”©í•˜ëŠ” ë°©ë²•ì„ ê³µë¶€í–ˆì—ˆìŠµë‹ˆë‹¤. . Intuition ê·¸ë˜í”„ ìƒì—ì„œ similarí•œ ë…¸ë“œë¼ë¦¬ ì„ë² ë”© ê³µê°„ì—ì„œë„ ê°€ê¹ë„ë¡ ì„ë² ë”© í•˜ì! . . ì„ë² ë”© ê³µê°„ì—ì„œì˜ ë…¸ë“œ ê°„ similarityëŠ” ê°„ë‹¨í•˜ê²Œ ì½”ì‚¬ì¸ ìœ ì‚¬ë„ë¥¼ í†µí•´ êµ¬í•  ìˆ˜ ìˆì§€ë§Œ, 1) ì›ë˜ ê·¸ë˜í”„ ìƒì—ì„œì˜ ë…¸ë“œ ê°„ similarityì™€ 2) ë…¸ë“œë¥¼ ì„ë² ë”© ë²¡í„°ë¡œ mappingí•˜ëŠ” encoderì€ ìš°ë¦¬ê°€ ìƒˆë¡œ ì •ì˜í•´ì•¼ í•©ë‹ˆë‹¤. . Encoder: Shallow Encoder . . ë¨¼ì €, ì¸í’‹ ê·¸ë˜í”„ì˜ ë…¸ë“œë¥¼ dì°¨ì›ì˜ ë²¡í„°ë¡œ ì„ë² ë”©í•˜ê¸° ìœ„í•´ ê°€ì¥ ê°„ë‹¨í•œ look-up table ë°©ì‹ì¸ Shallow Encoderë¥¼ ë‹¤ë¤˜ìŠµë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´, ë…¸ë“œì˜ ê°¯ìˆ˜ê°€ $|V|$ì¸ ê²½ìš° ì„ë² ë”© í–‰ë ¬ì˜ í¬ê¸°ëŠ” $|V| times d$ ê°€ ë©ë‹ˆë‹¤. . | Similarity Function: Random Walk (DeepWalk, Node2vec) ë˜í•œ, Random Walkìƒì—ì„œ co-occur ë˜ëŠ” ë‘ ë…¸ë“œëŠ” ê·¸ë˜í”„ ìƒ similarí•œ ë…¸ë“œë¼ê³  ì •ì˜í•˜ì˜€ìŠµë‹ˆë‹¤. Random Walkì˜ ì „ëµì— ë”°ë¼ ì„œë¡œ ë‹¤ë¥¸ DeepWalkì™€ Node2vec ë“±ì˜ ë°©ë²•ì„ ë°°ì› ë˜ ê²ƒ ê¸°ì–µ ë‚˜ì‹œë‚˜ìš”? . | Limitations of Shallow Encoder . ì´ë ‡ë“¯ Shallow Encoderì„ í†µí•´ì„œë„ ì„±ê³µì ìœ¼ë¡œ ë…¸ë“œì™€ ê·¸ë˜í”„ë¥¼ ì„ë² ë”©í•  ìˆ˜ ìˆì§€ë§Œ, ë‹¤ìŒê³¼ ê°™ì€ í•œê³„ì  ë•Œë¬¸ì— ë³´ë‹¤ ë” ê³ ë„í™”ëœ Encoderë¥¼ ì¬ì •ì˜ í•  í•„ìš”ê°€ ìˆìŠµë‹ˆë‹¤. . $O(|V|)$ íŒŒë¼ë¯¸í„°ê°€ í•„ìš”í•¨ ê·¸ë˜í”„ì˜ í¬ê¸°ê°€ ì»¤ì§€ë©´ ì»¤ì§ˆìˆ˜ë¡, ì¦‰ ë…¸ë“œì˜ ê°¯ìˆ˜ $|V|$ê°€ ì¦ê°€í•¨ì— ë”°ë¼, ì„ë² ë”© í–‰ë ¬ì˜ í¬ê¸°ë„ ì„ í˜•ì ìœ¼ë¡œ ì¦ê°€í•©ë‹ˆë‹¤. ë˜í•œ ê° ë…¸ë“œê°€ ëª¨ë‘ ì„œë¡œ ë‹¤ë¥¸ d ì°¨ì›ì˜ ì„ë² ë”© ë²¡í„°ë¥¼ ê°€ì§€ê¸° ë•Œë¬¸ì— íŒŒë¼ë¯¸í„° ê³µìœ ë„ ì¼ì–´ë‚˜ì§€ ì•ŠìŠµë‹ˆë‹¤. | Transductivity . ğŸ’¡ **Transductive Learning** ê·¸ë˜í”„ í•™ìŠµ ê´€ì ì—ì„œ Transductive Learningì´ë€ í•˜ë‚˜ì˜ ê·¸ë˜í”„ ìƒ ì¼ë¶€ ë…¸ë“œì™€ ì—£ì§€ì˜ ground truthë¥¼ ì•„ëŠ” ìƒíƒœì—ì„œ ë‚˜ë¨¸ì§€ ë…¸ë“œì™€ ì—£ì§€ì˜ ê°’ì„ ì¶”ì •í•˜ëŠ” ë°©ì‹ì…ë‹ˆë‹¤. í•™ìŠµ ê³¼ì • ì¤‘, ëª¨ë¸ì€ ground truthë¥¼ ì•Œì§€ ëª»í•˜ëŠ” ë…¸ë“œë¥¼ í¬í•¨í•œ ëª¨ë“  ë…¸ë“œì™€ ì—£ì§€ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤. ![Imgur](https://i.imgur.com/Jpte1Hu.png?1) ğŸ’¡ **Inductive Learning** ê·¸ë˜í”„ í•™ìŠµ ê´€ì ì—ì„œ Inductive Learningì´ë€ ground truthë¥¼ ì•Œê³  ìˆëŠ” ê·¸ë˜í”„(ë“¤)ì— ëŒ€í•´ ëª¨ë¸ì„ í•™ìŠµ í•œ í›„, ì „í˜€ ìƒˆë¡œìš´ ê·¸ë˜í”„ì˜ ë…¸ë“œì™€ ì—£ì§€ì˜ ê°’ì„ ì¶”ì •í•˜ëŠ” ë°©ì‹ì…ë‹ˆë‹¤. ì¦‰, í•™ìŠµì´ ì™„ë£Œëœ í›„ì—ëŠ” ëª¨ë¸ì´ ìƒˆë¡œìš´ ì²˜ìŒ ë³´ëŠ” ë…¸ë“œì˜ ê°’ì„ ì¶”ì •í•˜ëŠ” ë°ì—ë„ ì ìš©ë  ìˆ˜ ìˆë‹¤ëŠ” ì˜ë¯¸ì´ì£ . ![Imgur](https://i.imgur.com/Fi3vDRg.png?1) Shallow Encoderì€ Transductive Learningìœ¼ë¡œ í•™ìŠµí•´ì•¼ í•˜ëŠ” ëŒ€í‘œì ì¸ ì¼€ì´ìŠ¤ì…ë‹ˆë‹¤. í•™ìŠµ ë„ì¤‘ ë³´ì§€ ëª»í•œ ë…¸ë“œëŠ” look-up tableìƒ ì¡´ì¬í•  ë¦¬ ì—†ìœ¼ë‹ˆ ë§µí•‘ë˜ëŠ” ì„ë² ë”© ë²¡í„°ê°€ ì—†ì„ ê²ƒì´ê³ , ì„ë² ë”© ë²¡í„°ê°€ ì—†ë‹¤ë©´ node classification ë“±ì˜ downstream taskì—ì„œ ì˜ˆì¸¡ì´ ë¶ˆê°€ëŠ¥í•˜ê² ì£ ? ì´ëŸ° íŠ¹ì„± ë•Œë¬¸ì— ì‹œê°„ì— ë”°ë¼ ë…¸ë“œê°€ ì¶”ê°€ë  ìˆ˜ ìˆëŠ” evolving ê·¸ë˜í”„ì™€ ê°™ì€ ê²½ìš° ê·¸ë˜í”„ê°€ ë³€í•  ë•Œë§ˆë‹¤ ì „ì²´ ì„ë² ë”©ì„ ë‹¤ì‹œ scratchë¶€í„° í•™ìŠµí•´ì•¼ í•œë‹¤ëŠ” ë¶ˆí¸í•¨ì´ ìˆìŠµë‹ˆë‹¤. . | ë…¸ë“œ featureì„ í™œìš©í•˜ì§€ ì•ŠìŒ ëŒ€ë¶€ë¶„ì˜ ê·¸ë˜í”„ ë°ì´í„°ì…‹ì€ ìš°ë¦¬ê°€ í™œìš©í•  ìˆ˜ ìˆëŠ” ë…¸ë“œ featureì´ ì¡´ì¬í•©ë‹ˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´, ì†Œì…œ ê·¸ë˜í”„ì˜ ê²½ìš°, ë‹¨ìˆœíˆ ì² ìˆ˜ê°€ ì˜í¬ê°€ ì¹œêµ¬ë¼ëŠ” ì •ë³´ ì´ì™¸ì—ë„, ì² ìˆ˜ëŠ” ì„±ê· ê´€ëŒ€í•™êµì— ë‹¤ë‹ˆëŠ” 23ì„¸ ë‚¨í•™ìƒì´ë¼ëŠ” ì •ë³´ë„ ì¡´ì¬í•©ë‹ˆë‹¤. ë‹¨ìˆœí•œ ë…¸ë“œ ê°„ ì—°ê²° ìƒíƒœ ì´ì™¸ì—ë„ ì´ëŸ¬í•œ ë…¸ë“œ featureë¥¼ ê³ ë ¤í•˜ì—¬ ë…¸ë“œë¥¼ ì„ë² ë”© í•œë‹¤ë©´ ì •ë³´ëŸ‰ì´ ë” í’ë¶€í•´ì ¸ íš¨ê³¼ì ì¼ ê²ƒì…ë‹ˆë‹¤. | . Deep Graph Encoders . ì´ì œ ì§€ê¸ˆê» ë‹¤ë¤„ì™”ë˜ ê°„ë‹¨í•œ look-up tableë¡œ ì´ë£¨ì–´ì§„ encoderì—ì„œ ë²—ì–´ë‚˜, ì¢€ ë” ë³µì¡í•œ í˜•íƒœì˜ Deep Encoderì„ ê³µë¶€í•´ë´…ì‹œë‹¤. . . Deep Encoderì€ ì¸í’‹ ê·¸ë˜í”„ì— ìˆ˜ì°¨ë¡€ì˜ ë¹„ì„ í˜•ì ì¸ transformationì„ ê°€í•˜ì—¬ end-to-endìœ¼ë¡œ ìµœì¢… ì„ë² ë”©ì„ ì–»ëŠ” ë°©ì‹ì„ ë§í•©ë‹ˆë‹¤. ìˆ˜ì—… ìŠ¬ë¼ì´ë“œì— ì“°ì¸ ë§ ê·¸ëŒ€ë¡œ, . Deep Encoder = multiple layers of non-linear transformations based on graph structure . ë¡œ ìƒê°í•  ìˆ˜ ìˆê² ìŠµë‹ˆë‹¤. ì˜ ì™€ë‹¿ì§€ ì•Šìœ¼ì‹ ë‹¤ê³ ìš”? ì‚¬ì‹¤, ì¸í’‹ ë°ì´í„°ê°€ ìš°ë¦¬ê°€ ìµìˆ™ì¹˜ ì•Šì€ í˜•íƒœì˜ ê·¸ë˜í”„ë¼ ê·¸ë ‡ì§€, ì˜¤ëŠ˜ë‚ ì˜ ë¨¸ì‹ ëŸ¬ë‹/ë”¥ëŸ¬ë‹ ëª¨ë¸ì´ ì´ë¯¸ì§€ë‚˜ í…ìŠ¤íŠ¸ì™€ ê°™ì€ ì •í˜• ë°ì´í„°ë¥¼ ì²˜ë¦¬í•˜ëŠ” ë°©ì‹ê³¼ ìœ ì‚¬í•©ë‹ˆë‹¤. . . ìœ„ì˜ ë‘ ê·¸ë¦¼ì´ ìœ ì‚¬í•˜ë‹¤ëŠ” ì ì´ í•œëˆˆì— ë³´ì´ì‹¤ ê²ë‹ˆë‹¤. ì´í•´ë¥¼ ë•ê¸° ìœ„í•´ ê°€ì¥ ê¸°ë³¸ì ì¸ CNN êµ¬ì¡°ë¥¼ ìƒê°í•´ ë³¼ê¹Œìš”? ì›ë³¸ ì´ë¯¸ì§€ê°€ ì—¬ëŸ¬ convolution layerì„ ê±°ì¹˜ë©° ë”ìš± ë” ì¶•ì•½ëœ feature mapì„ ë§Œë“œëŠ” ë°©ì‹ê³¼ ìœ ì‚¬í•˜ê²Œ, ì¸í’‹ìœ¼ë¡œ ë“¤ì–´ì˜¨ ê·¸ë˜í”„ëŠ” ì—¬ëŸ¬ graph convolution layerì„ ê±°ì¹˜ë©° ì›ë³¸ ê·¸ë˜í”„ì˜ ì˜ë¯¸ë¥¼ ì ì ˆíˆ ì¶•ì•½í•˜ëŠ” ë…¸ë“œ ì„ë² ë”©ì„ ë§Œë“œëŠ” ê²ƒì…ë‹ˆë‹¤. . ë˜í•œ, ê°œ ê³ ì–‘ì´ ì´ë¯¸ì§€ ë¶„ë¥˜ ëª¨ë¸ì´ ì§€ë„ í•™ìŠµìœ¼ë¡œ í•™ìŠµë  ë•Œ í•™ìŠµ ë°ì´í„°ì— ëŒ€í•´ ê° ì´ë¯¸ì§€ê°€ ê°œì¸ì§€, ê³ ì–‘ì´ì¸ì§€ ë‚˜íƒ€ë‚´ëŠ” í´ë˜ìŠ¤ labelì„ í™œìš©í•˜ëŠ” ê²ƒê³¼ ê°™ì´, ë…¸ë“œ ë¶„ë¥˜ ë¬¸ì œì˜ ê²½ìš° ê° ë…¸ë“œì— ëŒ€í•œ í´ë˜ìŠ¤ labelì´ ìˆë‹¤ë©´ ì´ë¥¼ ì§ì ‘ í™œìš©í•˜ì—¬ encoderë¥¼ í•™ìŠµí•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. . ğŸ’¡ ì´ ê²½ìš° decoderì€ ë…¸ë“œ í´ë˜ìŠ¤ label ì…ë‹ˆë‹¤. ë¬¼ë¡ , ground truth labelì´ ì¡´ì¬í•˜ì§€ ì•ŠëŠ” ë¹„ì§€ë„ í•™ìŠµ ìƒí™©ì—ì„œëŠ” ê¸°ì¡´ Shallow Encoderì„ í•™ìŠµí•˜ë˜ ë°©ë²•ê³¼ ë™ì¼í•˜ê²Œ Random Walk ë“±ìœ¼ë¡œ ì •ì˜ë˜ëŠ” ì¸í’‹ ê·¸ë˜í”„ìƒ ë…¸ë“œ similarityë¥¼ ìœ ì§€í•˜ë„ë¡ í•™ìŠµí•  ìˆ˜ë„ ìˆê² ì£ . ì´ ë¶€ë¶„ì€ ë³¸ ê°•ì˜ ë§ì— ë‹¤ì‹œ ë‹¤ë£¨ë‹ˆê¹Œ ì´í•´ë˜ì§€ ì•ŠëŠ”ëŒ€ë„ ê±±ì • ë§ˆì„¸ìš”! :) . ğŸ’¡ ì´ ê²½ìš° decoderì€ ì„ë² ë”© ë²¡í„° ê°„ similarity metricì¸ ë²¡í„° ë‚´ì  ë“±ìœ¼ë¡œ ì •ì˜í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. (lecture 4) ì´ë ‡ê²Œ Deep Encoderì„ í†µí•´ ì–»ì€ ë…¸ë“œ/ê·¸ë˜í”„ ì„ë² ë”©ì€ ì—¬ëŸ¬ taskì—ì„œ agnosticí•˜ê²Œ í™œìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. . í•™ìŠµëœ ì„ë² ë”©ì„ í™œìš©í•  ìˆ˜ ìˆëŠ” ì—¬ëŸ¬ taskì˜ ì˜ˆ Node classification | Link prediction | Community detection | Network similarity # Why is it Hard? . | . | . ì•„ê¹Œ ì–¸ê¸‰í–ˆë“¯ì´ Deep Encoderì„ í†µí•´ ê·¸ë˜í”„ë¥¼ ì„ë² ë”© í•œë‹¤ëŠ” ê°œë…ì€ ì§€ê¸ˆê» ìš°ë¦¬ê°€ ì •í˜• ë°ì´í„°ë¥¼ ì²˜ë¦¬í–ˆë˜ ë°©ì‹ê³¼ ë¹„ìŠ·í•˜ê¸° ë•Œë¬¸ì— ë‚¯ì„¤ì§€ ì•ŠìŠµë‹ˆë‹¤. . ê·¸ë ‡ë‹¤ë©´ ê·¸ëƒ¥ ë„ë¦¬ ì‚¬ìš©ë˜ê³  ìˆëŠ” CNNì´ë‚˜ RNNì„ í™œìš©í•´ì„œ ê·¸ë˜í”„ë¥¼ ì„ë² ë”© í•˜ë©´ ë˜ì§€ ì•Šì„ê¹Œìš”? . . ê·¸ëŸ´ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. . ì •í˜• ë°ì´í„°ì¸ ì´ë¯¸ì§€, í…ìŠ¤íŠ¸ì— ë¹„í•´ ë¹„ì •í˜• ë°ì´í„°ì¸ ê·¸ë˜í”„ëŠ” ë„ˆë¬´ë‚˜ë„ ë³µì¡í•˜ê¸° ë•Œë¬¸ì´ì£ . ê·¸ë˜í”„ëŠ” ì´ë¯¸ì§€ì™€ ê°™ì´ (0,0) ë“±ì˜ ê¸°ì¤€ì ì„ ë‘˜ ìˆ˜ ì—†ìœ¼ë©°, í…ìŠ¤íŠ¸ì™€ ê°™ì´ ëª…ë°±í•œ ìˆœì„œê°€ ìˆì§€ë„ ì•ŠìŠµë‹ˆë‹¤. ê·¸ë˜í”„ëŠ” ì œê°ê¸° ë‹¤ë¥¸ ì‚¬ì´ì¦ˆ ì¼ ìˆ˜ ìˆìœ¼ë©° ê°ê°ì˜ topological structure ë˜í•œ ëª¨ë‘ ë‹¤ë¦…ë‹ˆë‹¤. ì‹¬ì§€ì–´ëŠ” ë…¸ë“œ ë§ˆë‹¤ multimodal featureì„ ê°€ì§ˆ ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤. . ë”°ë¼ì„œ, ë¹„ì •í˜• ê·¸ë˜í”„ êµ¬ì¡°ì—ì„œ ê° ë…¸ë“œì˜ êµ¬ì¡°ì  íŠ¹ì§• ë° ë…¸ë“œ featureì„ ê³ ë ¤í•˜ì—¬ ì ì ˆí•˜ê²Œ ì„ë² ë”© í•˜ëŠ” ìƒˆë¡œìš´ ë°©ë²•ì´ í•„ìš”í•©ë‹ˆë‹¤. . ğŸ’¡ ë…¸ë“œì˜ Multimodal feature ë‹¤ì‹œ ì†Œì…œ ê·¸ë˜í”„ë¥¼ ë– ì˜¬ë ¤ ë´…ì‹œë‹¤. ê° ë…¸ë“œëŠ” ì² ìˆ˜, ì˜í¬ë¥¼ í¬í•¨í•œ ê°œì¸ì„ ë‚˜íƒ€ë‚´ê³ , ì—£ì§€ëŠ” ê° ê°œì¸ ì‚¬ì´ì— ì¹œêµ¬ ê´€ê³„ê°€ ì„±ë¦½í•˜ëŠ”ì§€ë¥¼ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤. ì´ ë•Œ, ì² ìˆ˜ë¼ëŠ” ë…¸ë“œëŠ” í”„ë¡œí•„ ì‚¬ì§„(ì´ë¯¸ì§€), ìê¸° ì†Œê°œ ê¸€(í…ìŠ¤íŠ¸) ë“± ì—¬ëŸ¬ ë¶€ê°€ì ì¸ featureì„ ê°€ì§ˆ ìˆ˜ ìˆìŠµë‹ˆë‹¤. .",
            "url": "https://cs224w-kor.github.io/blog/graph%20neural%20network/gnn/graph%20convolution%20network/gcn/2022/07/27/lecture-0601.html",
            "relUrl": "/graph%20neural%20network/gnn/graph%20convolution%20network/gcn/2022/07/27/lecture-0601.html",
            "date": " â€¢ Jul 27, 2022"
        }
        
    
  
    
        ,"post10": {
            "title": "Lecture 5.3 - Collective Classification - Belief Propagation",
            "content": ". Lecture 5 . Lecture 5.1 - Message passing and Node Classification | Lecture 5.2 - Relational and Iterative Classification | Lecture 5.3 - Collective Classification : Belief Propagation | . . 5.3 - Collective Classification : Belief Propagation . Collective Classification : Belief Propagation . Collective Classification Models . Relational classifiers | Iterative classification | Loopy belief propagation | . . ê´€ê³„ ë¶„ë¥˜ì | ë°˜ë³µêµ¬ë¶„ | ë£¨í”¼ ì‹ ì•™ ì „íŒŒ | . . ğŸ’¬ ë¨¼ì € ì§€ê¸ˆê¹Œì§€ ë°°ìš´ ë‚´ìš©ìœ¼ë¡œëŠ” ê° ë…¸ë“œëŠ” ì´ì›ƒ ë…¸ë“œì˜ í™•ë¥ ì˜ ê°€ì¤‘í‰ê· ì„ ìì‹ ì˜ ìƒˆë¡œìš´ í™•ë¥ ë¡œ ì‚¼ê³  ìˆë‹¤. í˜¹ì€ ê° ë…¸ë“œëŠ” ì´ì›ƒ ë…¸ë“œì˜ ë ˆì´ë¸”ì„ í™œìš©í•´ ìì‹ ì˜ ìƒˆë¡œìš´ í™•ë¥ ì„ ê³„ì‚°í•˜ê²Œ ëœë‹¤. ì¦‰, ì´ì›ƒë…¸ë“œì˜ ì •ë³´ê°€ ê°ê°ì˜ ë…¸ë“œì—ì„œ ì‚¬ìš©ë˜ê³  ìˆëŠ” ê²ƒì´ë‹¤. ì´ê²ƒì„ ê° ë…¸ë“œëŠ” ì´ì›ƒë…¸ë“œì—ê²Œ Beliefë¥¼ ì „ë‹¬ë°›ëŠ”ë‹¤ê³  í•  ìˆ˜ ìˆë‹¤. ì¦‰ ì´ì›ƒ ë…¸ë“œì˜ beliefë¥¼ ë°›ì•„ ìì‹ ì˜ beliefë¥¼ ìƒì„±í•œë‹¤. ë‹¤ë¥´ê²Œ ë§í•˜ë©´, ëª¨ë¸ì€ ê° ë…¸ë“œì— ëŒ€í•´ ë°ì´í„°ë§ˆë‹¤ beliefë¥¼ ê°€ì§€ê³  ìˆê³ , ì´ì›ƒ ë…¸ë“œì˜ beliefë¥¼ ì´ìš©í•´ ê° ë…¸ë“œì˜ beliefë¥¼ ì—…ë°ì´íŠ¸í•˜ê³  ìˆë‹¤. ê·¸ë ‡ë‹¤ë©´ ì™œ êµ³ì´ ë°”ë¡œ ì´ì›ƒë…¸ë“œì—ì„œë§Œ beliefë¥¼ ë°›ì•„ì•¼ í• ê¹Œ. ì¢€ ë” ë¨¼ ë…¸ë“œì˜ beliefë„ ì¤‘ìš”í•˜ê²Œ ì‘ë™í•˜ì§€ ì•Šì„ê¹Œ? ì™œëƒí•˜ë©´ ê²°êµ­ ì´í„°ë ˆì´ì…˜ì„ ë°˜ë³µí•˜ì—¬ ì´ì›ƒë…¸ë“œì˜ beliefë¥¼ ë°›ê²Œ ëœë‹¤ë©´, í•´ë‹¹ beliefëŠ” ì´ì›ƒë…¸ë“œì˜ ì´ì›ƒë…¸ë“œì˜ beliefê°€ ì„ì—¬ìˆëŠ” ìƒíƒœê¸° ë•Œë¬¸ì—, ì´í„°ë ˆì´ì…˜ì„ ë°˜ë³µí•œë‹¤ëŠ” ê²ƒì€ ìì‹ ì˜ ì´ì›ƒë…¸ë“œì˜ ì´ì›ƒë…¸ë“œì˜ ì´ì›ƒë…¸ë“œì˜ .... beliefë¥¼ ë°›ê³  ìˆëŠ” ê²ƒì´ê¸° ë•Œë¬¸ì´ë‹¤. ì´ë¥¼ ì—­ìœ¼ë¡œ ìƒê°í•˜ì—¬ beliefê°€ ê·¸ë˜í”„ì— ì§ì ‘ íë¥´ë„ë¡ ì•Œê³ ë¦¬ì¦˜ì„ êµ¬ì„±í•œ ê²ƒì´ loopy belief propagationì´ ëœë‹¤. Loopy Belief Propagation . Belief Propagation is a dynamic programming approach to answering probability queries in a graph (e.g. probability of node $v$ belonging to class 1 ) | Iterative process in which neighbor nodes â€œtalkâ€ to each other, passing messages . ğŸ“Œ &quot;I (node v) believe you (node u) belong to class 1 with likelihood ...&quot; | When consensus is reached, calculate final belief | . . ë¯¿ìŒ ì „íŒŒëŠ” ê·¸ë˜í”„ì—ì„œ í™•ë¥  ì¿¼ë¦¬ì— ì‘ë‹µí•˜ëŠ” ë™ì  í”„ë¡œê·¸ë˜ë° ì ‘ê·¼ë²•ì´ë‹¤(ì˜ˆ: í´ë˜ìŠ¤ 1ì— ì†í•˜ëŠ” ë…¸ë“œ $v$ì˜ í™•ë¥ ). | ì¸ì ‘ ë…¸ë“œê°€ ë©”ì‹œì§€ë¥¼ ì „ë‹¬í•˜ë©´ì„œ ì„œë¡œ â€œëŒ€í™”â€í•˜ëŠ” ë°˜ë³µ í”„ë¡œì„¸ìŠ¤ . ğŸ“Œ &quot;ë‚˜(ë…¸ë“œ v)ëŠ” (ë…¸ë“œ u)ê°€ í´ë˜ìŠ¤ 1ì— ì†í•œë‹¤ê³  ë¯¿ê³  ìˆìŠµë‹ˆë‹¤.â€ | í•©ì˜ê°€ ì´ë£¨ì–´ì§€ë©´ ìµœì¢… ë¯¿ìŒì„ ê³„ì‚°í•©ë‹ˆë‹¤. | . . ğŸ’¬ Belief Propagation í™•ë¥  ì¿¼ë¦¬ì— ì‘ë‹µí•˜ê¸° ìœ„í•œ ë™ì  í”„ë¡œê·¸ë˜ë° ì ‘ê·¼ë²•ì´ë‹¤. ë˜í•œ ë°˜ë³µì ìœ¼ë¡œ ì´ì›ƒë…¸ë“œì™€ &#39;talk&#39;í•˜ë©´ì„œ ë©”ì‹œì§€ë¥¼ ì „ë‹¬í•˜ëŠ” ë°©ë²•ì´ë‹¤. ë”°ë¼ì„œ ì£¼ëœ ì•„ì´ë””ì–´ëŠ” ê° ë…¸ë“œëŠ” ë©”ì‹œì§€ë¥¼ ì´ì›ƒìœ¼ë¡œë¶€í„° ì–»ëŠ”ë‹¤. ê·¸ë¦¬ê³  ì—…ë°ì´íŠ¸í•˜ê³  ì•ìœ¼ë¡œ ì „ë‹¬í•œë‹¤. Message Passing: Basics . Task: Count the number of nodes in a graph* | Condition: Each node can only interact (pass message) with its neighbors | Example: path graph | . . ê³¼ì œ: ê·¸ë˜í”„ì˜ ë…¸ë“œ ìˆ˜ ê³„ì‚°* | ì¡°ê±´: ê° ë…¸ë“œëŠ” ì¸ì ‘ ë…¸ë“œì™€ë§Œ ìƒí˜¸ ì‘ìš©(ë©”ì‹œì§€ ì „ë‹¬)í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. | ì˜ˆ: ê²½ë¡œ ê·¸ë˜í”„ | . . Potential issues when the graph contains cycles. . Weâ€™ll get back to it later! . . ê·¸ë˜í”„ì— ì£¼ê¸°ê°€ í¬í•¨ë˜ì–´ ìˆì„ ë•Œ ë°œìƒí•  ìˆ˜ ìˆëŠ” ë¬¸ì œ. ë‚˜ì¤‘ì— ë‹¤ì‹œ ì–˜ê¸°í•˜ì! . . ğŸ’¬ ìœ„ì™€ ê°™ì´ ê°€ì¥ ë‹¨ìˆœí•œ ê·¸ë˜í”„ì˜ í˜•íƒœë¥¼ ìƒê°í•´ë³´ì. ìš°ë¦¬ê°€ ì›í•˜ëŠ” ê²ƒì€ ê·¸ë˜í”„ì˜ ë…¸ë“œ ìˆ˜ë¥¼ ê³„ì‚°í•˜ê³ ì í•œë‹¤. ì´ë•Œ beliefëŠ” ê° ì´í„°ë ˆì´ì…˜ë§ˆë‹¤ ì´ì›ƒ ë…¸ë“œë¡œë§Œ ì „ë‹¬ë  ìˆ˜ ìˆë‹¤. Message Passing: Algorithm . Task: Count the number of nodes in a graph | Algorithm: Define an ordering of nodes (that results in a path) | Edge directions are according to order of nodes Edge direction defines the order of message passing | . | For node $i$ from 1 to 6 Compute the message from node $i$ to $i+1$ (number of nodes counted so far) | Pass the message from node $i$ to $i+1$ | . | . | . . ê³¼ì œ: ê·¸ë˜í”„ì˜ ë…¸ë“œ ìˆ˜ ê³„ì‚° | ì•Œê³ ë¦¬ì¦˜: ë…¸ë“œ ìˆœì„œ ì •ì˜(ê²½ë¡œ ìƒì„±) | ì—ì§€ ë°©í–¥ì€ ë…¸ë“œ ìˆœì„œì— ë”°ë¦…ë‹ˆë‹¤. ì—ì§€ ë°©í–¥ì€ ë©”ì‹œì§€ ì „ë‹¬ ìˆœì„œë¥¼ ì •ì˜í•©ë‹ˆë‹¤. | . | ë…¸ë“œ $i$ì˜ ê²½ìš° 1 ~ 6ê¹Œì§€ ë…¸ë“œ $i$ì—ì„œ $i+1$ë¡œ ë©”ì‹œì§€ ê³„ì‚° (ì§€ê¸ˆê¹Œì§€ ì¹´ìš´íŠ¸ëœ ë…¸ë“œ ìˆ˜) | ë…¸ë“œ $i$ì—ì„œ $i+1$ë¡œ ë©”ì‹œì§€ ì „ë‹¬ | . | . | . . . ğŸ’¬ ì•Œê³ ë¦¬ì¦˜ì€ ë‹¤ìŒê³¼ ê°™ì„ ê²ƒì´ë‹¤. 1. ë…¸ë“œì˜ ìˆœì„œë¥¼ ì •í•œë‹¤. 2. 1ì—ì„œ ì •í•œ ìˆœì„œì— ë”°ë¼ ì—£ì§€ì˜ ë°©í–¥ì„ ì •í•œë‹¤. 3. $i$ë²ˆì§¸ ë…¸ë“œì— ëŒ€í•´ ë‹¤ìŒì„ ì‹œí–‰í•œë‹¤. $i-1$ ë…¸ë“œì—ì„œ belief(ì´ì „ê¹Œì§€ ì§€ë‚˜ì˜¨ ë…¸ë“œì˜ ìˆ˜)ë¥¼ ë°›ëŠ”ë‹¤. beliefì— $1$(ìì‹ ì— ëŒ€í•œ count)ì„ ë”í•œë‹¤. $i+1$ ë…¸ë“œë¡œ beliefë¥¼ ì „ë‹¬í•œë‹¤. . Message Passing: Basics . Task: Count the number of nodes in a graph Condition: Each node can only interact (pass message) with its neighbors Solution: Each node listens to the message from its neighbor, updates it, and passes it forward $m$ : the message . . ì‘ì—…: ê·¸ë˜í”„ì˜ ë…¸ë“œ ìˆ˜ ê³„ì‚° ì¡°ê±´: ê° ë…¸ë“œëŠ” ì¸ì ‘ ë…¸ë“œì™€ë§Œ ìƒí˜¸ ì‘ìš©(ë©”ì‹œì§€ ì „ë‹¬)í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì†”ë£¨ì…˜: ê° ë…¸ë“œëŠ” ì¸ì ‘ ë…¸ë“œë¡œë¶€í„° ë©”ì‹œì§€ë¥¼ ìˆ˜ì‹ í•˜ê³  ì—…ë°ì´íŠ¸í•œ í›„ ì „ë‹¬ $m$ : ë©”ì‹œì§€ . . . ğŸ’¬ ê·¸ë˜í”„ì˜ ë…¸ë“œì˜ ìˆ˜ë¥¼ ì„¸ëŠ” ë°©ë²•ì„ êµ¬í•˜ê¸° ìœ„í•´ì„œ ìœ„ì—ì„œ ë³¸ ì•Œê³ ë¦¬ì¦˜ì„ ë”°ë¼ ë¨¼ì € ë…¸ë“œì˜ ìˆœì„œì™€ ê·¸ì— ë”°ë¥¸ ë°©í–¥ì„ ì •í•´ì¤€ë‹¤. ê·¸ë¦¬ê³  ë‚œë’¤ë¡œ ì´ì „ ë…¸ë“œë¡œ ë¶€í„° messageë¥¼ ì „ë‹¬ë°›ê³ (listen) í˜„ì¬ë…¸ë“œì—ì„œ ë©”ì‹œì§€ë¥¼ ì—…ë°ì´íŠ¸í•˜ê³  ì•ì˜ ë…¸ë“œë¡œ ì „ë‹¬í•´ì¤€ë‹¤. Generalizing to a Tree . We can perform message passing not only on a path graph, but also on a tree-structured graph | Define order of message passing from leaves to root | . . ê²½ë¡œ ê·¸ë˜í”„ë¿ë§Œ ì•„ë‹ˆë¼ íŠ¸ë¦¬ êµ¬ì¡° ê·¸ë˜í”„ì—ì„œë„ ë©”ì‹œì§€ ì „ë‹¬ì„ ìˆ˜í–‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. | ë¦¬í”„ì—ì„œ ë£¨íŠ¸ë¡œ ì „ë‹¬ë˜ëŠ” ë©”ì‹œì§€ ìˆœì„œ ì •ì˜ | . . . ğŸ’¬ ê°™ì€ ì•Œê³ ë¦¬ì¦˜ì„ ìœ„ì™€ ê°™ì€ íŠ¸ë¦¬ êµ¬ì¡°ì— ì ìš©í•œë‹¤. íŠ¸ë¦¬ëŠ” parentì™€ childë¡œ êµ¬ì„±ë˜ì–´ ìˆê¸° ë•Œë¬¸ì—, ì „ì²´ ë…¸ë“œì˜ ìˆ˜ë¥¼ ì„¸ê¸° ìœ„í•´ì„œ childì—ì„œ parent ë°©í–¥ìœ¼ë¡œ beliefê°€ íë¥´ë©´ ë  ê²ƒì´ë‹¤. Message passing in a tree . Update beliefs in tree structure . íŠ¸ë¦¬ êµ¬ì¡°ì˜ ì‹ ë¢° ì—…ë°ì´íŠ¸ . . . ğŸ’¬ ì´ë•Œ ì™¼ìª½ ê·¸ë¦¼ê³¼ ê°™ì´ parent ë…¸ë“œëŠ” ìì‹ ì˜ child ë…¸ë“œì˜ beliefë¥¼ ë°›ì•„ ì¢…í•©í•˜ëŠ” ì¼ì¢…ì˜ ê³„ì‚°ì„ ìˆ˜í–‰í•œ ìˆ˜ ìì‹ ì˜ parent ë…¸ë“œë¡œ beliefë¥¼ ë„˜ê²¨ì£¼ê²Œ ëœë‹¤. ì´ë¥¼ í†µí•´ ìµœì¢…ì ìœ¼ë¡œ root ë…¸ë“œì—ì„œ ì „ì²´ ë…¸ë“œì˜ ìˆ˜ë¥¼ êµ¬í•  ìˆ˜ ìˆì„ ê²ƒì´ë‹¤. í•˜ì§€ë§Œ ì‹¤ì œë¡œ countë¥¼ beliefë¡œ ê°„ì£¼í•˜ê³  ì´ì›ƒ ë…¸ë“œì— ì „ë‹¬í•˜ì§€ ì•Šì„ ê²ƒì´ë‹¤. ì‹¤ì œë¡œ ì•Œê³ ë¦¬ì¦˜ì´ ì–´ë–»ê²Œ ë˜ì–´ ìˆëŠ”ì§€ ì‚´í´ë³´ì. Loopy BP Algorithm . What message will $i$ send to $j$ ? . It depends on what $i$ hears from its neighbors | Each neighbor passes a message to $i$ its beliefs of the state of $i$ | . . $i$ê°€ $j$ì— ë³´ë‚¼ ë©”ì‹œì§€ëŠ” ë¬´ì—‡ì…ë‹ˆê¹Œ? . $i$ê°€ ì´ì›ƒìœ¼ë¡œë¶€í„° ë¬´ì—‡ì„ ë“£ëŠëƒì— ë”°ë¼ ë‹¬ë¼ì§„ë‹¤. | ê° ì´ì›ƒì€ $i$ ìƒíƒœì— ëŒ€í•œ ë¯¿ìŒì„ $i$ì— ì „ë‹¬í•œë‹¤. | . . ğŸ“Œ I (node $i$ ) believe that you (node $j$ ) belong to class $Y_{j}$ with probability $ cdots$ ğŸ“Œ I(ë…¸ë“œ $i$)ëŠ” ë‹¹ì‹ (ë…¸ë“œ $j$)ì´ í™•ë¥ ë¡œ í´ë˜ìŠ¤ $Y_{j}$ì— ì†í•œë‹¤ê³  ë¯¿ëŠ”ë‹¤$ cdots$ . ğŸ’¬ ì¼ë°˜ì ìœ¼ë¡œëŠ” ì—¬ëŸ¬ ë…¸ë“œì˜ ì •ë³´ë¥¼ $i$ë…¸ë“œë¡œ ì „ë‹¬í•˜ê³ (hear) $i$ë…¸ë“œì—ì„œ $j$ë…¸ë“œë¡œ ì „ë‹¬í•œë‹¤. Notation . Label-label potential matrix $ psi$ : Dependency between a node and its neighbor. $ boldsymbol{ psi} left(Y_{i}, Y_{j} right)$ is proportional to the probability of a node $j$ being in class $Y_{j}$ given that it has neighbor $i$ in class $Y_{i}$. | Prior belief $ phi: phi left(Y_{i} right)$ is proportional to the probability of node $i$ being in class $Y_{i}$. | $m_{i rightarrow j} left(Y_{j} right)$ is $i^{ prime}$ s message / estimate of $j$ being in class $Y_{j}$. | $ mathcal{L}$ is the set of all classes/labels | . . ë ˆì´ë¸” ë ˆì´ë¸” ì ì¬ì  ë§¤íŠ¸ë¦­ìŠ¤ $ psi$ : ë…¸ë“œì™€ ì¸ì ‘ ë…¸ë“œ ê°„ì˜ ì¢…ì†ì„±. $ boldsymbol{ psi} left(Y_{i}, Y_{j} right)$ëŠ” ë…¸ë“œ $j$ê°€ í´ë˜ìŠ¤ $Y_{j}$ì— ìˆì„ í™•ë¥ ì— ë¹„ë¡€í•œë‹¤. | $ phi: phi left(Y_{i} right)$ëŠ” ë…¸ë“œ $i$ê°€ í´ë˜ìŠ¤ $Y_{i}$ì— í¬í•¨ë  í™•ë¥ ì— ë¹„ë¡€í•œë‹¤. | $m_{i rightarrow j} left(Y_{j} right)$ëŠ” í´ë˜ìŠ¤ $Y_{j}$ì— ì†í•˜ëŠ” $j$ì˜ $i^{ prime}$ ë©”ì‹œì§€/ì¶”ì •ì´ë‹¤. | $ mathcal{L}$ ëŠ” ëª¨ë“  í´ë˜ìŠ¤/ë¼ë²¨ì˜ ì§‘í•©ì…ë‹ˆë‹¤. | . . ğŸ’¬ **Notation** - $ psi$ (Label-Label Potential Matrix) : $ psi$ ëŠ” ê° ë…¸ë“œê°€ ì´ì›ƒë…¸ë“œì˜ í´ë˜ìŠ¤ì— ëŒ€í•œ ì˜í–¥ë ¥(ë¹„ë¡€)ì„ í–‰ë ¬ë¡œ í‘œí˜„í•œ ê²ƒì´ë‹¤. - ì˜ˆë¥¼ ë“¤ì–´ $ psi left(Y_{i}, Y_{j} right)$ ëŠ” ì´ì›ƒ ë…¸ë“œ iì˜ ë ˆì´ë¸”ì´ $Y_{i}$ ì¼ ë•Œ, ë…¸ë“œ $ mathrm{j}$ ê°€ $Y_{j}$ ë ˆì´ë¸”ì— ì†í•  í™•ë¥ ì˜ ë¹„ì¤‘ì´ë‹¤. - ë§Œì•½ $i$ì™€ $j$ê°€ Homophilyê°€ ì¡´ì¬í•œë‹¤ë©´(ê°™ì€ classë¥¼ ê°€ì§„ë‹¤ë©´) ëŒ€ê°ì›ì†Œë“¤ì˜ í¬ê¸°ëŠ” ë†’ì„ê²ƒì´ë‹¤. - ë˜í•œ ì´ í–‰ë ¬ì„ ì–»ê¸°ìœ„í•´ì„œëŠ” í•™ìŠµì´ í•„ìš”í•˜ë‹¤. - $ phi$ (Prior Belief) : ë…¸ë“œ $i$ê°€ $Y_{i}$ ì— ì†í•  í™•ë¥ ì— ë¹„ë¡€í•œë‹¤. - $m_{i rightarrow j} left(Y_{j} right)$ : $i$ì˜ ë©”ì„¸ì§€ê°€ $j$ë¡œ ì „ë‹¬ë˜ëŠ” ê²ƒì„ ì˜ë¯¸í•˜ëŠ”ë°, $i$ê°€ ì´ì›ƒ ë…¸ë“œë¡œ ë¶€í„° ë°›ì€ beliefì™€ ìì‹ ì˜ ì •ë³´ë¥¼ ì¢…í•©í•´ $j$ì˜ ë ˆì´ë¸”ì„ believeí•˜ëŠ” ê²ƒì„ ì˜ë¯¸í•œë‹¤. - $j$ì˜ ë…¸ë“œë¥¼ ì˜ˆì¸¡í•  ìˆ˜ ìˆë„ë¡ $i$ì—ì„œ $j$ë¡œ ì „ë‹¬í•˜ëŠ” ë©”ì‹œì§€ì´ë‹¤. - $L:$ ëª¨ë“  ë ˆì´ë¸”(í´ë˜ìŠ¤)ì„ í¬í•¨í•˜ëŠ” ì§‘í•© - $b_{i} left(Y_{i} right)$ : ë…¸ë“œ iì˜ í´ë˜ìŠ¤ê°€ $Y_{i}$ ì¼ belief Loopy BP Algorithm . Initialize all messages to 1 | Repeat for each node: | . ëª¨ë“  ë©”ì‹œì§€ë¥¼ 1ë¡œ ì´ˆê¸°í™” | ê° ë…¸ë“œì— ëŒ€í•´ ë°˜ë³µí•©ë‹ˆë‹¤. | . . After convergence: $b_{i} left(Y_{i} right)=$ node $i$ â€˜s belief of being in class $Y_{i}$ . ìˆ˜ë ´ í›„: $b_{i} left(Y_{i} right)=$ ë…¸ë“œ $i$ì˜ í´ë˜ìŠ¤ $Y_{i}$ì— ëŒ€í•œ ë¯¿ìŒ . . . 1. ê°€ì¥ ì²˜ìŒì—ëŠ” ëª¨ë“  ë…¸ë“œì˜ ë©”ì„¸ì§€ë¥¼ 1ë¡œ ì´ˆê¸°í™”í•œë‹¤. 2. ì´í›„ ê°€ìš´ë° ì´ë¯¸ì§€ì™€ ê°™ì´ ëª¨ë“  ë…¸ë“œì— ëŒ€í•´ ë‹¤ìŒ ë…¸ë“œë¡œ ë©”ì„¸ì§€ë¥¼ ì „ë‹¬í•˜ëŠ” ê³¼ì •ì„ ë°˜ë³µí•œë‹¤. ì´ë•Œ ê°€ìš´ë° ì´ë¯¸ì§€ì˜ ìˆ˜ì‹ì„ ì„¤ëª…í•´ë³´ìë©´, ê°€ì¥ ì•ì˜ ë¶„í™ìƒ‰ ë¶€ë¶„ì€ í˜„ì¬ ë…¸ë“œ $i$ì˜ ëª¨ë“  ë ˆì´ë¸”ì˜ ê°€ëŠ¥ì„±ì— ëŒ€í•´ ë°˜ë³µí•˜ì—¬ ë”í•œë‹¤ëŠ” ì˜ë¯¸ì´ë‹¤. ë…¹ìƒ‰ ë¶€ë¶„ì€ label-label potentialë¡œì„œ, $i$ë…¸ë“œì˜ ê° ë ˆì´ë¸”ë§ˆë‹¤ $j$ë…¸ë“œê°€ $Y_{j}$ ë ˆì´ë¸”ì„ ê°€ì§ˆ í™•ë¥ ì„ ê³„ì‚°í•˜ê²Œ ëœë‹¤. ì ìƒ‰ ë¶€ë¶„ì€ Priorë¡œì„œ $i$ë…¸ë“œê°€ $Y_{i}$ ë ˆì´ë¸”ì„ ê°€ì§ˆ í™•ë¥ ì„ ê³„ì‚°í•˜ê²Œ ëœë‹¤. ì²­ìƒ‰ ë¶€ë¶„ì€ $i$ ë…¸ë“œê°€ ë©”ì„¸ì§€ë¥¼ ë„˜ê²¨ë°›ëŠ” ì´ì›ƒ ë…¸ë“œì—ì„œ $i$ ë…¸ë“œê°€ $Y_{i}$ ë ˆì´ë¸”ì¼ beliefë¥¼ ë„˜ê²¨ ë°›ëŠ” ë¶€ë¶„ì´ë‹¤. ë§Œì•½ ìœ„ì˜ ê³¼ì •ì´ ì¶©ë¶„íˆ ë°˜ë³µë˜ì–´ ìˆ˜ë ´í•œë‹¤ë©´ ì„¸ë²ˆì§¸ ì´ë¯¸ì§€ì— í•´ë‹¹í•˜ëŠ” ì‹¤ì œ í™•ë¥  $[b_{i} left(Y_{i} right)]$ì´ ê³„ì‚°ë˜ê²Œ ëœë‹¤. 1. ì¦‰, Prior í™•ë¥ ì— beliefë¥¼ ëª¨ë‘ ê³±í•˜ì—¬ ìµœì¢…ì ì¸ belief ($[b_{i} left(Y_{i} right)]$) ë¥¼ ê²°ì •í•œë‹¤. ğŸ’¡ Q: ìˆ˜ë ´ í•˜ëŠ” ê²ƒì´ ë¬´ì—‡ì¸ì§€? ë¬´ì—‡ì´ ìˆ˜ë ´í•˜ëŠ” ê²ƒì¸ì§€ ìƒí™©ì— ëŒ€í•œ ì§ˆë¬¸ Example: Loopy Belief Propagation . Now we consider a graph with cycles | There is no longer an ordering of nodes | We apply the same algorithm as in previous slides: Start from arbitrary nodes | Follow the edges to update the neighboring nodes | . | . What if our graph has cycles? Messages from different subgraphs are no longer independent! But we can still run BP, but it will pass messages in loops. . . ì´ì œ ì£¼ê¸°ê°€ ìˆëŠ” ê·¸ë˜í”„ë¥¼ ì‚´í´ë´…ì‹œë‹¤. | ë” ì´ìƒ ë…¸ë“œ ìˆœì„œê°€ ì—†ìŠµë‹ˆë‹¤. | ì´ì „ ìŠ¬ë¼ì´ë“œì™€ ë™ì¼í•œ ì•Œê³ ë¦¬ì¦˜ì„ ì ìš©í•©ë‹ˆë‹¤. ì„ì˜ ë…¸ë“œì—ì„œ ì‹œì‘ | ê°€ì¥ìë¦¬ë¥¼ ë”°ë¼ ì¸ì ‘ ë…¸ë“œë¥¼ ì—…ë°ì´íŠ¸í•©ë‹ˆë‹¤. | . | . ë§Œì•½ ìš°ë¦¬ ê·¸ë˜í”„ì— ì£¼ê¸°ê°€ ìˆë‹¤ë©´? ë‹¤ë¥¸ í•˜ìœ„ ê·¸ë˜í”„ì˜ ë©”ì‹œì§€ëŠ” ë” ì´ìƒ ë…ë¦½ì ì´ì§€ ì•ŠìŠµë‹ˆë‹¤! í•˜ì§€ë§Œ BPëŠ” ì—¬ì „íˆ ì‹¤í–‰í•  ìˆ˜ ìˆì§€ë§Œ ë©”ì‹œì§€ë¥¼ ë£¨í”„ í˜•íƒœë¡œ ì „ë‹¬í•©ë‹ˆë‹¤. . . . ğŸ’¬ ì§€ê¸ˆê¹Œì§€ ì´ì•¼ê¸°í•œ ê·¸ë˜í”„ë“¤ì€ ìˆœí™˜í•˜ëŠ” êµ¬ì¡°ë¥¼ ê°€ì§€ê³  ìˆì§€ ì•Šì•„ ë©”ì„¸ì§€ë¥¼ ì „ë‹¬í•  ìˆœì„œë¥¼ ì •í•˜ëŠ”ë° ë¬¸ì œê°€ ì—†ì—ˆë‹¤. í•˜ì§€ë§Œ ìˆœí™˜í•˜ëŠ” êµ¬ì¡°ë¥¼ ê°€ì§€ëŠ” ê·¸ë˜í”„ì˜ ê²½ìš°ì—ëŠ” ë‹¨ìˆœí•˜ê²Œ ë…¸ë“œì˜ ìˆœì„œë¥¼ ì •í•´ì„œ ë©”ì„¸ì§€ë¥¼ ì „ë‹¬í•˜ë„ë¡ ë§Œë“¤ ìˆ˜ ì—†ë‹¤ ê·¸ì— ëŒ€í•´ ìì„¸íˆ ì‚´í´ë³´ì. ë§Œì•½ ìœ„ì™€ ê°™ì€ ê·¸ë˜í”„ê°€ ìˆê³ , ìœ„ì™€ ê°™ì€ ìˆœì„œë¡œ ë©”ì„¸ì§€ë¥¼ ì£¼ê³  ë°›ëŠ”ë‹¤ê³  ìƒê°í•´ë³´ì. $u$ ë…¸ë“œëŠ” $k$ì—ê²Œ ë©”ì„¸ì§€ë¥¼ ë°›ëŠ” ê²ƒì²˜ëŸ¼ ë³´ì´ì§€ë§Œ, ì‹¤ì œë¡œëŠ” ìê¸° ìì‹ ì˜ ë©”ì„¸ì§€ë§ˆì € ë°›ê³  ìˆëŠ” ìƒí™©ì´ë‹¤. ì¦‰, ë” ì´ìƒ ëª¨ë“  ë…¸ë“œê°€ ë…ë¦½ì ì´ì§€ ì•Šê³ , ì˜ì¡´ì„±ì´ ìƒê¸´ë‹¤. ìˆœì„œê°€ ë°˜ëŒ€ë¡œ íŠ¸ë¦¬ì™€ ê°™ì´ $j$ê°€ $i, k$ë¡œ ë©”ì„¸ì§€ë¥¼ ì „ë‹¬í•˜ê³ , $i, k$ê°€ $u$ë¡œ ë©”ì„¸ì§€ë¥¼ ì „ë‹¬í•œë‹¤ë©´, $j$ì˜ ë©”ì„¸ì§€ëŠ” $u$ì—ê²Œ ì¤‘ë³µë˜ì–´ ë‘ ë²ˆ ì „ë‹¬ë˜ëŠ” ë¬¸ì œê°€ ìƒê¸´ë‹¤. ì´ë ‡ê²Œ ë˜ë©´ ì•Œê³ ë¦¬ì¦˜ì´ í¬ê²Œ ë¬¸ì œê°€ ìƒê¸°ëŠ” ê²ƒ ê°™ì§€ë§Œ, ì‹¤ì œ ì ìš©í•´ë³´ë‹ˆ ê·¸ë ‡ì§€ ì•Šë‹¤ê³  í•œë‹¤. ì‹¤ì œ ê·¸ë˜í”„ë“¤ì€ ë¬´ì²™ í¬ê³ , ê±°ê¸°ì— ìˆœí™˜í•˜ëŠ” cycle êµ¬ì¡°ëŠ” ê·¸ë ‡ê²Œ í° ë¶€ë¶„ì„ ì°¨ì§€í•˜ì§€ ì•ŠëŠ”ë° ë°˜ë©´, ì „ì²´ êµ¬ì¡°ëŠ” ë§¤ìš° ë³µì¡í•˜ê¸° ë•Œë¬¸ì— Loopy BP ì•Œê³ ë¦¬ì¦˜ì´ ì˜ ì‘ë™í•œë‹¤ê³  í•œë‹¤. What Can Go Wrong? . Beliefs may not converge | Message $m_{u rightarrow i} left(Y_{i} right)$ is based on initial belief of $i$, not a separate evidence for $i$ | The initial belief of $i$ (which could be incorrect) is reinforced by the cycle | . iâ†’jâ†’kâ†’uâ†’ii rightarrow j rightarrow k rightarrow u rightarrow iiâ†’jâ†’kâ†’uâ†’i . However, in practice, Loopy BP is still a good heuristic for complex graphs which contain many branches. | . . ì‹ ë…ì´ ìˆ˜ë ´ë˜ì§€ ì•Šì„ ìˆ˜ ìˆë‹¤. | ë©”ì‹œì§€ $m_{u rightarrow i} left(Y_{i} right)$ëŠ” $i$ì— ëŒ€í•œ ë³„ë„ì˜ ì¦ê±°ê°€ ì•„ë‹ˆë¼ $i$ì˜ ì´ˆê¸° ë¯¿ìŒì— ê¸°ì´ˆí•œë‹¤. | (ì˜ëª»ë  ìˆ˜ ìˆìŒ) $i$ì˜ ì´ˆê¸° ì‹ ë…ì€ ì£¼ê¸°ì— ì˜í•´ ê°•í™”ëœë‹¤. | . iâ†’jâ†’kâ†’uâ†’ii rightarrow j rightarrow k rightarrow u rightarrow iiâ†’jâ†’kâ†’uâ†’i . ê·¸ëŸ¬ë‚˜ ì‹¤ì œë¡œ Loopy BPëŠ” ë§ì€ ë¶„ê¸°ë¥¼ í¬í•¨í•˜ëŠ” ë³µì¡í•œ ê·¸ë˜í”„ì— ì—¬ì „íˆ ì¢‹ì€ íœ´ë¦¬ìŠ¤í‹±ì´ë‹¤. | . . . Messages loop around and around: $2,4,8,16,32, ldots$ More and more convinced that these variables are $T$ ! | BP incorrectly treats this message as separate evidence that the variable is $ mathrm{T}$!. | Multiplies these two messages as if they were independent. But they donâ€™t actually come from independent parts of the graph. | One influenced the other (via a cycle). | . | . . ë©”ì‹œì§€ëŠ” ëŒê³  ëˆë‹¤: $2,4,8,16,32, ldots$ ì´ëŸ¬í•œ ë³€ìˆ˜ê°€ $T$ì„ì„ ì ì  ë” í™•ì‹ í•˜ê²Œ ëœë‹¤! | BPëŠ” ì´ ë©”ì‹œì§€ë¥¼ ë³€ìˆ˜ê°€ $ mathrm{T}$ë¼ëŠ” ë³„ë„ì˜ ì¦ê±°ë¡œ ì˜ëª» ì·¨ê¸‰í•œë‹¤. | ì´ ë‘ ë©”ì‹œì§€ë¥¼ ë…ë¦½í•œ ê²ƒì²˜ëŸ¼ ê³±í•©ë‹ˆë‹¤. í•˜ì§€ë§Œ ê·¸ê²ƒë“¤ì€ ì‚¬ì‹¤ ê·¸ë˜í”„ì˜ ë…ë¦½ì ì¸ ë¶€ë¶„ì—ì„œ ë‚˜ì˜¨ ê²ƒì´ ì•„ë‹™ë‹ˆë‹¤. | í•œ ì‚¬ëŒì´ ë‹¤ë¥¸ ì‚¬ëŒì—ê²Œ ì˜í–¥ì„ ì£¼ì—ˆë‹¤. | . | . . This is an extreme example. Often in practice, the cyclic influences are weak. (As cycles are long or include at least one weak correlation.) . . ì´ê²ƒì€ ê·¹ë‹¨ì ì¸ ì˜ˆì…ë‹ˆë‹¤. ì‹¤ì œë¡œ, ì£¼ê¸°ì ì¸ ì˜í–¥ì€ ì•½í•˜ë‹¤. (ì£¼ê¸°ê°€ ê¸¸ê±°ë‚˜ í•˜ë‚˜ ì´ìƒì˜ ì•½í•œ ìƒê´€ ê´€ê³„ë¥¼ í¬í•¨í•˜ê¸° ë•Œë¬¸ì—) . Advantages of Belief Propagation . Advantages: Easy to program &amp; parallelize | General: can apply to any graph model with any form of potentials Potential can be higher order: e.g. $ boldsymbol{ psi} left(Y_{i}, Y_{j}, Y_{k}, Y_{v} ldots right)$ | . | . | Challenges: Convergence is not guaranteed (when to stop), especially if many closed loops | . | Potential functions (parameters) Require training to estimate | . | . . ì¥ì : í”„ë¡œê·¸ë˜ë° ë° ë³‘ë ¬í™”ê°€ ìš©ì´í•¨ | ì¼ë°˜: ëª¨ë“  í˜•íƒœì˜ ì ì¬ë ¥ì´ ìˆëŠ” ê·¸ë˜í”„ ëª¨ë¸ì— ì ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì ì¬ë ¥ì€ ê³ ì°¨ì¼ ìˆ˜ ìˆë‹¤. ì˜ˆë¥¼ ë“¤ì–´ $ boldsymbol{ psi} left(Y_{i}, Y_{j}, Y_{k}, Y_{v} ldots right)$ | . | . | ê³¼ì œ: íŠ¹íˆ ë‹«íŒ ë£¨í”„ê°€ ë§ì€ ê²½ìš° ìˆ˜ë ´ì´ ë³´ì¥ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤(ì •ì§€ ì‹œê¸°). | . | ì ì¬ì  í•¨ìˆ˜(ëª¨ìˆ˜) í‰ê°€í•˜ë ¤ë©´ êµìœ¡ í•„ìš” | . | . . ğŸ’¬ **Advantages:** 1. ì½”ë”©ì´ ì‰½ê³ , ë³‘ë ¬í™”ê°€ ê°€ëŠ¥í•˜ë‹¤. 2. ì–´ë– í•œ ê·¸ë˜í”„ ëª¨ë¸ì´ë”ë¼ë„, potential matrixë¥¼ êµ¬ì„±í•  ìˆ˜ ìˆìœ¼ë¯€ë¡œ ë²”ìš©ì ì´ë‹¤. **Challenges:** 1. ìˆ˜ë ´ì´ ë³´ì¥ë˜ì§€ ì•Šì•„ ì–¸ì œ ë©ˆì¶°ì•¼ í• ì§€ ì•Œ ìˆ˜ ì—†ìœ¼ë©°, íŠ¹íˆ ìˆœí™˜êµ¬ì¡°ì¼ ê²½ìš° ìˆ˜ë ´ì´ ë³´ì¥ë˜ì§€ ì•Šì•„ ë°˜ë³µíšŸìˆ˜ë¥¼ ì§€ì •í•´ì£¼ì–´ì•¼ í•œë‹¤. 2. cycle êµ¬ì¡°ë¡œ ì¸í•´ ì¢…ì¢… ì ì€ ì´í„°ë§Œ ëŒë¦¬ê¸°ë„ í•œë‹¤. Summary . We learned how to leverage correlation in graphs to make prediction on nodes | Key techniques: Relational classification | Iterative classification | Loopy belief propagation | . | . . ìš°ë¦¬ëŠ” ê·¸ë˜í”„ì˜ ìƒê´€ ê´€ê³„ë¥¼ í™œìš©í•˜ì—¬ ë…¸ë“œì— ëŒ€í•œ ì˜ˆì¸¡ì„ í•˜ëŠ” ë°©ë²•ì„ ë°°ì› ë‹¤. | ì£¼ìš” ê¸°ìˆ : ê´€ê³„êµ¬ë¶„ | ë°˜ë³µêµ¬ë¶„ | ë£¨í”¼ ì‹ ì•™ ì „íŒŒ | . | . . CS224W: Machine Learning with Graphs 2021 Lecture 5.3 - Collective Classification .",
            "url": "https://cs224w-kor.github.io/blog/graph%20neural%20network/gnn/graph%20convolution%20network/gcn/2022/07/20/lecture-0503.html",
            "relUrl": "/graph%20neural%20network/gnn/graph%20convolution%20network/gcn/2022/07/20/lecture-0503.html",
            "date": " â€¢ Jul 20, 2022"
        }
        
    
  
    
        ,"post11": {
            "title": "Lecture 5.2 - Relational and Iterative Classification",
            "content": ". Lecture 5 . Lecture 5.1 - Message passing and Node Classification | Lecture 5.2 - Relational and Iterative Classification | Lecture 5.3 - Collective Classification : Belief Propagation | . . 5.2 - Relational and Iterative Classification . Relational Classification . Probabilistic Relational Classifier . Idea: Propagate node labels across the network Class probability $Y_v$ of node $v$ is a weighted average of class probabilities of its neighbors. | . | For labeled nodes $v$, initialize label $Y_v$ with ground-truth label $Y^*_v$. | For unlabeled nodes, initialize $Y_v$ = 0.5. | Update all nodes in a random order until convergence or until maximum number of iterations is reached. | Update for each node $v$ and label $c$ (e.g. 0 or 1 ) . P(Yv=c)=1âˆ‘(v,u)âˆˆEAv,uâˆ‘(v,u)âˆˆEAv,uP(Yu=c)P left(Y_{v}=c right)= frac{1}{ sum_{(v, u) in E} A_{v, u}} sum_{(v, u) in E} A_{v, u} P left(Y_{u}=c right)P(Yvâ€‹=c)=âˆ‘(v,u)âˆˆEâ€‹Av,uâ€‹1â€‹(v,u)âˆˆEâˆ‘â€‹Av,uâ€‹P(Yuâ€‹=c) If edges have strength/weight information, $A_{v, u}$ can be the edge weight between $v$ and $u$ | $P left(Y_{v}=c right)$ is the probability of node $v$ having label $c$ | . | Challenges: Convergence is not guaranteed | Model cannot use node feature information | . | . . ì•„ì´ë””ì–´: ë…¸ë“œ ë ˆì´ë¸”ì„ ë„¤íŠ¸ì›Œí¬ì— ì „íŒŒ ë…¸ë“œ $v$ì˜ í´ë˜ìŠ¤ í™•ë¥  $Y_v$ëŠ” ì´ì›ƒì˜ í´ë˜ìŠ¤ í™•ë¥ ì— ëŒ€í•œ ê°€ì¤‘ í‰ê· ì´ë‹¤. | . | ë ˆì´ë¸”ë§ëœ ë…¸ë“œ $v$ì˜ ê²½ìš° ì§€ìƒ ì‹¤ì¸¡ ë ˆì´ë¸” $Y^*_v$ë¡œ ë ˆì´ë¸” $Y_v$ë¥¼ ì´ˆê¸°í™”í•œë‹¤. | ë ˆì´ë¸”ì´ ì—†ëŠ” ë…¸ë“œì˜ ê²½ìš° $Y_v$ = 0.5ë¥¼ ì´ˆê¸°í™”í•©ë‹ˆë‹¤. | ìˆ˜ë ´í•  ë•Œê¹Œì§€ ë˜ëŠ” ìµœëŒ€ ë°˜ë³µ íšŸìˆ˜ì— ë„ë‹¬í•  ë•Œê¹Œì§€ ëª¨ë“  ë…¸ë“œë¥¼ ì„ì˜ì˜ ìˆœì„œë¡œ ì—…ë°ì´íŠ¸í•©ë‹ˆë‹¤. | ê° ë…¸ë“œ $v$ ë° ë ˆì´ë¸” $c$ì— ëŒ€í•œ ì—…ë°ì´íŠ¸(ì˜ˆ: 0 ë˜ëŠ” 1) . P(Yv=c)=1âˆ‘(v,u)âˆˆEAv,uâˆ‘(v,u)âˆˆEAv,uP(Yu=c)P left(Y_{v}=c right)= frac{1}{ sum_{(v, u) in E} A_{v, u}} sum_{(v, u) in E} A_{v, u} P left(Y_{u}=c right)P(Yvâ€‹=c)=âˆ‘(v,u)âˆˆEâ€‹Av,uâ€‹1â€‹(v,u)âˆˆEâˆ‘â€‹Av,uâ€‹P(Yuâ€‹=c) ê°€ì¥ìë¦¬ì˜ ê°•ë„/ë¬´ê²Œ ì •ë³´ê°€ ìˆëŠ” ê²½ìš° $A_{v,u}$ëŠ” $v$ì™€ $u$ ì‚¬ì´ì˜ ì—ì§€ ê°€ì¤‘ì¹˜ì¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤. | $P left(Y_{v}=c right)$ëŠ” ë…¸ë“œ $v$ê°€ $c$ ë ˆì´ë¸”ì„ ê°€ì§ˆ í™•ë¥ ì´ë‹¤. | . | ê³¼ì œ: ìˆ˜ë ´ì´ ë³´ì¥ë˜ì§€ ì•ŠìŒ | ëª¨ë¸ì€ ë…¸ë“œ í”¼ì³ ì •ë³´ë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. | . | . . ğŸ’¬ ê¸°ë³¸ ì•„ì´ë””ì–´ : ë…¸ë“œ $v$ì˜ ë ˆì´ë¸” í™•ë¥  $Y_v$ëŠ” ë…¸ë“œ $v$ì˜ ì£¼ë³€ë…¸ë“œì˜ ë ˆì´ë¸” í™•ë¥ ì˜ ê°€ì¤‘í‰ê· ê³¼ ê°™ë‹¤. ì¦‰, ë ˆì´ë¸”ì´ ì—†ëŠ” ë…¸ë“œì— ëŒ€í•´ ì´ì›ƒ ë…¸ë“œë“¤ì˜ ë ˆì´ë¸” í™•ë¥ ì„ ê°€ì¤‘í‰ê· í•˜ì—¬ ì˜ˆì¸¡í•˜ê²Œ ëœë‹¤. ì´ì§„ ë¶„ë¥˜ë¬¸ì œë¼ ê°€ì •í•˜ê³ , ëª¨ë“  ë…¸ë“œì— ë ˆì´ë¸” í™•ë¥ ì´ ì¡´ì¬í•´ì•¼ í•˜ê¸° ë•Œë¬¸ì—, ë ˆì´ë¸”ì´ ì—†ëŠ” ë…¸ë“œëŠ” 0.5ì˜ í™•ë¥ ë¡œ ì´ˆê¸°í™”í•˜ì—¬ ì‹œì‘í•˜ê²Œ ëœë‹¤. ì—…ë°ì´íŠ¸ëŠ” ë°˜ë³µì ìœ¼ë¡œ ì§„í–‰ë˜ë©°, ëª¨ë“  ë…¸ë“œì— ëŒ€í•´ ìˆ˜ë ´í•˜ê±°ë‚˜ ë°˜ë³µíšŸìˆ˜ì— ë„ë‹¬í•  ê²½ìš° ë©ˆì¶”ê²Œ ëœë‹¤. ë…¸ë“œÂ $*v*$ì— ëŒ€í•œ í™•ë¥ ì„ ê³„ì‚°í•˜ëŠ” ë²•ì€ ìœ„ ìˆ˜ì‹ê³¼ ê°™ë‹¤. ì´ë•Œ í–‰ë ¬ AëŠ” ì¸ì ‘í–‰ë ¬ì— í•´ë‹¹í•œë‹¤. ì¦‰, ì£¼ë³€ì˜ ì´ì›ƒë…¸ë“œì˜ í™•ë¥ Â $*P left(Y_{v}=c right)*$ë¥¼ í‰ê· í•˜ì—¬ ì‚¬ìš©í•˜ë˜, í•´ë‹¹ ì´ì›ƒë…¸ë“œì™€ ì—°ê²°ëœ degreeë§Œí¼ ê°€ì¤‘ì¹˜,Â $*A_{v,u}*$ë¥¼ ì£¼ê²Œ ëœë‹¤. ì´ë•Œ ë‘ê°€ì§€ ë¬¸ì œì ì´ ìˆë‹¤. 1. ìœ„ ìˆ˜ì‹ì€ ìˆ˜ë ´ì´ ë³´ì¥ë˜ì§€ ì•ŠëŠ”ë‹¤. 2. ëª¨ë¸ì´ ë…¸ë“œì˜ ë³€ìˆ˜ë¥¼ í™œìš©í•˜ì§€ ì•ŠëŠ”ë‹¤. ì´ì— ëŒ€í•´ì„  ì´í›„ ëª¨ë¸ì„ í†µí•´ ê°œì„ ë  ê²ƒì´ë¼ ê¸°ëŒ€í•´ë³´ì. Example: Initialization . Initialization: . All labeled nodes with their labels | All unlabeled nodes 0.5 (belonging to class 1 with probability 0.5) | . . ì´ˆê¸°í™”: . ë¼ë²¨ì´ í‘œì‹œëœ ëª¨ë“  ë…¸ë“œ | í‘œì‹œë˜ì§€ ì•Šì€ ëª¨ë“  ë…¸ë“œ 0.5 (í™•ë¥  0.5ì˜ í´ë˜ìŠ¤ 1ì— ì†í•¨) | . . ğŸ’¬ ìœ„ ê·¸ë˜í”„ì—ì„œ ë³¸ë˜ ë ˆì´ë¸”ì´ ìˆëŠ” ë…¸ë“œëŠ” ë…¹ìƒ‰ê³¼ ì ìƒ‰ìœ¼ë¡œ í‘œì‹œê°€ ë˜ì–´ ìˆë‹¤. ì´ì— ëŒ€í•´ ì´ì§„ë¶„ë¥˜ ë¬¸ì œì´ê¸° ë•Œë¬¸ì—, ë…¹ìƒ‰ì„ ê¸°ì¤€ìœ¼ë¡œ í™•ë¥ ì„ ê³„ì‚°í•˜ì—¬, ë…¹ìƒ‰ ë…¸ë“œëŠ” 1, ì ìƒ‰ ë…¸ë“œëŠ” 0, ë ˆì´ë¸”ì´ ì—†ëŠ” íšŒìƒ‰ë…¸ë“œëŠ” 0.5ë¡œ ì´ˆê¸°í™”í•œë‹¤. Example: $1^{st}$ Iteration, Update Node 3 . Update for the $1^{st}$ Iteration: For node 3, $N_3=[1,2,4]$ | . | . . $1^{st}$ ë°˜ë³µì— ëŒ€í•œ ì—…ë°ì´íŠ¸: ë…¸ë“œ 3ì˜ ê²½ìš° $N_3=[1,2,4]$ | . | . . ğŸ’¬ ê° ë…¸ë“œë³„ë¡œ ì´ì›ƒë…¸ë“œì˜ í™•ë¥ ì„ ì´ìš©í•´ ìˆœì°¨ì ìœ¼ë¡œ í™•ë¥ ì„ ì—…ë°ì´íŠ¸í•œë‹¤. ìœ„ ê·¸ë˜í”„ì˜ ê²½ìš° undirectedì´ê³  ì—£ì§€ê°€ ê° ë…¸ë“œ ê°„ ìµœëŒ€ í•˜ë‚˜ë§Œ ì¡´ì¬í•˜ê¸° ë•Œë¬¸ì— ë‹¨ìˆœ í‰ê· ì„ í†µí•´ ìƒˆë¡œìš´ í™•ë¥ ì„ ê³„ì‚°í•˜ê²Œ ëœë‹¤. Example: $1^{st}$ Iteration, Update Node 4 . Update for the $1^{st}$ Iteration: For node 4, $N_4=[1,3,5,6]$ | . | . . $1^{st}$ ë°˜ë³µì— ëŒ€í•œ ì—…ë°ì´íŠ¸: ë…¸ë“œ 4ì˜ ê²½ìš° $N_4=[1,3,5,6]$ | . | . . ğŸ’¬ ìœ„ì—ì„œ ì—…ë°ì´íŠ¸ëœ 3ë²ˆ ë…¸ë“œì˜ í™•ë¥ ì„ ì´ìš©í•´ 4ë²ˆ ë…¸ë“œ ì—­ì‹œ ì—…ë°ì´íŠ¸í•˜ê²Œ ëœë‹¤. ì¦‰, ë…¸ë“œë¥¼ ì—…ë°ì´íŠ¸í•˜ëŠ” ìˆœì„œì— ë”°ë¼ ê³„ì‚°ì´ ì¡°ê¸ˆì”© ë‹¬ë¼ì§€ê²Œ ëœë‹¤. Example: $1^{st}$ Iteration, Update Node 5 . Update for the $1^{st}$ Iteration: For node 5, $N_5=[4,6,7,8]$ | . | . . $1^{st}$ ë°˜ë³µì— ëŒ€í•œ ì—…ë°ì´íŠ¸: ë…¸ë“œ 5ì˜ ê²½ìš° $N_5=[4,6,7,8]$ | . | . . ğŸ’¬ 3, 4ë²ˆ ë…¸ë“œë¥¼ ì—…ë°ì´íŠ¸í•˜ê³  ë‚˜ì„œ 5ë²ˆ ë…¸ë“œë¥¼ ì—…ë°ì´íŠ¸í•œë‹¤. ì´ë•Œ ì—­ì‹œ ì—…ë°ì´íŠ¸ëœ 4ë²ˆ ë…¸ë“œì˜ í™•ë¥ ì„ ì´ìš©í•˜ê²Œ ëœë‹¤. Example: After $1^{st}$ Iteration . After Iteration 1 (a round of updates for all unlabeled nodes) ë°˜ë³µ í›„ 1 (ë¼ë²¨ì´ ì§€ì •ë˜ì§€ ì•Šì€ ëª¨ë“  ë…¸ë“œì— ëŒ€í•œ ì—…ë°ì´íŠ¸ ë¼ìš´ë”©) . . ğŸ’¬ ì²«ë²ˆì§¸ ì´í„°ê°€ ì¢…ë£Œëœ í›„ì˜ ëª¨ìŠµë‹ˆë‹¤. 9ë²ˆ ë…¸ë“œì˜ ê²½ìš° ë…¹ìƒ‰ ë…¸ë“œë§Œ ì—°ê²°ë˜ì–´ ìˆê¸° ë•Œë¬¸ì— í™•ë¥ ì´ 1ë¡œ ê³ ì •ëœ ëª¨ìŠµì„ ë³´ì´ê³  ìˆë‹¤. ì´ì™¸ì— 8ë²ˆ ë…¸ë“œ ì—­ì‹œ ì£¼ë³€ì— ë…¹ìƒ‰ ë…¸ë“œ 2ê°œ, í™•ë¥ ì´ ë†’ì€ ë…¸ë“œ(5ë²ˆ) í•œê°œê°€ ì´ì›ƒë…¸ë“œì´ê¸° ë•Œë¬¸ì— í™•ë¥ ì´ ë†’ì€ ê²ƒì„ ë³¼ ìˆ˜ ìˆë‹¤. í•˜ì§€ë§Œ 4ë²ˆ ë…¸ë“œì˜ ê²½ìš° ë…¹ìƒ‰ ë…¸ë“œì™€ ì ìƒ‰ ë…¸ë“œ ê°ê° í•˜ë‚˜ì”© ì—°ê²°ë˜ì–´ ìˆê³ , ë…¹ìƒ‰ê³¼ ê°€ê¹Œìš´ ë…¸ë“œì™€ ì ìƒ‰ê³¼ ê°€ê¹Œìš´ ë…¸ë“œ í•˜ë‚˜ì”© ì—°ê²°ë˜ì–´ ìˆì–´ 0.5ì— ê°€ê¹Œìš´ í™•ë¥ ì„ ë³´ì´ê³  ìˆë‹¤. Example: After $2^{nd}$ Iteration . After Iteration 2 (ë°˜ë³µ í›„ 2) . . Example: After $3^{rd}$ Iteration . After Iteration 3 (ë°˜ë³µ í›„ 3) . . Example: After $4^{th}$ Iteration . After Iteration 4 (ë°˜ë³µ í›„ 4) . . ğŸ’¬ ì´í›„ 0.5ë³´ë‹¤ í™•ë¥ ì´ í° ë…¸ë“œë“¤ì€ class 1ì´ë¼ ë¶„ë¥˜í•˜ê³ , 0.5ë³´ë‹¤ ì‘ì€ ë…¸ë“œë“¤ì€ class 0ìœ¼ë¡œ ë¶„ë¥˜í•œë‹¤. class 0 : 1, 2, 3 class 1 : 4, 5, 6, 7, 8, 9 Example: Convergence . All scores stabilize after 4 iterations. . We therefore predict: . Nodes 4,5,8,9 belong to class 1 ($ğ‘ƒ_{Y_v}$ &gt; 0.5) | Nodes 3 belong to class 0 ($ğ‘ƒ_{Y_v}$ &lt; 0.5) | . | . . 4íšŒ ë°˜ë³µ í›„ ëª¨ë“  ì ìˆ˜ê°€ ì•ˆì •ë©ë‹ˆë‹¤. . ë”°ë¼ì„œ ë‹¤ìŒê³¼ ê°™ì´ ì˜ˆì¸¡í•œë‹¤. . ë…¸ë“œ 4,5,8,9ê°€ í´ë˜ìŠ¤ 1($ğ‘ƒ_{Y_v}$ &gt; 0.5)ì— ì†í•¨ | ë…¸ë“œ 3ì€ í´ë˜ìŠ¤ 0($ğ‘ƒ_{Y_v}$ &lt; 0.5)ì— ì†í•©ë‹ˆë‹¤. | . | . . ğŸ’¬ ëª‡ ë²ˆì˜ iterationsì„ ì§€ë‚˜ì ëª¨ë“  í™•ë¥ ê°’ì´ ìˆ˜ë ´í•˜ê³  ìˆëŠ” ëª¨ìŠµì„ ë³´ì—¬ ì¢…ë£Œë˜ì—ˆìœ¼ë‚˜ ì´ ëª¨ë¸ì˜ ìˆ˜ë ´ì´ ë³´ì¥ë˜ì§€ ì•ŠëŠ” ë‹¨ì ì´ ì¡´ì¬í•œë‹¤. ì´ë¥¼ ì´ì „ì— ë°°ì› ë˜ ê°œë…ê³¼ ì—°ê²°ì§€ì–´ ìƒê°í•˜ìë©´, Influenceê°€ ë…¹ì•„ìˆëŠ” ëª¨ë¸ì´ë¼ê³  í•  ìˆ˜ ìˆê² ë‹¤. ê°€ê¹Œìš´ ë…¸ë“œì˜ ì˜í–¥ì„ ë°›ì•„ ì´ì›ƒ ë…¸ë“œì™€ ë¹„ìŠ·í•œ ë ˆì´ë¸” ë¶„í¬ë¥¼ ê°€ì§€ë„ë¡ ì—…ë°ì´íŠ¸í•˜ê³  ìˆê¸° ë•Œë¬¸ì´ë‹¤. ê·¸ ê²°ê³¼ 8ë²ˆ ë…¸ë“œëŠ” ì´ì›ƒë…¸ë“œê°€ ë…¹ìƒ‰ì¼ í™•ë¥ ì´ ë†’ìœ¼ë‹ˆ í•´ë‹¹ ë¶„í¬ì™€ ë¹„ìŠ·í•´ì§€ê³ , 4ë²ˆ ë…¸ë“œëŠ” ì´ì›ƒë…¸ë“œë¡œ ë…¹ìƒ‰ê³¼ ì ìƒ‰ì— ê°€ê¹Œìš´ ë…¸ë“œë“¤ì´ ëª¨ë‘ ìˆì–´ 0.5ì— ê°€ê¹Œìš´ í™•ë¥ ì„ ê°€ì§€ê²Œ ë˜ì—ˆë‹¤. Relational Classificationì€ ê·¸ë˜í”„ì˜ êµ¬ì¡°ì  ì •ë³´ë¥¼ ì¼ë¶€ í™œìš©í•˜ê³ , ë…¸ë“œ ë ˆì´ë¸”ì€ í™œìš©í•˜ì§€ë§Œ, ë…¸ë“œì˜ ë³€ìˆ˜ = node featured information(attributes)ë¥¼ í™œìš©í•˜ì§€ ëª»í•œë‹¤ëŠ” ë‹¨ì ì´ í¬ê²Œ ì‘ìš©í•œë‹¤. ë‹¨ì§€ ë…¸ë“œì˜ ë¼ë²¨ê³¼ ì´ì›ƒë“¤ì˜ ì—£ì§€ë¥¼ ì´ìš©í•˜ëŠ” ê²ƒì¸ ë„¤íŠ¸ì›Œí¬ ì •ë³´ë§Œ ì‚¬ìš©í•˜ê²Œ ëœë‹¤. ê²°êµ­ ì£¼ì–´ì§„ ì •ë³´ë¥¼ ìµœëŒ€í•œ í™œìš©í•˜ì§€ ëª»í•˜ëŠ” ë¨¸ì‹ ëŸ¬ë‹ ëª¨ë¸ì€ ë¶€ì¡±í•œ ì ì´ ë§ì€ ëª¨ë¸ì¼ ë¿ì´ë‹¤. Iterative Classification . Iterative Classification . Relational classifier does not use node attributes. | How can one leverage them? | Main idea of iterative classification: . Classify node $v$ based on its attributes $f_v$ as well as labels $z_v$ of neighbor set $N_v$. . | Input: Graph $f_{v}$ : feature vector for node $v$ | Some nodes $v$ are labeled with $Y_{v}$ | . | Task: Predict label of unlabeled nodes | Approach: Train two classifiers: $ phi_{1} left(f_{v} right)=$ Predict node label based on node feature vector $f_{v}$. This is called base classifier. | $ phi_{2} left(f_{v}, z_{v} right)=$ Predict label based on node feature vector $f_{v}$ and summary $z_{v}$ of labels of $vâ€™{ text {s }}$ neighbors. . This is called relational classifier. . | . . ê´€ê³„ ë¶„ë¥˜ìê°€ ë…¸ë“œ íŠ¹ì„±ì„ ì‚¬ìš©í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤. | ì–´ë–»ê²Œ ê·¸ë“¤ì„ í™œìš©í•  ìˆ˜ ìˆì„ê¹Œ? | ë°˜ë³µ ë¶„ë¥˜ì˜ ì£¼ìš” ê°œë…: . ë…¸ë“œ $v$ëŠ” ì†ì„± $f_v$ì™€ ì¸ì ‘ ì„¸íŠ¸ $N_v$ì˜ ë ˆì´ë¸” $z_v$ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ë¶„ë¥˜í•œë‹¤. . | ì…ë ¥: ê·¸ë˜í”„ $f_{v}$ : ë…¸ë“œ $v$ì˜ í”¼ì³ ë²¡í„° | ì¼ë¶€ ë…¸ë“œ $v$ëŠ” $Y_{v}$ë¡œ ë ˆì´ë¸” ì§€ì •ë¨ | . | ì‘ì—…: ë ˆì´ë¸”ì´ ì—†ëŠ” ë…¸ë“œì˜ ë ˆì´ë¸” ì˜ˆì¸¡ | ì ‘ê·¼ë²•: ë‘ ê°€ì§€ ë¶„ë¥˜ê¸° í›ˆë ¨: $ phi_{1} left(f_{v} right)=$ ë…¸ë“œ íŠ¹ì§• ë²¡í„° $f_{v}$ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë…¸ë“œ ë ˆì´ë¸”ì„ ì˜ˆì¸¡í•œë‹¤. ì´ë¥¼ ê¸°ë³¸ ë¶„ë¥˜ê¸°ë¼ê³  í•©ë‹ˆë‹¤. | $ phi_{2} left(f_{v}, z_{v} right)=$ ë…¸ë“œ íŠ¹ì§• ë²¡í„° $f_{v}$ì™€ $vâ€™{ text {s}}$ ì´ì›ƒ ë ˆì´ë¸”ì˜ ìš”ì•½ $z_{v}$ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë ˆì´ë¸”ì„ ì˜ˆì¸¡í•œë‹¤. . ì´ë¥¼ ê´€ê³„ ë¶„ë¥˜ê¸°ë¼ê³  í•©ë‹ˆë‹¤. . | . . ğŸ’¬ Relational ClassifierëŠ” ë…¸ë“œì˜ ë³€ìˆ˜ë¥¼ í™œìš©í•˜ì§€ ì•ŠëŠ” ê²ƒì´ ë‹¨ì ì´ë¼ê³  í–ˆë‹¤. Iterative ClassifierëŠ” ë…¸ë“œì˜ ë³€ìˆ˜ë¥¼ í™œìš©í•˜ì—¬ ì´ë¥¼ ê°œì„ í–ˆë‹¤. í•µì‹¬ ì•„ì´ë””ì–´ëŠ” ë‹¤ìŒê³¼ ê°™ë‹¤. &gt; ë…¸ë“œ $`v`$ë¥¼ ë¶„ë¥˜í•  ë•Œ, ë…¸ë“œì˜ ë³€ìˆ˜ $`f_v`$ë¥¼ ì´ì›ƒë…¸ë“œ ì§‘í•© $`N_v`$ì˜ ë ˆì´ë¸” $`z_u`$ì™€ í•¨ê»˜ ì‚¬ìš©í•˜ì. ì…ë ¥ì€ ê·¸ë˜í”„ë¥¼ ì‚¬ìš©í•˜ë©°, $f_v$ëŠ” ë…¸ë“œ $v$ì˜ ë³€ìˆ˜ ë²¡í„°ë¥¼ ì˜ë¯¸í•˜ë©°, ì—¬ê¸°ì„œ ì¼ë¶€ ë…¸ë“œëŠ” ë ˆì´ë¸” $Y_v$ë¥¼ ê°–ëŠ”ë‹¤. ëª©í‘œëŠ” ë ˆì´ë¸”ì´ ì—†ëŠ” ë…¸ë“œì— ëŒ€í•´ ë ˆì´ë¸”ì„ ì˜ˆì¸¡í•˜ëŠ” ê²ƒì´ë‹¤. ì´ë¥¼ ìœ„í•´ í™œìš©í•˜ëŠ” ê²ƒì€ ë‘ ê°œì˜ ë¶„ë¥˜ê¸°ë¥¼ í™œìš©í•œë‹¤. 1. $ phi_{1} left(f_{v} right)$ : ë…¸ë“œ $v$ì˜ ë³€ìˆ˜ $f_v$ ë§Œì„ ì´ìš©í•´ ë ˆì´ë¸”ì„ ì˜ˆì¸¡í•˜ëŠ” ëª¨ë¸ 2. $ phi_{2} left(f_{v}, z_{v} right)$: ë…¸ë“œ $v$ì˜ ë³€ìˆ˜ $*f_v*$ì™€ ì´ì›ƒ ë…¸ë“œì˜ ë ˆì´ë¸”ì— ëŒ€í•œ ê¸°ìˆ  í†µê³„ë²¡í„° $*z_v*$ë¥¼ ì´ìš©í•˜ì—¬ ë ˆì´ë¸”ì„ ì˜ˆì¸¡í•˜ëŠ” ëª¨ë¸ $z_u$: ì´ì›ƒ ë…¸ë“œì˜ ë¼ë²¨ì„ í‘œí˜„í•˜ëŠ” ë²¡í„° (ë¼ë²¨ì˜ ë¹„ìœ¨, ê°œìˆ˜ ë“±ìœ¼ë¡œ í‘œí˜„ëœë‹¤.) Computing the Summary $z_v$ . How do we compute the summary $z_v$ of labels of $vâ€™s$ neighbors $N_v$? . $z_v$ = vector that captures labels around node $v$ Histogram of the number (or fraction) of each label in $N_v$ | Most common label in $N_v$ | Number of different labels in $N_v$ | . | . . $v$ì˜ ì¸ì ‘ $N_v$ ë ˆì´ë¸”ì˜ ìš”ì•½ $z_v$ëŠ” ì–´ë–»ê²Œ ê³„ì‚°í•©ë‹ˆê¹Œ? . $z_v$ = ë…¸ë“œ $v$ ì£¼ë³€ì˜ ë ˆì´ë¸”ì„ ìº¡ì²˜í•˜ëŠ” ë²¡í„° $N_v$ ë‹¨ìœ„ì˜ ê° ë ˆì´ë¸”ì˜ ìˆ«ì(ë˜ëŠ” ë¶€ë¶„) íˆìŠ¤í† ê·¸ë¨ | $N_v$ì˜ ê°€ì¥ ì¼ë°˜ì ì¸ ë ˆì´ë¸” | $N_v$ì˜ ì„œë¡œ ë‹¤ë¥¸ ë ˆì´ë¸” ìˆ˜ | . | . . ğŸ’¬ ìœ„ì™€ ê°™ì€ ê·¸ë˜í”„ì—ì„œ ì²­ìƒ‰ ë…¸ë“œì— ëŒ€í•œ $z_{v}$ ëŠ” ì´ì›ƒ ë…¸ë“œì˜ ìƒ‰ì˜ count ë¶„í¬ë‚˜ ì¡´ì¬ ìœ ë¬´ ë¶„í¬, ë¹„ìœ¨ ë¶„í¬ ë“±ì„ ì‚¬ìš©í•´ ë§Œë“¤ ìˆ˜ ìˆë‹¤. - count ë¶„í¬ : [ë…¹ìƒ‰ ë…¸ë“œì˜ ìˆ˜, ì ìƒ‰ ë…¸ë“œì˜ ìˆ˜] = $[2,1]$ - ì¡´ì¬ ìœ ë¬´ ë¶„í¬ : [ë…¹ìƒ‰ ë…¸ë“œ ì¡´ì¬ ì—¬ë¶€, ì ìƒ‰ ë…¸ë“œ ì¡´ì¬ ì—¬ë¶€]= $[1,1]$ - ë¹„ìœ¨ ë¶„í¬ : [ë…¹ìƒ‰ ë…¸ë“œ ë¹„ìœ¨, ì ìƒ‰ ë…¸ë“œ ë¹„ìœ¨] =$ left[ frac{2}{3}, frac{1}{3} right]$ ë‘ ë¶„ë¥˜ê¸°ë¥¼ ì´ìš©í•˜ì—¬ í•™ìŠµê³¼ ì˜ˆì¸¡ ê³¼ì •ì´ ì¡°ê¸ˆ ë³µì¡í•œë° í¬ê²Œ ë‘ ë‹¨ê³„ë¡œ ë‚˜ëˆŒ ìˆ˜ ìˆë‹¤. Architecture of Iterative Classifiers . Phase 1: Classify based on node attributes alone On the labeled training set, train two classifiers: Base classifier: $ phi_{1} left(f_{v} right)$ to predict $Y_{v}$ based on $f_{v}$ | Relational classifier: $ phi_{2} left(f_{v}, z_{v} right)$ to predict $Y_{v}$ based on $f_{v}$ and summary $z_{v}$ of labels of $v^{ prime}$s neighbors | . | . | Phase 2: Iterate till convergence On test set, set labels $Y_{v}$ based on the classifier $ phi_{1}$, compute $z_{v}$ and predict the labels with $ phi_{2}$ | Repeat for each node $v$ : Update $z_{v}$ based on $Y_{u}$ for all $u in N_{v}$ | Update $Y_{v}$ based on the new $z_{v} left( phi_{2} right)$ | . | Iterate until class labels stabilize or max number of iterations is reached | Note: Convergence is not guaranteed | . | . . 1ë‹¨ê³„: ë…¸ë“œ ì†ì„±ë§Œì„ ê¸°ì¤€ìœ¼ë¡œ ë¶„ë¥˜ ë¼ë²¨ì´ ë¶€ì°©ëœ êµìœ¡ ì„¸íŠ¸ì—ì„œ ë‘ ê°€ì§€ ë¶„ë¥˜ê¸°ë¥¼ êµìœ¡í•©ë‹ˆë‹¤. ê¸°ë³¸ ë¶„ë¥˜ì: $f_{v}$ë¥¼ ê¸°ë°˜ìœ¼ë¡œ $Y_{v}$ë¥¼ ì˜ˆì¸¡í•˜ê¸° ìœ„í•œ $ phi_{1} left(f_{v} right)$ | ê´€ê³„ ë¶„ë¥˜ì: $f_{v}$ì™€ $v^{ prime}$s ì¸ì ‘ ë ˆì´ë¸”ì˜ ìš”ì•½ $z_{v}$ë¥¼ ê¸°ë°˜ìœ¼ë¡œ $Y_{v}$ë¥¼ ì˜ˆì¸¡í•˜ê¸° ìœ„í•œ $ phi_{2} left(f_{v}, z_{v} right)$ | . | . | 2ë‹¨ê³„: ìˆ˜ë ´ë  ë•Œê¹Œì§€ ë°˜ë³µ í…ŒìŠ¤íŠ¸ ì„¸íŠ¸ì—ì„œ $ phi_{1}$ ë¶„ë¥˜ê¸°ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ë ˆì´ë¸” $Y_{v}$ì„ ì„¤ì •í•˜ê³  $z_{v}$ë¥¼ ê³„ì‚°í•˜ê³  $ phi_{2}$ë¡œ ë ˆì´ë¸”ì„ ì˜ˆì¸¡í•œë‹¤. | ê° ë…¸ë“œ $v$ì— ëŒ€í•´ ë°˜ë³µ: ëª¨ë“  $u in N_{v}$ì— ëŒ€í•´ $Y_{u}$ë¥¼ ê¸°ì¤€ìœ¼ë¡œ $z_{v}$ ì—…ë°ì´íŠ¸ | ìƒˆ $z_{v} left( phi_{2} right)$ë¥¼ ê¸°ì¤€ìœ¼ë¡œ $Y_{v}$ ì—…ë°ì´íŠ¸ | . | í´ë˜ìŠ¤ ë ˆì´ë¸”ì´ ì•ˆì •ë˜ê±°ë‚˜ ìµœëŒ€ ë°˜ë³µ íšŸìˆ˜ì— ë„ë‹¬í•  ë•Œê¹Œì§€ ë°˜ë³µ | ì°¸ê³ : ìˆ˜ë ´ì´ ë³´ì¥ë˜ì§€ ì•ŠìŒ | . | . . ğŸ’¬ Phase 1 : **Train** í•™ìŠµ ë°ì´í„°ì˜ ê²½ìš°ì— ëª¨ë“  ë…¸ë“œì— ë ˆì´ë¸”ì´ ë‹¬ë ¤ìˆë‹¤ê³  ê°„ì£¼í•˜ê³  ë‘ ë¶„ë¥˜ê¸°ë¥¼ í•™ìŠµí•œë‹¤. 1. $ phi_{1} left(f_{v} right): f_{v}$ ë¥¼ ì´ìš©í•´ $Y_{v}$ ë¥¼ ì˜ˆì¸¡í•œë‹¤. : ë…¸ë“œ í”¼ì³ ì •ë³´ë“¤ë§Œì„ ì´ìš©í•˜ì—¬ ë…¸ë“œë¼ë²¨ì„ ì˜ˆì¸¡í•˜ëŠ” ëª¨ë¸ 2. $ phi_{2} left(f_{v}, z_{v} right): f_{v}$ ì™€ $z_{v}$ ë¥¼ ì´ìš©í•´ $Y_{v}$ ë¥¼ ì˜ˆì¸¡í•œë‹¤. ì´ë•Œ, $z_{v}$ ëŠ” ì‹¤ì œ ë ˆì´ë¸”ì„ ì´ìš©í•´ êµ¬ì„±í•œë‹¤. : ë…¸ë“œ í”¼ì³ ì •ë³´ + ì´ì›ƒë“¤ì˜ ë¼ë²¨ì •ë³´ë¥¼ ì´ìš©í•˜ì—¬ ë…¸ë“œ ë¼ë²¨ì„ ì˜ˆì¸¡í•˜ëŠ” ëª¨ë¸ Phase 2 : **Inference** í…ŒìŠ¤íŠ¸ ë°ì´í„°ì˜ ê²½ìš°ì—” ì¼ë¶€ ë…¸ë“œì—ë§Œ ë ˆì´ë¸”ì´ ë‹¬ë ¤ìˆë‹¤ê³  ê°„ì£¼í•œë‹¤. í˜¹ì€ ë ˆì´ë¸”ì´ ì•„ì˜ˆ ì—†ì„ ìˆ˜ë„ ìˆë‹¤ê³  ê°„ì£¼í•œë‹¤. ì´ë•Œ $ phi_{1} left(f_{v} right)$ ëŠ” $f_{v}$ ê°€ ë³€í•˜ì§€ ì•Šê¸° ë•Œë¬¸ì— ì´ˆê¸°ì— í•œë²ˆë§Œ ê³„ì‚°í•˜ì—¬ ë¼ë²¨ $Y_{v}$ ë¥¼ ì˜ˆì¸¡í•œë‹¤. ì´ë¥¼ í†µí•´ ëª¨ë“  ë…¸ë“œì— $Y_{v}$ ê°€ í• ë‹¹ëœë‹¤. ëª¨ë“  ë…¸ë“œì— $Y_{v}$ ê°€ í• ë‹¹ëœ í›„ ë‹¤ìŒê³¼ ê°™ì€ ê³¼ì •ì„ ìˆ˜ë ´í•˜ê±°ë‚˜ ìµœëŒ€ë°˜ë³µíšŸìˆ˜ì— ë„ë‹¬í•  ë•Œê¹Œì§€ ë°˜ë³µí•œë‹¤. 1. ìƒˆë¡œìš´ $Y_{v}$ ì— ë§ì¶”ì–´ $z_{u}$ ë¥¼ ì—…ë°ì´íŠ¸í•œë‹¤. $ left(u in N_{v} right)$ 2. ìƒˆë¡œìš´ $z_{u}$ ì— ë§ì¶”ì–´ $Y_{z}= phi_{2} left(f_{v}, z_{v} right)$ ë¥¼ ì—…ë°ì´íŠ¸í•œë‹¤. ë‹¤ë§Œ, í•´ë‹¹ ëª¨ë¸ ë˜í•œ ìˆ˜ë ´ì„ ë³´ì¥í•˜ì§€ ì•Šê¸°ì—, ìµœëŒ€ ë°˜ë³µ íšŸìˆ˜ë¥¼ ì§€ì •í•œë‹¤. Example: Web Page Classification . Input: Graph of web pages | Node: Web page | Edge: Hyper-link between web pages Directed edge: a page points to another page | . | Node features: Webpage description For simplicity, we only consider two binary features | . | Task: Predict the topic of the webpage | Baseline: Train a classifier (e.g., linear classifier) to classify pages based on node attributes. | . . ì…ë ¥: ì›¹í˜ì´ì§€ ê·¸ë˜í”„ | ë…¸ë“œ: ì›¹ í˜ì´ì§€ | ê°€ì¥ìë¦¬: ì›¹ í˜ì´ì§€ ê°„ì˜ í•˜ì´í¼ë§í¬ (Directed Edge) ë°©í–¥ ê°€ì¥ìë¦¬: í•œ í˜ì´ì§€ê°€ ë‹¤ë¥¸ í˜ì´ì§€ë¥¼ ê°€ë¦¬í‚¤ë‹¤ | . | ë…¸ë“œ ê¸°ëŠ¥: ì›¹ í˜ì´ì§€ ì„¤ëª… (TF-IDF ë“±ì˜ í† í° ì •ë³´, ì—¬ê¸°ì„  2ì°¨ì› ë²¡í„°ë¡œ í‘œí˜„) ë‹¨ìˆœì„±ì„ ìœ„í•´ ë‘ ê°œì˜ ì´ì§„ ê¸°ëŠ¥ë§Œ ê³ ë ¤í•œë‹¤. | . | ì‘ì—…: ì›¹ í˜ì´ì§€ì˜ ì£¼ì œ ì˜ˆì¸¡ | ê¸°ì¤€: ë…¸ë“œ ì†ì„±ì„ ê¸°ì¤€ìœ¼ë¡œ í˜ì´ì§€ë¥¼ ë¶„ë¥˜í•˜ë„ë¡ ë¶„ë¥˜ê¸°(ì˜ˆ: ì„ í˜• ë¶„ë¥˜ê¸°)ë¥¼ í›ˆë ¨í•©ë‹ˆë‹¤. | . . . ğŸ’¬ Web Pageì˜ ì£¼ì œë¥¼ ì˜ˆì¸¡í•˜ì—¬ ë¶„ë¥˜í•˜ê¸° ìœ„í•´ì„œ ì•„ë˜ì˜ ë°ì´í„°ê°€ ì‚¬ìš©ëœë‹¤. - Input : ì›¹í˜ì´ì§€ ê·¸ë˜í”„ - Node : ì›¹í˜ì´ì§€ - Edge : ì›¹í˜ì´ì§€ ê°„ í•˜ì´í¼ë§í¬(Directed Edge) - Node Features : ì›¹í˜ì´ì§€ ì •ë³´(TF-IDF ë“±ì˜ í† í° ì •ë³´, ì—¬ê¸°ì„  2ì°¨ì› ë²¡í„°ë¡œ í‘œí˜„) - Task : ê° ì›¹í˜ì´ì§€ì˜ ì£¼ì œ ì˜ˆì¸¡ . Each node maintains vectors $z_v$ of neighborhood labels: $I$ = Incoming neighbor label information vector. | $O$ = Outgoing neighbor label information vector. $I_0$ = 1 if at least one of the incoming pages is labelled 0. . Similar definitions for $I_0$, $O_0$, and $O_1$ . | . | . | . . ê° ë…¸ë“œëŠ” ì´ì›ƒ ë ˆì´ë¸”ì˜ ë²¡í„° $z_v$ë¥¼ ìœ ì§€í•œë‹¤. $I$ = ë“¤ì–´ì˜¤ëŠ” ì¸ì ‘ ë ˆì´ë¸” ì •ë³´ ë²¡í„°. | $O$ = ë‚˜ê°€ëŠ” ì¸ì ‘ ë ˆì´ë¸” ì •ë³´ ë²¡í„°. $I_0$ = 1 (ìˆ˜ì‹  í˜ì´ì§€ ì¤‘ í•˜ë‚˜ ì´ìƒì´ 0ìœ¼ë¡œ í‘œì‹œëœ ê²½ìš°) . $I_0$, $O_0$ ë° $O_1$ì— ëŒ€í•œ ìœ ì‚¬í•œ ì •ì˜ . | . | . | . . . ğŸ’¬ $f_{v}$ : ë³€ìˆ˜ ë²¡í„° (TF_IDFë“±) $I$ : incoming neighbor ë ˆì´ë¸”ì— ëŒ€í•œ ê¸°ìˆ  í†µê³„ì¹˜ ë²¡í„°([0ì¸ ì´ì›ƒë…¸ë“œ ìœ ë¬´, 1ì¸ ì´ì›ƒë…¸ë“œ ìœ ë¬´ $O$ : outgoing neighbor ë ˆì´ë¸”ì— ëŒ€í•œ ê¸°ìˆ  í†µê³„ì¹˜ ë²¡í„°([0ì¸ ì´ì›ƒë…¸ë“œ ìœ ë¬´, 1ì¸ ì´ì›ƒë…¸ë“œ ìœ ë¬´ $*z_v*$ë¥¼ ì—¬ëŸ¬ ë°©ë²•ë“¤ì¤‘ ë“¤ì–´ì˜¤ê³  ë‚˜ì˜¤ëŠ” ì—£ì§€ë“¤ì˜ ë¼ë²¨ê°œìˆ˜ë¡œ ì„¤ì •í•˜ê¸°ë¡œ í•œë‹¤. ë”°ë¼ì„œ íšŒìƒ‰ ë…¸ë“œì—ì„œëŠ” 1ë²ˆì¸ ì´ˆë¡ ë…¸ë“œì—ì„œë§Œ ì—£ì§€ê°€ ë“¤ì–´ì˜¤ê³  0ë²ˆ í´ë˜ìŠ¤ì¸ ë…¸ë“œì—ì„œ ë“¤ì–´ì˜¤ëŠ”ê²ƒì´ ì—†ìœ¼ë¯€ë¡œ $I=[0,1]$ì´ ëœë‹¤. ë˜í•œ 0ë²ˆ, 1ë²ˆ í´ë˜ìŠ¤ ë…¸ë“œë¡œ ëª¨ë‘ ë‚˜ê°€ë¯€ë¡œ $O=[1,1]$ìœ¼ë¡œ êµ¬ì„±í•œë‹¤. Iterative Classifier - Step 1 . On training labels, train two classifiers: Node attribute vector only: $ phi_1(f_v)$ | Node attribute and link vectors $z_v$: $ phi_2(f_v,z_v)$ | . | . Train classifiers | Apply classifier to unlab. set | Iterate | Update relational features $z_v$ | Update label $Y_v$ | . êµìœ¡ ë¼ë²¨ì—ì„œ ë‘ ê°€ì§€ ë¶„ë¥˜ê¸°ë¥¼ êµìœ¡í•©ë‹ˆë‹¤. ë…¸ë“œ ì†ì„± ë²¡í„°ë§Œ: $ phi_1(f_v)$ | ë…¸ë“œ ì†ì„± ë° ë§í¬ ë²¡í„° $z_v$: $ phi_2(f_v,z_v)$ | . | . ë¶„ë¥˜ê¸° í•™ìŠµ | ë ˆì´ë¸” í•´ì œ ì„¸íŠ¸ì— ë¶„ë¥˜ì ì ìš© | ë°˜ë³µ | ê´€ê³„ ê¸°ëŠ¥ ì—…ë°ì´íŠ¸ $z_v$ | ë ˆì´ë¸” ì—…ë°ì´íŠ¸ $Y_v$ | . . ğŸ’¬ $ phi_1(f_v)$ì—ì„œëŠ” í”¼ì³ ì •ë³´($f_v$)ë§Œì„ ì‚¬ìš©í•˜ë©°, $ phi_2(f_v,z_v)$ëŠ” í”¼ì³ ì •ë³´($f_v$)ì™€ ì´ì›ƒë¼ë²¨ ì •ë³´($I,O)$ë¥¼ ì´ìš©í•˜ì—¬ í•™ìŠµí•œë‹¤. Iterative Classifier - Step 2 . On the unlabeled set: Use trained node feature **vector classifier $ phi_1$ to set $Y_v**$ | . | . Train classifiers | Apply classifier to unlab. set | Iterate | Update relational features $z_v$ | Update label $Y_v$ | . ë¼ë²¨ì´ ì—†ëŠ” ì„¸íŠ¸ì—ì„œ: í›ˆë ¨ëœ ë…¸ë“œ íŠ¹ì§• ë²¡í„° ë¶„ë¥˜ê¸° $ phi_1$ë¥¼ ì‚¬ìš©í•˜ì—¬ $Y_v$ ì„¤ì • | . | . ë¶„ë¥˜ê¸° í•™ìŠµ | ë ˆì´ë¸” í•´ì œ ì„¸íŠ¸ì— ë¶„ë¥˜ì ì ìš© | ë°˜ë³µ | ê´€ê³„ ê¸°ëŠ¥ ì—…ë°ì´íŠ¸ $z_v$ | ë ˆì´ë¸” ì—…ë°ì´íŠ¸ $Y_v$ | . ğŸ’¬ $ phi_1$ì´ ë¶€ì—¬í•œ ë¼ë²¨ì„ ì´ìš©í•´ $z_v$ë¥¼ ì—…ë°ì´íŠ¸í•œë‹¤. Iterative Classifier - Step 3.1 . Update $z_v$ for all nodes: | . Train classifiers | Apply classifier to unlab. set | Iterate | Update relational features $z_v$ | Update label $Y_v$ | . ëª¨ë“  ë…¸ë“œì— ëŒ€í•´ $z_v$ ì—…ë°ì´íŠ¸: | . ë¶„ë¥˜ê¸° í•™ìŠµ | ë ˆì´ë¸” í•´ì œ ì„¸íŠ¸ì— ë¶„ë¥˜ì ì ìš© | ë°˜ë³µ | ê´€ê³„ ê¸°ëŠ¥ ì—…ë°ì´íŠ¸ $z_v$ | ë ˆì´ë¸” ì—…ë°ì´íŠ¸ $Y_v$ | . . ğŸ’¬ $ phi_2$ê°€ ì—…ë°ì´íŠ¸ ëœ $z_v$ì™€ $I$, $O$ ì •ë³´ë¥¼ ì‚¬ìš©í•˜ì—¬ ë¼ë²¨ì„ ì˜ˆì¸¡í•œë‹¤. Iterative Classifier - Step 3.2 . **Re-classify all nodes with $ phi_2$: | . Train classifiers | Apply classifier to unlab. set | Iterate | Update relational features $z_v$ | Update label $Y_v$ | . $ phi_2$ë¡œ ëª¨ë“  ë…¸ë“œë¥¼ ë‹¤ì‹œ ë¶„ë¥˜í•©ë‹ˆë‹¤. | . ë¶„ë¥˜ê¸° í•™ìŠµ | ë ˆì´ë¸” í•´ì œ ì„¸íŠ¸ì— ë¶„ë¥˜ì ì ìš© | ë°˜ë³µ | ê´€ê³„ ê¸°ëŠ¥ ì—…ë°ì´íŠ¸ $z_v$ | ë ˆì´ë¸” ì—…ë°ì´íŠ¸ $Y_v$ | . ğŸ’¬ $ phi_2$ì— ì˜í•´ ë¼ë²¨ì´ ë³€í™”í–ˆìœ¼ë¯€ë¡œ, ë‹¤ì‹œ $z_v$ë¥¼ ì—…ë°ì´íŠ¸í•œë‹¤. Iterative Classifier - Iterate . Continue until convergence Update $z_v$ based on $Y_v$ | Update $Y_v$ = $ phi_2(f_v,z_v)$ | . | . Train classifiers | Apply classifier to unlab. set | Iterate | Update relational features $z_v$ | Update label $Y_v$ | . ì •í•©ë  ë•Œê¹Œì§€ ê³„ì†í•©ë‹ˆë‹¤ $Y_v$ë¥¼ ê¸°ì¤€ìœ¼ë¡œ $z_v$ ì—…ë°ì´íŠ¸ | ì—…ë°ì´íŠ¸ $Y_v$= $ phi_2(f_v,z_v)$ | . | . ë¶„ë¥˜ê¸° í•™ìŠµ | ë ˆì´ë¸” í•´ì œ ì„¸íŠ¸ì— ë¶„ë¥˜ì ì ìš© | ë°˜ë³µ | ê´€ê³„ ê¸°ëŠ¥ ì—…ë°ì´íŠ¸ $z_v$ | ë ˆì´ë¸” ì—…ë°ì´íŠ¸ $Y_v$ | . ğŸ’¬ $z_v$ê°€ ì—…ë°ì´íŠ¸ ë˜ì—ˆìœ¼ë¯€ë¡œ $ phi_2$ê°€ ë ˆì´ë¸”ì„ ì˜ˆì¸¡í•œë‹¤. Iterative Classifier - Final Prediction . Stop iteration After convergence or when maximum iterations are reached | . | . . ë°˜ë³µ ì¤‘ì§€ ìˆ˜ë ´ í›„ ë˜ëŠ” ìµœëŒ€ ë°˜ë³µ íšŸìˆ˜ì— ë„ë‹¬í•œ ê²½ìš° | . | . . . ğŸ’¬ $ phi_2$ë¥¼ í†µí•œ ì˜ˆì¸¡ê³¼ $z_v$ì— ëŒ€í•œ ì—…ë°ì´íŠ¸ë¥¼ ì¢…ë£Œ ê³ ì „ì— ë„ë‹¬í•  ë•Œê¹Œì§€ ë°˜ë³µí•œë‹¤. Summary . We talked about 2 approaches to collective classification | Relational classification Iteratively update probabilities of node belonging to a label class based on its neighbors | . | Iterative classification Improve over collective classification to handle attribute/feature information | Classify node ğ‘£ based on its features as well as labels of neighbors | . | . . ì§‘ë‹¨ ë¶„ë¥˜ì— ëŒ€í•œ 2ê°€ì§€ ì ‘ê·¼ë²•ì— ëŒ€í•´ ì´ì•¼ê¸°í–ˆìŠµë‹ˆë‹¤ | ê´€ê³„êµ¬ë¶„ ì¸ì ‘ ê´€ê³„ì— ë”°ë¼ ë ˆì´ë¸” í´ë˜ìŠ¤ì— ì†í•˜ëŠ” ë…¸ë“œì˜ í™•ë¥ ì„ ë°˜ë³µì ìœ¼ë¡œ ì—…ë°ì´íŠ¸í•©ë‹ˆë‹¤. | . | ë°˜ë³µêµ¬ë¶„ íŠ¹ì„±/íŠ¹ì§• ì •ë³´ë¥¼ ì²˜ë¦¬í•˜ê¸° ìœ„í•´ ì§‘í•© ë¶„ë¥˜ë³´ë‹¤ ê°œì„  | íŠ¹ì§•ê³¼ ì´ì›ƒì˜ labelì„ ê¸°ì¤€ìœ¼ë¡œ ë…¸ë“œ based ë¶„ë¥˜ | . | . . CS224W: Machine Learning with Graphs 2021 Lecture 5.2 - Relational and Iterative Classification .",
            "url": "https://cs224w-kor.github.io/blog/graph%20neural%20network/gnn/graph%20convolution%20network/gcn/2022/07/20/lecture-0502.html",
            "relUrl": "/graph%20neural%20network/gnn/graph%20convolution%20network/gcn/2022/07/20/lecture-0502.html",
            "date": " â€¢ Jul 20, 2022"
        }
        
    
  
    
        ,"post12": {
            "title": "Lecture 5.1 - Message passing and Node Classification",
            "content": ". Lecture 5 . Lecture 5.1 - Message passing and Node Classification | Lecture 5.2 - Relational and Iterative Classification | Lecture 5.3 - Collective Classification : Belief Propagation | . . 5.1 - Message passing and Node Classification . Message Passing and Node Classification . Todayâ€™s Lecture: outline . Main question today: Given a network with labels on some nodes, how do we assign labels to all other nodes in the network? Example: In a network, some nodes are fraudsters, and some other nodes are fully trusted. How do you find the other fraudsters and trustworthy nodes? We already discussed node embeddings as a method to solve this in Lecture 3 . . ì˜¤ëŠ˜ ì£¼ìš” ì§ˆë¬¸: ì¼ë¶€ ë…¸ë“œì— ë ˆì´ë¸”ì´ ìˆëŠ” ë„¤íŠ¸ì›Œí¬ì—ì„œ ë„¤íŠ¸ì›Œí¬ì˜ ë‹¤ë¥¸ ëª¨ë“  ë…¸ë“œì— ë ˆì´ë¸”ì„ í• ë‹¹í•˜ë ¤ë©´ ì–´ë–»ê²Œ í•´ì•¼ í•©ë‹ˆê¹Œ? ì˜ˆ: ë„¤íŠ¸ì›Œí¬ì—ì„œ ì¼ë¶€ ë…¸ë“œëŠ” ì‚¬ê¸°ê¾¼ì´ê³  ë‹¤ë¥¸ ì¼ë¶€ ë…¸ë“œëŠ” ì™„ì „íˆ ì‹ ë¢°ë©ë‹ˆë‹¤. ë‹¤ë¥¸ ì‚¬ê¸°ê¾¼ê³¼ ì‹ ë¢°í•  ìˆ˜ ìˆëŠ” ë…¸ë“œë¥¼ ì–´ë–»ê²Œ ìƒê°í•˜ì‹­ë‹ˆê¹Œ? ìš°ë¦¬ëŠ” ì´ê²ƒì„ í•´ê²°í•˜ê¸° ìœ„í•œ ë°©ë²•ìœ¼ë¡œ ë…¸ë“œ ì„ë² ë”©ì„ ì´ë¯¸ ê°•ì˜ 3ì—ì„œ ë…¼ì˜í•˜ì˜€ìŠµë‹ˆë‹¤ . . ğŸ’¬ ì´ë²ˆ ìˆ˜ì—…ì—ì„œ ë‹¤ë£¬ ë‚´ìš©ì€ í•œ ê·¸ë˜í”„ì—ì„œ íŠ¹ì • ë…¸ë“œë“¤ì— ë ˆì´ë¸”ì— ë§¤ê²¨ì ¸ ìˆì„ ë•Œ, ë‹¤ë¥¸ ë…¸ë“œë“¤ì— ë ˆì´ë¸”ì„ ë§¤ê¸°ëŠ” ë°©ë²•ì— ëŒ€í•´ ë‹¤ë£¨ê²Œ ëœë‹¤. Example: Node Classification . . Given labels of some nodes Letâ€™s predict labels of unlabeled nodes This is called semi-supervised node classification . . ì¼ë¶€ ë…¸ë“œì˜ ì§€ì •ëœ ë ˆì´ë¸” ë ˆì´ë¸”ì´ ì—†ëŠ” ë…¸ë“œì˜ ë ˆì´ë¸”ì„ ì˜ˆì¸¡í•´ ë´…ì‹œë‹¤. ì´ë¥¼ ì¤€ì§€ë„ ë…¸ë“œ ë¶„ë¥˜ë¼ê³  í•©ë‹ˆë‹¤. . . ğŸ’¬ ìœ„ ê·¸ë¦¼ê³¼ ê°™ì´ ê·¸ë˜í”„ ì „ì²´ì—ì„œ ì¼ë¶€ ë…¸ë“œì— ë ˆì´ë¸”ì´ ìˆê³ , ë‚˜ë¨¸ì§€ ë…¸ë“œì—ëŠ” ë ˆì´ë¸”ì´ ì—†ì„ ë•Œ, ì´ë¥¼ ì˜ˆì¸¡í•˜ëŠ” ë°©ë²•ë¡ ì„ ë‹¤ë£¬ë‹¤. Todayâ€™s Lecture: outline . Main question today: Given a network with labels on some nodes, how do we assign labels to all other nodes in the network? | Today we will discuss an alternative framework: Message passing | Intuition: Correlations (dependencies) exist in networks. In other words: Similar nodes are connected. | Key concept is collective classification: Idea of assigning labels to all nodes in a network together. | . | We will look at three techniques today: Relational classification | Iterative classification | Correct &amp; Smooth | . | . . ì˜¤ëŠ˜ ì£¼ìš” ì§ˆë¬¸: ì¼ë¶€ ë…¸ë“œì— ë ˆì´ë¸”ì´ ìˆëŠ” ë„¤íŠ¸ì›Œí¬ì—ì„œ ë„¤íŠ¸ì›Œí¬ì˜ ë‹¤ë¥¸ ëª¨ë“  ë…¸ë“œì— ë ˆì´ë¸”ì„ í• ë‹¹í•˜ë ¤ë©´ ì–´ë–»ê²Œ í•´ì•¼ í•©ë‹ˆê¹Œ? | ì˜¤ëŠ˜ì€ ëŒ€ì²´ í”„ë ˆì„ì›Œí¬ì— ëŒ€í•´ ë…¼ì˜í•˜ê² ìŠµë‹ˆë‹¤: ë©”ì‹œì§€ ì „ë‹¬ | ì§ê´€: ìƒê´€ ê´€ê³„(ì˜ì¡´ì„±) ë„¤íŠ¸ì›Œí¬ì— ì¡´ì¬í•˜ë‹¤ ì¦‰, ìœ ì‚¬í•œ ë…¸ë“œê°€ ì—°ê²°ë˜ì–´ ìˆìŠµë‹ˆë‹¤. | í•µì‹¬ ê°œë…ì€ ì§‘ë‹¨ ë¶„ë¥˜: ë„¤íŠ¸ì›Œí¬ì˜ ëª¨ë“  ë…¸ë“œì— labelì„ í•¨ê»˜ í• ë‹¹í•˜ëŠ” ì•„ì´ë””ì–´ì…ë‹ˆë‹¤. | . | ì˜¤ëŠ˜ì€ ì„¸ ê°€ì§€ ê¸°ë²•ì„ ì‚´í´ë³´ê² ìŠµë‹ˆë‹¤. ê´€ê³„êµ¬ë¶„ | ë°˜ë³µêµ¬ë¶„ | ì •í™•í•˜ê³  ë§¤ë„ëŸ¬ìš´ | . | . . ğŸ’¬ ì´ë¥¼ Semi-supervised node classificationì´ë¼ í•œë‹¤ê³  í•œë‹¤. ì´ë¯¸ ë ˆì´ë¸”ì´ ìˆëŠ” ì •ë³´ì™€, ì—†ëŠ” êµ¬ì¡°ë¥¼ í•™ìŠµí•˜ì—¬ ìƒˆë¡œì´ ë ˆì´ë¸”ì„ ì˜ˆì¸¡í•˜ê¸° ë•Œë¬¸ìœ¼ë¡œ ë³´ì¸ë‹¤. ì´ë¥¼ Message Passingì´ë¼ëŠ” í”„ë ˆì„ ì›Œí¬ë¥¼ ì‚´í´ë³´ê²Œ ë˜ëŠ”ë°, message passing ì˜ ì£¼ìš” ê°œë…ì€ ìƒê´€ê´€ê³„(correlation)ì´ë‹¤. ì¦‰, ë¹„ìŠ·í•œ ë…¸ë“œ ê°„ì—ëŠ” ìƒê´€ê´€ê³„ê°€ ìˆì„ ê²ƒì´ê¸° ë•Œë¬¸ì—, ë…¸ë“œ ê°„ì— ìƒê´€ê´€ê³„ë¥¼ íŒŒì•…í•˜ì—¬ ì´ë¥¼ ì´ìš©í•´ ë ˆì´ë¸”ì„ ì˜ˆì¸¡í•˜ê³ ì í•œë‹¤. ì—¬ê¸°ì„œ ë°˜ë³µì ìœ¼ë¡œ labeling ì‘ì—…ì´ ë°œìƒí•˜ê¸° ë•Œë¬¸ì—, ì´ì „ ìˆ˜ì—…ì¸ Pagerankì™€ ë¹„ìŠ·í•œ ì ì´ ìˆë‹¤ê³  ë„˜ì–´ê°€ì. Correlations Exist in Networks . Behaviors of nodes are correlated across the links of the network | Correlation: Nearby nodes have the same color (belonging to the same class) | . . ë…¸ë“œì˜ ë™ì‘ì€ ë„¤íŠ¸ì›Œí¬ ë§í¬ ì „ì²´ì—ì„œ ìƒê´€ë¨ | ìƒê´€: ê·¼ì²˜ ë…¸ë“œì˜ ìƒ‰ìƒì€ ë™ì¼í•©ë‹ˆë‹¤(ë™ì¼í•œ í´ë˜ìŠ¤ì— ì†í•¨). | . . . Two explanations for why behaviors of nodes in networks are correlated: . | ë„¤íŠ¸ì›Œí¬ì—ì„œ ë…¸ë“œì˜ ë™ì‘ì´ ìƒê´€ë˜ëŠ” ì´ìœ ì— ëŒ€í•œ ë‘ ê°€ì§€ ì„¤ëª…ì€ ë‹¤ìŒê³¼ ê°™ë‹¤. . | . . . ğŸ’¬ ìœ„ì˜ ê·¸ë˜í”„ì—ì„œ, ë¹„ìŠ·í•œ ë…¸ë“œ(ê°™ì€ ë ˆì´ë¸”ì„ ê°€ì§€ê³  ìˆëŠ” ë…¸ë“œ, ì´ˆë¡ìƒ‰, ë¹¨ê°„ìƒ‰ê³¼ ê°™ì´)ëŠ” ê·¼ì²˜ì— ìœ„ì¹˜í•˜ëŠ” ê²ƒì„ ì•Œ ìˆ˜ ìˆë‹¤. ì´ì™€ ê°™ì´, ë…¸ë“œ ê°„ì˜ ê´€ê³„ë¥¼ íŒŒì•…í•˜ì—¬, ì„œë¡œ ê·¼ì ‘í•œ ë…¸ë“œë¥¼ ì°¾ì„ ìˆ˜ ìˆë‹¤ë©´, ê±°ê¸°ì— ë¹„ìŠ·í•œ ë ˆì´ë¸”ì„ ë§¤ê¸¸ ìˆ˜ ìˆì„ ê²ƒì´ë‹¤. ì—¬ê¸°ì„œ ì´ì œ ì–´ë–»ê²Œ ë…¸ë“œ ê°„ì˜ ìƒê´€ê´€ê³„ë¥¼ ì´ëŒì–´ ë‚¼ ìˆ˜ ìˆì„ì§€ ì‚´í´ë³´ì. ë…¸ë“œ ê°„ì˜ ìƒê´€ê´€ê³„ëŠ” Homophily, Influence ë‘ ê°œë…ì— ì˜í•´ ì •ì˜ë˜ì–´ì§„ë‹¤. ë‘ ê°œë… ëª¨ë‘ ì‚¬íšŒê³¼í•™ ë¶„ì•¼ì—ì„œ social networkë¥¼ ë¶„ì„í•˜ë©´ì„œ ì“°ì¸ ê°œë…ìœ¼ë¡œ ë³´ì´ëŠ”ë°, í•˜ë‚˜ì”© ìì„¸íˆ ì‚´í´ë³´ë„ë¡ í•˜ì. Social Homophily . Homophily: The tendency of individuals to associate and bond with similar others | â€œBirds of a feather flock togetherâ€ | It has been observed in a vast array of network studies, based on a variety of attributes (e.g., age, gender, organizational role, etc.) | Example: Researchers who focus on the same research area are more likely to establish a connection (meeting at conferences, interacting in academic talks, etc.) | . . ë™ì„±ì• : ê°œì¸ì´ ìœ ì‚¬í•œ íƒ€ì¸ê³¼ ì—°ê´€ë˜ê³  ìœ ëŒ€ê°ì„ ê°–ëŠ” ê²½í–¥ | â€œê¹ƒí„¸ ê°™ì€ ìƒˆë“¤â€ | ë‹¤ì–‘í•œ ì†ì„±(ì˜ˆ: ì—°ë ¹, ì„±ë³„, ì¡°ì§ ì—­í•  ë“±)ì„ ê¸°ë°˜ìœ¼ë¡œ í•œ ê´‘ë²”ìœ„í•œ ë„¤íŠ¸ì›Œí¬ ì—°êµ¬ì—ì„œ ê´€ì°°ë˜ì—ˆë‹¤. | ì˜ˆì‹œ: ë™ì¼í•œ ì—°êµ¬ ë¶„ì•¼ì— ì§‘ì¤‘í•˜ëŠ” ì—°êµ¬ìëŠ” (í•™íšŒì—ì„œì˜ íšŒì˜, í•™ìˆ  ê°•ì—°ì—ì„œì˜ ìƒí˜¸ì‘ìš© ë“±) ì—°ê²°ì„ í™•ë¦½í•  ê°€ëŠ¥ì„±ì´ ë†’ë‹¤. | . . Homophily: Example . Example of homophily . Online social network Nodes = people | Edges = friendship | Node color = interests (sports, arts, etc.) | . | People with the same interest are more closely connected due to homophily | . . ë™ìŒì´ì˜ ì˜ˆ . ì˜¨ë¼ì¸ ì†Œì…œ ë„¤íŠ¸ì›Œí¬ ë…¸ë“œ = ì‚¬ëŒ | ê°€ì¥ìë¦¬ = ìš°ì • | ë…¸ë“œ ìƒ‰ìƒ = ê´€ì‹¬ ì‚¬í•­(ìŠ¤í¬ì¸ , ì˜ˆìˆ  ë“±) | . | ë™ì¢…ì• ë¡œ ì¸í•´ ê°™ì€ ê´€ì‹¬ì‚¬ë¥¼ ê°€ì§„ ì‚¬ëŒë“¤ì´ ë” ë°€ì ‘í•˜ê²Œ ì—°ê²°ë˜ì–´ ìˆë‹¤. | . . . ğŸ’¬ ê°œì¸ì˜ íŠ¹ì§•ì€ ë‚˜ì´, ì„±ë³„, ì§ì—…, ì·¨ë¯¸, ê±°ì£¼ì§€ ë“± ë‹¤ì–‘í•œ ìš”ì†Œê°€ ìˆì„ ê²ƒì´ë‹¤. HomophilyëŠ” ê°œì¸ë“¤ì´ ë¹„ìŠ·í•œ íŠ¹ì§•ì„ ê°€ì§€ëŠ” íƒ€ì¸ë“¤ê³¼ ì„œë¡œ ì—°ê²°ë˜ê³ , í•¨ê»˜ í–‰ë™í•˜ë ¤ê³  í•œë‹¤ëŠ” ê°œë…ì´ë‹¤. ì˜ˆë¥¼ ë“¤ì–´, ë¨¸ì‹ ëŸ¬ë‹ ì—°êµ¬ìë“¤ì€ ë¹„ìŠ·í•œ í•™íšŒë¥¼ ë™ì‹œì— ì°¸ì—¬í•˜ê³ , ë¹„ìŠ·í•œ ì»¤ë®¤ë‹ˆí‹°ë¥¼ ì½ê³  í† ë¡ í•˜ê²Œ ë˜ë©´ì„œ ìì—°ìŠ¤ë ˆ ì¹œë¶„ì„ ìŒ“ê²Œ ëœë‹¤. ë˜í•œ, ê°€ìˆ˜ë“¤ì€ ì„œë¡œ ê°™ì´ ê³µì—°í•˜ê³ , ì„œë¡œì˜ ì•¨ë²”ì„ ë“¤ìœ¼ë©´ì„œ ì„œë¡œ ì‚¬íšŒì ìœ¼ë¡œ ì—°ê²°ë˜ê²Œ ëœë‹¤. ìœ„ì˜ ê·¸ë˜í”„ëŠ” í•œ í•™êµì˜ í•™ìƒë“¤ì„ ë‚˜íƒ€ë‚¸ ê·¸ë˜í”„ì¸ë°, í•™ìƒ ê°œì¸ì´ ë…¸ë“œ, ì¹œë¶„ì´ ì—£ì§€ë¡œ í‘œí˜„ë˜ì–´ ìˆë‹¤. ì´ë•Œ ë…¸ë“œì˜ ë ˆì´ë¸”ì¸ ìƒ‰ì€ ê° í•™ìƒì˜ ê´€ì‹¬ì‚¬ë¡œ ìš´ë™, ì˜ˆìˆ  ë“±ì´ ìˆë‹¤. ì§ê´€ì ìœ¼ë¡œ ì‚´í´ë³´ì•„ë„ ì•Œ ìˆ˜ ìˆì§€ë§Œ ì´ 4ê°œì˜ ì‘ì€ ê·¸ë£¹ìœ¼ë¡œ ë‚˜ëˆ„ì–´ì§ˆ ìˆ˜ ìˆìœ¼ë©°, ê° ê·¸ë£¹ì€ ë¹„ìŠ·í•œ ê´€ì‹¬ì‚¬ë¥¼ ê°€ì§€ëŠ” í•™ìƒë“¤ì´ ëª¨ì—¬ìˆëŠ” ê²ƒì„ ì•Œ ìˆ˜ ìˆë‹¤. ì´ë¥¼ Homophilyë¼ê³  í•  ìˆ˜ ìˆì„ ê²ƒì´ë‹¤. Social Influence: Example . Influence: Social connections can influence the individual characteristics of a person. Example: I recommend my musical preferences to my friends, until one of them grows to like my same favorite genres! | . | ì˜í–¥: ì‚¬íšŒì  ê´€ê³„ëŠ” ê°œì¸ì˜ íŠ¹ì„±ì— ì˜í–¥ì„ ë¯¸ì¹  ìˆ˜ ìˆë‹¤. ì˜ˆì‹œ: ì¹œêµ¬ ì¤‘ í•œ ëª…ì´ ì œê°€ ì¢‹ì•„í•˜ëŠ” ì¥ë¥´ë¥¼ ì¢‹ì•„í•˜ê²Œ ë  ë•Œê¹Œì§€ ì¹œêµ¬ë“¤ì—ê²Œ ì œ ìŒì•…ì  ì·¨í–¥ì„ ì¶”ì²œí•©ë‹ˆë‹¤! | . | . . ğŸ’¬ InfluenceëŠ” ì‚¬íšŒì ìœ¼ë¡œ ì—°ê²°ëœ ê°œì¸ ê°„ì—ëŠ” ì„œë¡œ ì˜í–¥ì„ ì£¼ê³  ë°›ìœ¼ë©´ì„œ ë¹„ìŠ·í•œ íŠ¹ì§•ì„ ê°€ì§€ê²Œ ëœë‹¤ëŠ” ê²ƒì„ ì˜ë¯¸í•œë‹¤. ì˜ˆë¥¼ ë“¤ì–´, ë‚´ê°€ 1, 2í•™ë…„ ë•ŒëŠ” ê²½ìƒê³„ì—´ ì¹œêµ¬ë“¤ê³¼ ì¹œí•˜ê²Œ ì§€ë‚´ë©´ì„œ, ê²½ì œ, ì‚¬íšŒ, ì œë„ ë“±ì— ê´€ì‹¬ì„ ê°€ì§€ê³  í•´ë‹¹ ì§êµ°ì„ í¬ë§í–ˆë‹¤ë©´, 3, 4í•™ë…„ì´ ë˜ë©´ì„œ í†µê³„ë‚˜ ìˆ˜í•™, ì»´í“¨í„° ê³µí•™ ì¹œêµ¬ë“¤ê³¼ ì¹œí•˜ê²Œ ì§€ë‚´ë©´ì„œ ML, DL, ì½”ë”© ë“±ì— ê´€ì‹¬ì„ ê°€ì§€ê³  í•´ë‹¹ ì§êµ°ì„ í¬ë§í•˜ê²Œ ëœ ê²ƒì´ Influence ë•Œë¬¸ì´ë¼ê³  í•  ìˆ˜ ìˆì„ ê²ƒì´ë‹¤. How do we leverage node correlations in networks? . Classification with Network Data . How do we leverage this correlation observed in networks to help predict node labels? . | ë„¤íŠ¸ì›Œí¬ì—ì„œ ê´€ì°°ëœ ì´ ìƒê´€ ê´€ê³„ë¥¼ í™œìš©í•˜ì—¬ ë…¸ë“œ ë ˆì´ë¸”ì„ ì˜ˆì¸¡í•˜ëŠ” ë°©ë²•ì€ ë¬´ì—‡ì…ë‹ˆê¹Œ? . | . . How do we predict the labels for the nodes in grey? . ë…¸ë“œì˜ ë ˆì´ë¸”ì„ íšŒìƒ‰ìœ¼ë¡œ ì–´ë–»ê²Œ ì˜ˆì¸¡í•©ë‹ˆê¹Œ? . Motivation . Similar nodes are typically close together or directly connected in the network: Guilt-by-association: If I am connected to a node with label ğ‘‹, then I am likely to have label ğ‘‹ as well. | Example: Malicious/benign web page: Malicious web pages link to one another to increase visibility, look credible, and rank higher in search engines | . | Classification label of a node ğ‘£ in network may depend on: Features of ğ‘£ | Labels of the nodes in ğ‘£â€™s neighborhood | Features of the nodes in ğ‘£â€™s neighborhood | . | . . ìœ ì‚¬í•œ ë…¸ë“œëŠ” ì¼ë°˜ì ìœ¼ë¡œ ë„¤íŠ¸ì›Œí¬ì—ì„œ ì„œë¡œ ê°€ê¹Œì´ ìˆê±°ë‚˜ ì§ì ‘ ì—°ê²°ë©ë‹ˆë‹¤. ì—°ê´€ë³„ ì£„ì±…ê°: ë ˆì´ë¸” ğ‘‹ì´ ìˆëŠ” ë…¸ë“œì— ì—°ê²°ë˜ì–´ ìˆë‹¤ë©´ ë ˆì´ë¸” ğ‘‹ë„ ìˆì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤. | ì˜ˆ: ì•…ì„±/ì•…ì„± ì›¹ í˜ì´ì§€: ì•…ì„± ì›¹ í˜ì´ì§€ëŠ” ê°€ì‹œì„±ì„ ë†’ì´ê³  ì‹ ë¢°ë„ë¥¼ ë†’ì´ë©° ê²€ìƒ‰ ì—”ì§„ì—ì„œ ë” ë†’ì€ ìˆœìœ„ë¥¼ ì°¨ì§€í•˜ê¸° ìœ„í•´ ì„œë¡œ ì—°ê²°ë©ë‹ˆë‹¤. | . | ë„¤íŠ¸ì›Œí¬ì— ìˆëŠ” ë…¸ë“œ $v$ì˜ ë¶„ë¥˜ ë¼ë²¨ì€ ë‹¤ìŒ ì¡°ê±´ì— ë”°ë¼ ë‹¬ë¼ì§ˆ ìˆ˜ ìˆìŠµë‹ˆë‹¤. $v$ì˜ ê¸°ëŠ¥ | $v$ì˜ ì´ì›ƒì— ìˆëŠ” ë…¸ë“œì˜ ë ˆì´ë¸” | $v$ì˜ ì´ì›ƒì— ìˆëŠ” ë…¸ë“œì˜ ê¸°ëŠ¥ | . | . ğŸ’¬ í•œ ê·¸ë˜í”„ ë‚´ì—ì„œ ë¹„ìŠ·í•œ ë…¸ë“œëŠ” ê°€ê¹Œì´ ìœ„ì¹˜í•˜ê±°ë‚˜ ì§ì ‘ ì—°ê²°ë˜ì–´ ìˆì„ ê²ƒì´ë‹¤. ì´ë¥¼ Guilt-by-associationì´ë¼ê³  í•˜ëŠ”ë°, ë…¸ë“œ bê°€ ì•„ì§ ë ˆì´ë¸”ì´ ì—†ëŠ” ìƒíƒœì—ì„œ, ì´ì›ƒë…¸ë“œ xê°€ 1ë¡œ ë ˆì´ë¸” ë˜ì–´ ìˆë‹¤ë©´, ì´ì›ƒë…¸ë“œ xì™€ ê°€ê¹ê¸° ë•Œë¬¸ì— ë…¸ë“œ b ì—­ì‹œ 1ë¡œ ë ˆì´ë¸” ë  ê°€ëŠ¥ì„±ì´ ë†’ë‹¤ëŠ” ê°œë…ì´ë‹¤. êµ¬ì²´ì ì¸ ì˜ˆì‹œë¡œëŠ” ìŠ¤íŒ¸ ì‚¬ì´íŠ¸ë“¤ì´ ì•ˆì „í•œ ì‚¬ì´íŠ¸ì™€ì˜ ì—°ê²°ê³ ë¦¬ëŠ” ìƒì„±í•  ìˆ˜ ì—†ê¸° ë•Œë¬¸ì—, ë…¸ì¶œë„ì™€ ì‹ ë¢°ë„ë¥¼ ë†’ì´ê¸° ìœ„í•´ ì„œë¡œ ë§í¬ë¥¼ ì—°ê²°í•˜ëŠ” ê²½í–¥ì´ ìˆëŠ”ë°, ì´ë¥¼ ì´ìš©í•´ì„œ ìŠ¤íŒ¸ ì‚¬ì´íŠ¸ í•˜ë‚˜ë¥¼ ì¡ì„ ìˆ˜ ìˆë‹¤ë©´, ì„œë¡œ ì—°ê²°ëœ ë‹¤ë¥¸ ìŠ¤íŒ¸ ì‚¬ì´íŠ¸ë„ ìƒ‰ì¶œ í•  ìˆ˜ ìˆë‹¤ê³  í•œë‹¤. ì´ë•Œ, ë…¸ë“œ vì˜ ë¶„ë¥˜ì— ì´ìš©í•˜ëŠ” ì •ë³´ë“¤ì€ ë‹¤ìŒê³¼ ê°™ë‹¤. 1. ë…¸ë“œ vì˜ ë³€ìˆ˜ë“¤ 2. ë…¸ë“œ vì˜ ì´ì›ƒ ë…¸ë“œë“¤ì˜ ë ˆì´ë¸” 3. ë…¸ë“œ vì˜ ì´ì›ƒ ë…¸ë“œë“¤ì˜ ë³€ìˆ˜ë“¤ Semi-supervised Learning . Formal setting: . Given: . Graph | Few labeled nodes | . Find: Class (red/green) of remaining nodes . Main assumption: There is homophily in the network . . ê³µì‹ ì„¤ì •: . ì œê³µë¨: . ê·¸ë˜í”„ | ë ˆì´ë¸”ì´ ì§€ì •ëœ ë…¸ë“œ ëª‡ ê°œ | . ì°¾ê¸°: ë‚˜ë¨¸ì§€ ë…¸ë“œì˜ í´ë˜ìŠ¤(ë¹¨ê°„ìƒ‰/ë…¹ìƒ‰) . ì£¼ìš” ê°€ì •: ë„¤íŠ¸ì›Œí¬ì— ë™ì§ˆì„±ì´ ìˆìŠµë‹ˆë‹¤. . . Example task: . Let ğ‘¨ be a ğ‘›Ã—ğ‘› adjacency matrix over ğ‘› nodes | Let Y = $[0,1]^n$ be a vector of labels: $Y_v$ = 1 belongs to Class1 | $Y_v$ = 0 belongs to Class0 | There are unlabeled node needs to be classified | . | Goal: Predict which unlabeled nodes are likely Class 1, and which are likely Class 0 | . . ì‘ì—… ì˜ˆ: . Aë¥¼ nê°œ ë…¸ë“œì˜ nxn ì¸ì ‘ í–‰ë ¬ë¡œ ì„¤ì • | Y = $[0,1]^n$ ì„ ë ˆì´ë¸” ë²¡í„°ë¼ê³  í•˜ì: $Y_v$ = 1ì´ í´ë˜ìŠ¤ 1ì— ì†í•¨ | $Y_v$ = 0ì´ í´ë˜ìŠ¤ 0ì— ì†í•¨ | ë¼ë²¨ì´ ì§€ì •ë˜ì§€ ì•Šì€ ë…¸ë“œê°€ ë¶„ë¥˜ë˜ì–´ì•¼ í•©ë‹ˆë‹¤ | . | ëª©í‘œ: ë ˆì´ë¸”ì´ ì—†ëŠ” ë…¸ë“œê°€ í´ë˜ìŠ¤ 1ì¼ ê°€ëŠ¥ì„±ì´ ë†’ê³  í´ë˜ìŠ¤ 0ì¼ ê°€ëŠ¥ì„±ì´ ë†’ì€ ë…¸ë“œë¥¼ ì˜ˆì¸¡í•©ë‹ˆë‹¤. | . . ğŸ’¬ ì‹¤ì œë¡œ ì´ìš©í•˜ê²Œ ë˜ëŠ” ì…ë ¥ê°’ì€ ì¸ì ‘í–‰ë ¬: $A_{n*n}$, ë ˆì´ë¸” ë²¡í„° $Y=[0,1]^n$, $Y_v$ **= 1ì´ í´ë˜ìŠ¤ 1**ì— ì†í•¨, $Y_v$ **= 0ì´ í´ë˜ìŠ¤ 0**ì— ì†í•¨ì´ ìˆìœ¼ë©°, ì•„ì§ ë ˆì´ë¸”ì´ ì—†ëŠ” ë…¸ë“œì— ëŒ€í•´ ë ˆì´ë¸”ì´ 0 í˜¹ì€ 1ì¼ í™•ë¥ ì„ ê³„ì‚°í•˜ëŠ” ê²ƒì´ ëª©í‘œì´ë‹¤. Problem Setting . How to predict the labels $Y_v$ for the unlabeled nodes $v$ (in grey color)? Each node $v$ has a feature vector $f_v$ Labels for some nodes are given (1 for green, 0 for red) Task: Find $P(Y_v)$ given all features and the network ë¼ë²¨ì´ ì—†ëŠ” ë…¸ë“œ $v$(íšŒìƒ‰)ì— ëŒ€í•œ ë ˆì´ë¸” $Y_v$ë¥¼ ì˜ˆì¸¡í•˜ëŠ” ë°©ë²• ê° ë…¸ë“œ $v$ì—ëŠ” íŠ¹ì§• ë²¡í„° $f_v$ê°€ ìˆìŠµë‹ˆë‹¤. ì¼ë¶€ ë…¸ë“œì˜ ë¼ë²¨ì´ ì œê³µë©ë‹ˆë‹¤(ë…¹ìƒ‰ì€ 1, ë¹¨ê°„ìƒ‰ì€ 0). ì‘ì—…: ëª¨ë“  ê¸°ëŠ¥ ë° ë„¤íŠ¸ì›Œí¬ê°€ ì£¼ì–´ì§„ $P(Y_v)$ ì°¾ê¸° . P(Yv)=?P(Y_v)=?P(Yvâ€‹)=? . . Example applications: . Many applications under this setting: Document classification | Part of speech tagging | Link prediction | Optical character recognition | Image/3D data segmentation | Entity resolution in sensor networks | Spam and fraud detection | . | . . ì´ ì„¤ì • ì•„ë˜ì˜ ë§ì€ ì‘ìš© í”„ë¡œê·¸ë¨: ë¬¸ì„œêµ¬ë¶„ | ìŒì„± íƒœê·¸ì˜ ì¼ë¶€ | ë§í¬ ì˜ˆì¸¡ | ê´‘í•™ì‹ ë¬¸ì ì¸ì‹ | ì˜ìƒ/3D ë°ì´í„° ë¶„í•  | ì„¼ì„œ ë„¤íŠ¸ì›Œí¬ì˜ ì—”í‹°í‹° í•´ìƒë„ | ìŠ¤íŒ¸ ë° ë¶€ì • í–‰ìœ„ íƒì§€ | . | . . ğŸ’¬ ì´ëŸ¬í•œ ë°©ë²•ë¡ ì€ ìœ„ì™€ ê°™ì€ ë‹¤ì–‘í•œ ë¶„ì•¼ì— ì ìš©ì´ ê°€ëŠ¥í•˜ë‹¤. Collective Classification Overview (1) . collective classificationì„ ì „ë°˜ì ìœ¼ë¡œ ì‚´í´ë³´ë©´ ë‹¤ìŒê³¼ ê°™ë‹¤. . ğŸ“Œ Intuition(ì§ê´€) : ë…¸ë“œ ê°„ ìƒê´€ê´€ê³„ë¥¼ ì´ìš©í•´ ì„œë¡œ ì—°ê²°ëœ ë…¸ë“œë“¤ì„ ë™ì‹œì— ë¶„ë¥˜í•˜ê¸° ì´ë•Œ 1ì°¨ ë§ˆë¥´ì½”í”„ ì—°ì‡„ë¥¼ ì‚¬ìš©í•œë‹¤. ì¦‰, ë…¸ë“œ $ mathrm{v}$ ì˜ ë ˆì´ë¸” $Y_{v}$ ë¥¼ ì˜ˆì¸¡í•˜ê¸° ìœ„í•´ì„œëŠ” ì´ì›ƒë…¸ë“œ $N_{v}$ ë§Œ í•„ìš”í•˜ë‹¤ëŠ” ê²ƒì´ë‹¤. 2ì°¨ ë§ˆë¥´ì½”í”„ ì—°ì‡„ë¥¼ ì‚¬ìš©í•  ê²½ìš° $N_{v}$ ì˜ ì´ì›ƒë…¸ë“œ ì—­ì‹œ ì‚¬ìš©í•  ê²ƒì´ë‹¤. 1ì°¨ ë§ˆë¥´í¬í¬ ì—°ì‡„ë¥¼ ì‚¬ìš©í•  ê²½ìš° ì‹ì€ ë‹¤ìŒê³¼ ê°™ì•„ì§ˆ ê²ƒì´ë‹¤. . P(Yv)=P(Yvâˆ£Nv)P left(Y_{v} right)=P left(Y_{v} mid N_{v} right)P(Yvâ€‹)=P(Yvâ€‹âˆ£Nvâ€‹) . collective classificationì€ í•˜ë‚˜ì˜ ëª¨ë¸ì„ ì´ìš©í•˜ê±°ë‚˜ ê¸°ì¡´ì˜ ë¶„ë¥˜ëª¨ë¸ì²˜ëŸ¼ í•œë²ˆì˜ ê³¼ì •ìœ¼ë¡œ êµ¬ì„±ë˜ì§€ ì•Šê³  ì´ ì„¸ê°€ì§€ ê³¼ì •ìœ¼ë¡œ êµ¬ì„±ëœë‹¤. . Collective Classification Overview (2) . Local Classifier . ìµœì´ˆë¡œ ë ˆì´ë¸”ì„ í• ë‹¹í•˜ê¸° ìœ„í•´ ì‚¬ìš©ë˜ëŠ” ë¶„ë¥˜ê¸°ì´ë‹¤. ì¦‰, ê·¸ë˜í”„ì—ì„œ ë ˆì´ë¸”ì´ ì—†ëŠ” ë…¸ë“œë“¤ì— ëŒ€í•´ ìš°ì„  ë…¸ë“œë¥¼ ìƒì„±í•´ì•¼ í•˜ê¸° ë•Œë¬¸ì—, ê¸°ì¡´ì˜ ë¶„ë¥˜ë¬¸ì œì™€ ë™ì¼í•˜ê²Œ êµ¬ì„±ëœë‹¤. ì´ë•Œ ì˜ˆì¸¡ ê³¼ì •ì€ ê° ë…¸ë“œì˜ ë³€ìˆ˜ë§Œ ì‚¬ìš©í•˜ì—¬ ì´ë¯¸ ë ˆì´ë¸”ì´ ìˆëŠ” ë…¸ë“œë¡œ í•™ìŠµí•˜ê³ , ë ˆì´ë¸”ì´ ì—†ëŠ” ë…¸ë“œë¡œ ì˜ˆì¸¡í•˜ê²Œ ëœë‹¤. ê·¸ë˜í”„ì˜ êµ¬ì¡°ì  ì •ë³´ê°€ ì‚¬ìš©ë˜ì§€ ì•ŠëŠ”ë‹¤ëŠ” ì ì— ìœ ì˜í•˜ì. . Relational Classifier . ë…¸ë“œ ê°„ ìƒê´€ê´€ê³„ë¥¼ íŒŒì•…í•˜ê¸° ìœ„í•´ ì´ì›ƒ ë…¸ë“œì˜ ë ˆì´ë¸”ê³¼ ë³€ìˆ˜ë¥¼ ì‚¬ìš©í•˜ëŠ” ë¶„ë¥˜ê¸°ì´ë‹¤. ì´ë¥¼ í†µí•´ ì´ì›ƒ ë…¸ë“œì˜ ë ˆì´ë¸”ê³¼ ë³€ìˆ˜ì™€ í˜„ì¬ ë…¸ë“œì˜ ë³€ìˆ˜ë¥¼ ì´ìš©í•´ í˜„ì¬ ë…¸ë“œì˜ ë ˆì´ë¸”ì„ ì˜ˆì¸¡í•  ìˆ˜ ìˆë‹¤. ì´ë•Œ, ì´ì›ƒë…¸ë“œì˜ ì •ë³´ê°€ ì‚¬ìš©ë˜ê¸° ë•Œë¬¸ì—, ê·¸ë˜í”„ì˜ êµ¬ì¡°ì  ì •ë³´ê°€ ì‚¬ìš©ëœë‹¤. . Collective Inference . Collective Classificationì€ í•œë²ˆì˜ ì˜ˆì¸¡ìœ¼ë¡œ ì¢…ë£Œë˜ì§€ ì•ŠëŠ” ê²ƒì´ í•µì‹¬ì´ë‹¤. íŠ¹ì • ì¡°ê±´ì„ ë§Œì¡±í•  ë•Œê¹Œì§€ ê° ë…¸ë“œì— ëŒ€í•´ ë¶„ë¥˜í•˜ê³  ë ˆì´ë¸”ì„ ì—…ë°ì´íŠ¸í•œë‹¤. ì´ë•Œì˜ ì¡°ê±´ì´ë€ ë”ì´ìƒ ë ˆì´ë¸”ì´ ë³€í•˜ì§€ ì•Šê±°ë‚˜, ì •í•´ì§„ íšŸìˆ˜ë¥¼ ì˜ë¯¸í•œë‹¤. ì´ë•Œ ë™ì¼í•œ ë³€ìˆ˜ë¥¼ ê°€ì§„ ë…¸ë“œë¼ í•˜ë”ë¼ë„ ê·¸ë˜í”„ì˜ êµ¬ì¡°ì— ë”°ë¼ ìµœì¢… ì˜ˆì¸¡ì´ ë‹¬ë¼ì§ˆ ìˆ˜ ìˆë‹¤ëŠ” ì ì„ ìœ ë…í•˜ì. . Overview of What is Coming . We focus on semi-supervised binary node classification | We will introduce three approaches: Relational classification | Iterative classification | Correct &amp; Smooth | . | . . ìš°ë¦¬ëŠ” ì¤€ì§€ë„ ì´ì§„ ë…¸ë“œ ë¶„ë¥˜ì— ì´ˆì ì„ ë§ì¶˜ë‹¤. | ë‹¤ìŒ ì„¸ ê°€ì§€ ì ‘ê·¼ ë°©ì‹ì„ ì†Œê°œí•©ë‹ˆë‹¤. ê´€ê³„êµ¬ë¶„ | ë°˜ë³µêµ¬ë¶„ | ì •í™•í•˜ê³  ë§¤ë„ëŸ¬ìš´ | . | . . CS224W: Machine Learning with Graphs 2021 Lecture 5.1 - Message passing and Node Classification .",
            "url": "https://cs224w-kor.github.io/blog/graph%20neural%20network/gnn/graph%20convolution%20network/gcn/2022/07/20/lecture-0501.html",
            "relUrl": "/graph%20neural%20network/gnn/graph%20convolution%20network/gcn/2022/07/20/lecture-0501.html",
            "date": " â€¢ Jul 20, 2022"
        }
        
    
  
    
        ,"post13": {
            "title": "Lecture 4.4 - Matrix Factorization and Node Embeddings",
            "content": ". Lecture 4. Graph as Matrix . Lecture 4.1 - PageRank | Lecture 4.2 - PageRank, How to Solve? | Lecture 4.3 - Random Walk with Restarts | Lecture 4.4 - Matrix Factorization and Node Embeddings | . . Recall: Node Embeddings &amp; Embedding matrix . ì´ì „ ê°•ì˜ì—ì„œ ë°°ì› ë˜ embedding matrix $ mathbf{Z}$ì— ëŒ€í•´ ë‹¤ì‹œ ë– ì˜¬ë ¤ë´…ì‹œë‹¤. ì´ ë§¤íŠ¸ë¦­ìŠ¤ëŠ” ê·¸ë˜í”„ì˜ ê° ë…¸ë“œë“¤ì„ ì ì¬ë³€ìˆ˜ ê³µê°„(embedding space)ìœ¼ë¡œ encodingí•˜ëŠ” í–‰ë ¬ë¡œ ì—´ì˜ ì°¨ì›ì€ embeddingí•˜ëŠ” í¬ê¸°, í–‰ì˜ ì°¨ì›ì€ ê·¸ë˜í”„ì— ìˆëŠ” ë…¸ë“œì˜ ìˆ˜ê°€ ë©ë‹ˆë‹¤. ì´ ë§¤íŠ¸ë¦­ìŠ¤ì˜ í•œ ì—´ì€ íŠ¹ì • ë…¸ë“œ $u$ì˜ embedding vector $ mathbf{z}_u$ë¥¼ ë‚˜íƒ€ë‚´ê²Œ ë©ë‹ˆë‹¤. . . . ì´ëŸ¬í•œ Node embeddingì—ì„œ objectiveëŠ” ê·¸ë˜í”„ìƒì—ì„œ ì‹¤ì œë¡œ ìœ ì‚¬í•œ ë…¸ë“œë“¤ì˜ simliarityê°€ embedding vectorë“¤ì˜ ë‚´ì (inner product)ê°’ë„ ë†’ë„ë¡ ë§Œë“œëŠ” ê²ƒì…ë‹ˆë‹¤. . ğŸ“Œ Objective: Maximize $ mathbf{z}_{v}^{ mathrm{T}} mathbf{z}_{u}$ for node pairs $(u, v)$ that are similar Matrix Factorization . Embedding matrixë¥¼ Matriz Factorization ê´€ì ì—ì„œ ë‹¤ì‹œ ìƒê°í•´ë´…ì‹œë‹¤. ê·¸ë˜í”„ë¥¼ ë…¸ë“œë“¤ê°„ì˜ ì—°ê²°ì´ ë˜ì–´ ìˆìœ¼ë©´ 1, ì•„ë‹ˆë©´ 0ìœ¼ë¡œ ë‚˜íƒ€ë‚¸ ì¸ì ‘í–‰ë ¬ $ mathbf{A}$ì„ embedding matrix $ mathbf{Z}$ë¡œ factorization í•œë‹¤ê³  ìƒê°í•´ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì¦‰ $ mathbf{Z}^{ mathrm{T}}$ì™€ $ mathbf{Z}$ì˜ ë‚´ì ìœ¼ë¡œ ì¸ì ‘í–‰ë ¬ $ mathbf{A}$ë¥¼ ë§Œë“œëŠ” ê²ƒì…ë‹ˆë‹¤. . ZTZ=A mathbf{Z}^{ mathrm{T}} mathbf{Z} = mathbf{A}ZTZ=A . . í•˜ì§€ë§Œ embedding matrix $ mathbf{Z}$ì˜ í–‰ì˜ ìˆ˜, ì¦‰ embedding dimension $d$ëŠ” ë…¸ë“œì˜ ìˆ˜ $n$ë³´ë‹¤ ì‘ìœ¼ë¯€ë¡œ ì™„ë²½í•œ factorizationì„ í•  ìˆ˜ ì—†ê³  ëŒ€ì‹ ì— ì´ë¥¼ ìµœì í™” ê¸°ë²•ì„ ì‚¬ìš©í•˜ì—¬ ê·¼ì ‘(approzimate)ì‹œí‚¬ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì´ ìµœì í™”ë¥¼ ëª©ì í•¨ìˆ˜ëŠ” ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤. . minâ¡Zâˆ¥Aâˆ’ZTZâˆ¥2 min _{ mathbf{Z}} left |A- boldsymbol{Z}^{T} boldsymbol{Z} right |_{2}Zminâ€‹âˆ¥ . âˆ¥â€‹Aâˆ’ZTZâˆ¥ . âˆ¥â€‹2â€‹ . ê²°ë¡ ì€ edge connectivityë¡œ ì •ì˜ëœ node similarityì„ ë‚˜íƒ€ë‚´ëŠ” decoder($ mathbf{Z}$)ì˜ ë‚´ì ì€ $ mathbf{A}$ì˜ matrix factorizationê³¼ ë™ì¼í•˜ë‹¤ëŠ” ê²ƒì…ë‹ˆë‹¤. . RandomWalk-based Similarity . DeepWalkì™€ node2vec ì•Œê³ ë¦¬ì¦˜ì—ì„œëŠ” random walksë¥¼ ê¸°ë°˜ìœ¼ë¡œí•œ ì¢€ ë” ë³µì¡í•œ node similarityë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤. 2ê°œì˜ ì•Œê³ ë¦¬ì¦˜ ëª¨ë‘ì—ì„œ matrix factorizationì„ ì‚¬ìš©í•˜ê³  ìˆìŠµë‹ˆë‹¤. DeepWalkì—ì„œ ì‚¬ìš©í•˜ëŠ” node simliarityëŠ” ë‹¤ìŒê³¼ ê°™ì´ ì •ì˜ë©ë‹ˆë‹¤. (node2vecì€ ì´ë³´ë‹¤ ì¡°ê¸ˆ ë” ë³µì¡í•©ë‹ˆë‹¤. ìì„¸í•œ ë‚´ìš©ì„ í™•ì¸í•˜ê³  ì‹¶ìœ¼ë©´ Network Embedding as Matrix Factorization paperë¥¼ ì°¸ê³ ) . . Limitations . Matrix factorizationê³¼ random walkë¡œ node embeddingì„ í•  ê²½ìš° ëª‡ê°€ì§€ ì œì•½(ë‹¨ì )ì´ ìˆìŠµë‹ˆë‹¤. . ê·¸ë˜í”„ì— ìƒˆë¡œìš´ ë…¸ë“œê°€ ìƒê²¼ì„ ë•Œ ëŒ€ì‘í•˜ì§€ ëª»í•©ë‹ˆë‹¤. trainingê³¼ì •ì—ì„œ ë³´ì§€ ëª»í•œ ë…¸ë“œê°€ ìƒê²¼ì„ ë•Œ scratchë¶€í„° ë‹¤ì‹œ ê³„ì‚°í•´ì•¼ í•©ë‹ˆë‹¤. | . êµ¬ì¡°ì ì¸ ìœ ì‚¬ì„±ì„ íŒŒì•…í•˜ì§€ ëª»í•©ë‹ˆë‹¤. ì•„ë˜ì˜ ê·¸ë¦¼ì—ì„œ 1-2-3ê³¼ 11-12-13ì€ ê·¸ë˜í”„ì—ì„œ ë¹„ìŠ·í•œ êµ¬ì¡°ë¥¼ ê°€ì§€ê³  ìˆì§€ë§Œ ê° ë…¸ë“œë§ˆë‹¤ uniqueí•œ embedding ê°’ìœ¼ë¡œ ì¸í•´ êµ¬ì¡°ì ì¸ ìœ ì‚¬ì„±ì„ íŒŒì•…í•˜ì§€ ëª»í•©ë‹ˆë‹¤. | . ë…¸ë“œ, ì—£ì§€, ê·¸ë˜í”„ì˜ feature ì •ë³´ë¥¼ í™œìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. DeepWalkë‚˜ node2vecì— ì“°ì¸ embeddingì€ ë…¸ë“œì— ìˆëŠ” feature ì •ë³´ë¥¼ í™œìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì´ëŠ” ì¶”í›„ì— ë°°ìš¸ Deep Representation Learningìœ¼ë¡œ í•´ê²°í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. | . Algorithms ì •ë¦¬ . PageRank: ê·¸ë˜í”„ì—ì„œ ë…¸ë“œì˜ importanceë¥¼ ì¸¡ì •í•˜ëŠ” ì•Œê³ ë¦¬ì¦˜ì´ë©° ì¸ì ‘í–‰ë ¬ì˜ power iterationìœ¼ë¡œ ê³„ì‚°í•  ìˆ˜ ìˆë‹¤. ì´ 3ê°€ì§€ ê´€ì ì—ì„œ í•´ì„í•  ìˆ˜ ìˆë‹¤. | (1) Flow formulation | (2) Random walk &amp; Stationary distribution | (3) Linear Algebra - eigenvector | . | Personalized PageRank(PPR): PageRankì—ì„œ ì¢€ ë” ë°œì „ì‹œí‚¨ ì•Œê³ ë¦¬ì¦˜ìœ¼ë¡œ random walkë¡œ êµ¬í•œ íŠ¹ì • ë…¸ë“œì˜ ì¤‘ìš”ì„±ì„ ë” ê³ ë ¤í•˜ì—¬ teleportë¥¼ í•˜ëŠ” ì•Œê³ ë¦¬ì¦˜ | Random walks ê¸°ë°˜ Node Embeddingsì€ Matrix factorizationìœ¼ë¡œ í‘œí˜„ë  ìˆ˜ ìˆë‹¤. | . ê·¸ë˜í”„ë¥¼ í–‰ë ¬ë¡œ ì´í•´í•˜ëŠ” ê²ƒì€ ìœ„ì˜ ì•Œê³ ë¦¬ì¦˜ë“¤ì„ ì´í•´í•˜ëŠ”ë° ë§¤ìš° ì¤‘ìš”í•˜ë‹¤ëŠ” ê²ƒì„ ì•Œ ìˆ˜ ìˆìŠµë‹ˆë‹¤. . . Original Lecture Video : CS224W: Machine Learning with Graphs 2021 Lecture 4.4 - Matrix Factorization and Node Embeddings .",
            "url": "https://cs224w-kor.github.io/blog/matrix/node%20embedding/factorization/random%20walks/2022/07/13/lecture-0404.html",
            "relUrl": "/matrix/node%20embedding/factorization/random%20walks/2022/07/13/lecture-0404.html",
            "date": " â€¢ Jul 13, 2022"
        }
        
    
  
    
        ,"post14": {
            "title": "Lecture 4.3 - Random Walk with Restarts",
            "content": ". Lecture 4. Graph as Matrix . Lecture 4.1 - PageRank | Lecture 4.2 - PageRank, How to Solve? | Lecture 4.3 - Random Walk with Restarts | Lecture 4.4 - Matrix Factorization and Node Embeddings | . . Recommendation . ì¶”ì²œ ì‹œìŠ¤í…œì—ì„œ ì´ë¶„ê·¸ë˜í”„(Bipartite graph)ë¡œ userì™€ itemì˜ (êµ¬ë§¤)ê´€ê³„ë¥¼ ë‚˜íƒ€ë‚¸ Bipartite User-Item GraphëŠ” ë‹¤ìŒ ê·¸ë¦¼ê³¼ ê°™ìŠµë‹ˆë‹¤. ì—¬ê¸°ì—ì„œ íŠ¹ì • item Që¥¼ êµ¬ë§¤í•œ userì—ê²Œ ì–´ë–¤ itemì„ ì¶”ì²œí•´ì£¼ëŠ” ê²ƒì´ ì¢‹ì„ì§€ë¥¼ ê³ ë¯¼í•œë‹¤ë©´, ì§ê´€ì ìœ¼ë¡œ item Qê°€ item Pì™€ ë¹„ìŠ·í•˜ê²Œ userë“¤ê³¼ ê´€ê³„ë¥¼ ê°€ì§€ê³  ìˆì„ ë•Œ item Pë¥¼ ì´ ìœ ì €ì—ê²Œ ì¶”ì²œí•˜ëŠ” ê²ƒì´ ì¢‹ì„ ê²ƒì´ë¼ê³  ìƒê°í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì¦‰, item Qì™€ item Pê°€ ì–¼ë§ˆë‚˜ ê°€ê¹Œìš´ ê´€ê³„ì¸ì§€ íŒë‹¨í•˜ëŠ” ê²ƒì´ ì¤‘ìš”í•©ë‹ˆë‹¤. . . Node proximity Measurements . ë…¸ë“œ ê·¼ì ‘ì„±(proximity) ì¸¡ì •ì— ëŒ€í•´ ìƒê°í•´ë³´ê¸° ìœ„í•´ ì•„ë˜ì˜ 3ê°€ì§€ ì¼€ì´ìŠ¤ë¥¼ ë³´ê² ìŠµë‹ˆë‹¤. A-Aâ€™ì€ B-Bâ€™ë³´ë‹¤ ë” ê°€ê¹Œìš´ ê´€ê³„ë¥¼ ê°€ì§€ê³  ìˆë‹¤ê³  í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì™œëƒí•˜ë©´ A-Aâ€™ pathì—ì„œ userì„ í•œë²ˆë§Œ ê±°ì¹˜ëŠ”ë° ë°˜í•´, B-Bâ€™pathì—ì„œëŠ” B-user-item-user-Bâ€™ ë¡œ pathì˜ ê¸¸ì´ê°€ ë” ê¸¸ê¸° ë•Œë¬¸ì…ë‹ˆë‹¤. A-Aâ€™ì™€ C-Câ€™ë¥¼ ë¹„êµí•´ë³´ë©´ C-Câ€™ì´ ë” ê°€ê¹Œìš´ ê´€ê³„ë¥¼ ê°€ì§€ê³  ìˆë‹¤ê³  íŒë‹¨í•  ìˆ˜ ìˆëŠ”ë° ê·¸ ì´ìœ ëŠ” C-Câ€™ì´ A-Aâ€™ë³´ë‹¤ ë” ë§ì€ ê³µí†µì˜ ì´ì›ƒ(Common Neighbors)ë¥¼ ê°€ì§€ê³  ìˆê¸° ë•Œë¬¸ì…ë‹ˆë‹¤. C-Câ€™ì€ A-Aâ€™ì˜ shortest pathê°€ 2ê°œìˆëŠ” ê²ƒìœ¼ë¡œë„ ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤. . . Proximity on Graphs . ì´ì „ì— PageRankë¥¼ ë‹¤ì‹œ ë– ì˜¬ë ¤ë³´ë©´, (1) rankëŠ” nodeì˜ â€œimportanceâ€ë¥¼ ì •ì˜í•˜ë©° (2) ê·¸ë˜í”„ì˜ ëª¨ë“  nodeë“¤ì— ê· ì¼ ë¶„í¬ë¡œ teleport ì´ë™ì„ í•  ìˆ˜ ìˆëŠ” ì•Œê³ ë¦¬ì¦˜ì´ì—ˆìŠµë‹ˆë‹¤. . ì—¬ê¸°ì— ì¢€ ë” ì•„ì´ë””ì–´ë¥¼ ë§ë¶™ì—¬ì„œ Personalized PageRank ì•Œê³ ë¦¬ì¦˜ì„ ìƒê°í•´ ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ê·¸ë˜í”„ì˜ ëª¨ë“  ë…¸ë“œë“¤ì— ëŒ€í•´ ê· ì¼ ë¶„í¬ë¡œ teleport ì´ë™ì„ í•˜ëŠ” ê²ƒì´ ì•„ë‹Œ, ê·¸ë˜í”„ ë…¸ë“œë“¤ì˜ ë¶€ë¶„ì§‘í•©(subset) $ mathbf{S}$ì˜ ë…¸ë“œë“¤ë¡œë§Œ teleport ì´ë™ì„ í•˜ë„ë¡ í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ëª¨ë“  ë…¸ë“œë“¤ë¡œ ëœë¤í•˜ê²Œ teleportí•˜ì§€ ì•Šê³  ì¢€ ë” ì—°ê´€ì„±ì´ ë†’ì€ ë…¸ë“œë“¤ë¡œ teleportí•  ìˆ˜ ìˆë„ë¡ í•˜ëŠ” ê²ƒì…ë‹ˆë‹¤. item Qì™€ item Pê°€ ë” ì—°ê´€ì„±ì´ ë†’ë‹¤ëŠ” ê²ƒ(Node proxmity â†‘)ì„ ì–´ë–»ê²Œ ì•Œ ìˆ˜ ìˆì„ê¹Œìš”? ì´ëŠ” Random Walksë¡œ í™•ì¸í•´ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤. . Random Walks . item Qê°€ ìš°ë¦¬ê°€ ì•Œê³ ì‹¶ì€ item ë…¸ë“œë“¤ì˜ ì§‘í•©ì¸ QUERY_NODESì§‘í•©ì— ì†í•´ìˆë‹¤ê³  í•´ë´…ì‹œë‹¤. Bipartite User-Item Graph ìƒì—ì„œ QUERY_NODES ì§‘í•©ì— ì†í•´ ìˆëŠ” ì–´ë–¤ ë…¸ë“œ(item Q)ì—ì„œ ì‹œì‘í•˜ì—¬ ëœë¤í•˜ê²Œ ì›€ì§ì´ë©´ì„œ ê³¼ì •ì„ ê¸°ë¡í•©ë‹ˆë‹¤. ì´ ê³¼ì •ì„ ê¸°ë¡í•œë‹¤ëŠ” ê²ƒì€ itemâ†”user ì‚¬ì´ë¥¼ ê³„ì† ëœë¤í•˜ê²Œ ì›€ì§ì´ë©´ì„œ ë°©ë¬¸(visit)í•˜ê²Œ ëœ item ë…¸ë“œì—ëŠ” +1 countë¥¼ í•˜ëŠ” ê²ƒì„ ì˜ë¯¸í•©ë‹ˆë‹¤. ì´ë ‡ê²Œ ëœë¤í•˜ê²Œ ì›€ì§ì´ë©´ì„œ ì´ë™ì„ ê²°ì •í•  ë•Œë§ˆë‹¤ ì¼ì • í™•ë¥  ALPHA ë§Œí¼ ì¬ì‹œì‘ì„ í•˜ê²Œë˜ëŠ”ë°, ì¬ì‹œì‘ì‹œì—ëŠ” QUERY_NODESì§‘í•©ì— ì†í•´ ìˆëŠ” í•˜ë‚˜ì˜ ë…¸ë“œë¡œ ì´ë™í•´ì„œ ë‹¤ì‹œ ëœë¤í•˜ê²Œ ì›€ì§ì´ê¸° ì‹œì‘í•©ë‹ˆë‹¤. (ì•„ë˜ pseudo code ì°¸ê³ ) . . ì´ë ‡ê²Œ ê³„ì† Random Walksë¥¼ í•˜ë‹¤ë³´ë©´ item ë…¸ë“œì˜ visit ìˆ˜ê°€ ë†’ì„ìˆ˜ë¡ query item Qì™€ ë†’ì€ ê´€ê³„ì„±ì„ ê°€ì§„ê²ƒìœ¼ë¡œ íŒë‹¨í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. . Benefits . ì´ì™€ ê°™ì€ Random Walksë¥¼ í†µí•œ ì‹œë®¬ë ˆì´ì…˜ê³¼ visit ìˆ˜ë¡œ ë…¸ë“œë“¤ê°„ì˜ ê·¼ì ‘ì„±(proximity)ì„ íŒë‹¨í•˜ëŠ”ë° ì¢‹ì€ ì´ìœ ëŠ” ë‹¤ìŒê³¼ ê°™ì€ ì‚¬í•­ë“¤ì„ ê³ ë ¤í•˜ì—¬ similarityë¥¼ ë‚˜íƒ€ë‚¼ ìˆ˜ ìˆëŠ” ë°©ë²•ì´ê¸° ë•Œë¬¸ì…ë‹ˆë‹¤. . Multiple connnections | Multiple paths | Direct and Indirect connections | Degree of the node | . PageRank Varients ì •ë¦¬ . PageRankì™€ ì´ë¥¼ ë³€í˜•í•œ ì´ 3ê°€ì§€ ì•Œê³ ë¦¬ì¦˜ë“¤ì„ ì •ë¦¬í•˜ë©´ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤. . PageRank Personalized PR Random Walk w/ Restarts . ëª¨ë“  ë…¸ë“œë“¤ì— ê°™ì€ í™•ë¥ ë¡œ teleport ì´ë™ | íŠ¹ì • ë…¸ë“œë“¤ë¡œ íŠ¹ì • í™•ë¥ ì„ ê°€ì§€ê³  teleport ì´ë™ | í•­ìƒ ë˜‘ê°™ì€ 1ê°œì˜ ë…¸ë“œë¡œ ì´ë™ | . . . Original Lecture Video : CS224W: Machine Learning with Graphs 2021 Lecture 4.3 - Random Walk with Restarts .",
            "url": "https://cs224w-kor.github.io/blog/matrix/pagerank/recommendation/proximity/random%20walks/ppr/restarts/2022/07/13/lecture-0403.html",
            "relUrl": "/matrix/pagerank/recommendation/proximity/random%20walks/ppr/restarts/2022/07/13/lecture-0403.html",
            "date": " â€¢ Jul 13, 2022"
        }
        
    
  
    
        ,"post15": {
            "title": "Lecture 4.2 - PageRank, How to Solve?",
            "content": ". Lecture 4. Graph as Matrix . Lecture 4.1 - PageRank | Lecture 4.2 - PageRank, How to Solve? | Lecture 4.3 - Random Walk with Restarts | Lecture 4.4 - Matrix Factorization and Node Embeddings | . . ì´ì „ì˜ ê°•ì˜ì—ì„œ Powe iteration ë°©ë²•ìœ¼ë¡œ ë°˜ë³µì ì¸ ë§¤íŠ¸ë¦­ìŠ¤ ê³± ì—°ì‚°ìœ¼ë¡œ $ mathbf{r}$ì„ êµ¬í•  ìˆ˜ ìˆìŒì„ í™•ì¸í–ˆìŠµë‹ˆë‹¤. ì´ ë°©ë²•ì— ëŒ€í•´ ì¡°ê¸ˆ ë” êµ¬ì²´ì ìœ¼ë¡œ ì‚´í´ë³´ê² ìŠµë‹ˆë‹¤. . Power Iteration Method . power iterationì€ 2ê°€ì§€ í‘œí˜„ì‹ì´ ìˆëŠ”ë° í•˜ë‚˜ëŠ” ë²¡í„°ì˜ ìš”ì†Œ ê´€ì ì—ì„œì˜ ì—…ë°ì´íŠ¸ ì‹(ì™¼ìª½)ê³¼ ë‹¤ë¥¸ í•˜ë‚˜ëŠ” ë§¤íŠ¸ë¦­ìŠ¤ ê´€ì ì˜ ì—…ë°ì´íŠ¸ ì‹(ì˜¤ë¥¸ìª½)ìœ¼ë¡œ ë‚˜íƒ€ë‚¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤. . . ê³¼ì •ì„ ì‚´í´ë³´ë©´ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤. . ì²˜ìŒ ì´ˆê¸°í™”ë¡œ ëª¨ë“  ë…¸ë“œì˜ importance scoreë¥¼ ë˜‘ê°™ì€ ê°’ìœ¼ë¡œ ë§Œë“¤ì–´ ì¤ë‹ˆë‹¤.(ë°˜ë³µì ì¸ ì—°ì‚°ìœ¼ë¡œ ìˆ˜ë ´ì„ ë³´ì¥í•˜ë¯€ë¡œ ì‚¬ì‹¤ ì–´ë–¤ ê°’ìœ¼ë¡œ ì´ˆê¸°í™”í•˜ë“  ìƒê´€ì—†ìŠµë‹ˆë‹¤.) $ boldsymbol{r}^{(0)}=[1 / N, ldots ., 1 / N]^{T}$ | ë°˜ë³µì ì¸ ì—°ì‚°ì„ í•˜ë©´ì„œ $ mathbf{r}$ ê°’ì„ ì—…ë°ì´íŠ¸í•©ë‹ˆë‹¤. $ boldsymbol{r}^{(t+1)}= boldsymbol{M} cdot boldsymbol{r}^{(t)}$ | ìˆ˜ë ´ì¡°ê±´ $ left| boldsymbol{r}^{( boldsymbol{t}+ mathbf{1})}- boldsymbol{r}^{(t)} right|_{1}&lt; varepsilon$ ì„ ë§Œì¡±í•  ë•Œê¹Œì§€ 2ë²ˆ ê³¼ì •ì˜ ì—°ì‚°ì„ ì§„í–‰í•©ë‹ˆë‹¤. | ì˜ˆì‹œ ê·¸ë˜í”„ì—ì„œì˜ power iteration ê³¼ì •ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤. . . . Three Questions . Does this converge? ë°˜ë³µì ì¸ ì—°ì‚°ê³¼ì •ì„ í†µí•´ ê°’ì´ ìˆ˜ë ´í•˜ëŠ”ê°€? | Does it converge to what we want? ìˆ˜ë ´í•œ ê°’ì´ ìš°ë¦¬ê°€ ì›í•˜ëŠ” ê°’ì¸ê°€? | Are results reasonable? ì—°ì‚° ê²°ê³¼ê°€ í•©ë‹¹í•œê°€?(ë§ì´ ë˜ëŠ”ê°€?) | (ì–´ìƒ‰í•œ í•œêµ­ì–´ ë²ˆì—­ë³´ë‹¤ ì˜ì–´ë¡œëœ ì§ˆë¬¸ì—ì„œ ì–»ì–´ê°€ëŠ” insightê°€ ì¢‹ì„ ê²ƒ ê°™ìŠµë‹ˆë‹¤.) . Problems . PageRankì—ëŠ” 2ê°€ì§€ì˜ ë¬¸ì œê°€ ìˆìŠµë‹ˆë‹¤. . Dead Ends | out-linkë¥¼ ê°€ì§€ì§€ ì•ŠëŠ” ì¼ë¶€ í˜ì´ì§€(ë…¸ë“œ)ë“¤ì—ì„œ ìƒê¸°ëŠ” ë¬¸ì œë¡œ ì´ëŸ° í˜ì´ì§€ë“¤ì—ì„œ importanceê°€ leak out ë©ë‹ˆë‹¤. leak outì˜ ì„¸ì–´ë‚˜ê°€ë‹¤ ë¼ëŠ” ëœ» ê·¸ëŒ€ë¡œ importance flowì˜ íë¦„ì—ì„œ ê°’ì´ ì„¸ì–´ë‚˜ê°€ëŠ” ë¬¸ì œë¥¼ ë§í•©ë‹ˆë‹¤. . ì•„ë˜ì˜ ì˜ˆì‹œì—ì„œ í˜ì´ì§€ bì—ì„œ ë‚˜ê°€ëŠ” out-linkê°€ ì—†ë‹¤ë³´ë‹ˆ importance updateë¥¼ í•œ ê²°ê³¼ê°€ $r_a = 0, r_b=0$ì´ ë¨ì„ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì´ëŠ” ì•ì„œ page rank $ mathbf{r}$ vectorì˜ ì •ì˜ì—ì„œ ì•½ì†í•œ ëª¨ë“  ë…¸ë“œì˜ importanceì˜ í•©ì´ 1ì´ ëœë‹¤ëŠ” column stochastic ìˆ˜í•™ì  ì „ì œì—ì„œ ë²—ì–´ë‚œ ê²°ê³¼ ì…ë‹ˆë‹¤. . . Spider traps | íŠ¹ì • í˜ì´ì§€ì˜ ëª¨ë“  out-linkë“¤ì´ ë‹¤ë¥¸í˜ì´ì§€ë¡œ ë‚˜ê°€ì§€ ì•Šì•„ ê²°êµ­ spider trap í˜ì´ì§€ê°€ ëª¨ë“  importance ê°’ì„ ë…ì°¨ì§€í•˜ê²Œ ë©ë‹ˆë‹¤. . ì•„ë˜ì˜ ì˜ˆì‹œì—ì„œ aì—ì„œ walkë¥¼ ì‹œì‘í•˜ë”ë¼ê³  bë¡œ ì´ë™í•œ í›„ bì—ì„œ ë¹ ì ¸ë‚˜ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì´ëŸ° ê²½ìš° importance update ê²°ê³¼ ëª¨ë“  importanceë¥¼ í˜ì´ì§€ bê°€ ê°€ì§€ê²Œ ë˜ì–´ $r_a = 0, r_b=1$ì´ ë©ë‹ˆë‹¤. ì´ëŸ° ê²½ìš° í˜ì´ì§€ aì— ì•„ë¬´ë¦¬ í° ì›¹ ê·¸ë˜í”„ê°€ ì—°ê²°ë˜ì–´ ìˆë‹¤ê³  í•˜ë”ë¼ë„ ì´ë™í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì‚¬ì‹¤ spider trapì€ column stochasticì„ ë§Œì¡±í•˜ê¸° ë•Œë¬¸ì— ìˆ˜í•™ì ìœ¼ë¡œ ë¬¸ì œë˜ì§„ ì•ŠìŠµë‹ˆë‹¤. í•˜ì§€ë§Œ ìš°ë¦¬ê°€ ì›í•˜ì§€ ì•ŠëŠ” ê°’ì— ìˆ˜ë ´í•˜ëŠ” ë¬¸ì œë¡œ ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤. . . Solutions . ìœ„ì˜ 2ê°€ì§€ ë¬¸ì œë“¤ ëª¨ë‘ Teleportsë¡œ í•´ê²°í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. . . Dead Endsë¥¼ Teleportsë¡œ í•´ê²°í•˜ê¸° | Dead Endsì¸ m í˜ì´ì§€ì—ì„œ column stochasticì„ ë§Œì¡±í•˜ì§€ ì•Šê³  ëª¨ë“  ê°’ì´ 0ì´ ë˜ì§€ ì•Šë„ë¡ ìì‹ ì„ í¬í•¨í•œ ê·¸ë˜í”„ì˜ ëª¨ë“  ë…¸ë“œë“¤ë¡œ uniform random í•˜ê²Œ teleport ì´ë™ì„ í•˜ë„ë¡ í•©ë‹ˆë‹¤. ì´ë•Œ ê·¸ë˜í”„ì˜ ë…¸ë“œê°€ ì´ 3ê°œì´ë¯€ë¡œ mì—´ì˜ í–‰ë ¬ê°’ì„ $1/3$ìœ¼ë¡œ ì±„ì›Œ $ mathbf{M}$ì„ ì™„ì„±í•©ë‹ˆë‹¤. . . Spider Trapsë¥¼ Teleportsë¡œ í•´ê²°í•˜ê¸° | Spier Trapì¸ m í˜ì´ì§€ì—ì„œ ë‹¤ë¥¸ ë…¸ë“œë¡œ ë¹ ì ¸ë‚˜ê°ˆ ìˆ˜ ìˆë„ë¡ ì¼ì • í™•ë¥  $1- beta$ë§Œí¼ random í˜ì´ì§€ë¡œ ì í•‘(teleport)í•  ìˆ˜ ìˆë„ë¡ í•©ë‹ˆë‹¤. ì¦‰, í™•ë¥  $ beta$ë§Œí¼ì€ ì›ë˜ ê·¸ë˜í”„ì˜ out-links ì¤‘ì— ê³¨ë¼ì„œ(random) ì´ë™í•˜ê³  ë‚˜ë¨¸ì§€ í™•ë¥ ($1- beta$)ë¡œëŠ” out-linkì™€ ìƒê´€ì—†ì´ ê·¸ë˜í”„ì˜ ëª¨ë“  í˜ì´ì§€ë“¤ ì¤‘ì— ê³¨ë¼ì„œ ì´ë™í•˜ì—¬ ê±°ë¯¸ì¤„, Spider trapì—ì„œ ë²—ì–´ë‚˜ê²Œ ë˜ëŠ” ê²ƒ ì…ë‹ˆë‹¤. ë³´í†µ $ beta$ê°’ìœ¼ë¡œëŠ” 0.8~0.9ê°’ì„ ì‚¬ìš©í•˜ëŠ” ê²ƒì´ ì¼ë°˜ì ì…ë‹ˆë‹¤. . . The Google Matrix . PageRankì—ì„œ ìƒê¸¸ ìˆ˜ ìˆëŠ” 2ê°€ì§€ ë¬¸ì œë¥¼ Teleportë¡œ í•´ê²°í•œë‹¤ë©´ PageRank Equationì€ ë‹¤ìŒê³¼ ê°™ì´ ë°”ê¿€ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì²«ë²ˆì§¸ í•­ì€ ê¸°ì¡´ì˜ ìˆ˜ì‹ì— ìˆë˜ ë¶€ë¶„ìœ¼ë¡œ í˜ì´ì§€ $i$ì˜ out-linkë¥¼ randomí•˜ê²Œ ê³¨ë¼ì„œ ì´ë™í•˜ëŠ” ê²ƒì—ë‹¤ í™•ë¥  $ beta$ê°’ì„ ê³±í•´ ë³´í†µ 0.8~0.9ì˜ í™•ë¥ ë¡œ out-linkë¥¼ í†µí•´ ì´ë™í•˜ê²Œ í•©ë‹ˆë‹¤. . ë‘ë²ˆì§¸ í•­ì€ Teleportë¥¼ë¡œ out-linkì™€ ìƒê´€ì—†ì´ ê·¸ë˜í”„ì˜ ëª¨ë“  í˜ì´ì§€ë“¤ì¤‘ í•˜ë‚˜ë¡œ ëœë¤í•˜ê²Œ ìˆœê°„ì´ë™í•˜ëŠ” ê²ƒì„ ìˆ˜ì‹ì ìœ¼ë¡œ í‘œí˜„í•œ ë¶€ë¶„ì…ë‹ˆë‹¤. ê·¸ë˜í”„ì— ì¡´ì¬í•˜ëŠ” ëª¨ë“  í˜ì´ì§€ì˜ ìˆ˜ë¥¼ $N$ì´ë¼ê³  í•  ë•Œ, ì¶”ê°€ì ìœ¼ë¡œ $1/N$ì˜ í™•ë¥ ë¡œ í˜ì´ì§€ $j$ë¡œ ê°ˆ ìˆ˜ ìˆê³  ì´ëŠ” ì•ì„œ í™•ë¥  $ beta$ë¥¼ ì œì™¸í•œ ë‚˜ë¨¸ì§€ í™•ë¥ , ì•½ 0.2~0.1ì˜ í™•ë¥ ë¡œ ì´ë™í•˜ëŠ” ê²ƒì´ë¯€ë¡œ $1- beta$ë¥¼ ê³±í•´ì¤ë‹ˆë‹¤. . rj=âˆ‘iâ†’jÎ²ridi+(1âˆ’Î²)1Nr_{j}= sum_{i rightarrow j} beta frac{r_{i}}{d_{i}}+(1- beta) frac{1}{N}rjâ€‹=iâ†’jâˆ‘â€‹Î²diâ€‹riâ€‹â€‹+(1âˆ’Î²)N1â€‹ . (ë‹¨, ìœ„ì˜ ìˆ˜ì‹ì€ $ mathbf{M}$ì— dead endsê°€ ì—†ë‹¤ê³  ê°€ì •í•˜ë©°, ì‹¤ì œë¡œ ëª¨ë“  dead endsë¥¼ ì—†ì• ê±°ë‚˜ dead endsì¸ ë¶€ë¶„ë“¤ì—ëŠ” random teleportë¥¼ í™•ë¥ 1ë¡œ ë”°ë¥´ê²Œ í•˜ì—¬ ê³„ì† ë‹¤ë¥¸ ë…¸ë“œë¡œ ì´ë™í•˜ê²Œ í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.) . êµ¬ê¸€ ë§¤íŠ¸ë¦­ìŠ¤ëŠ” ì´ì™€ í¬ê²Œ ë‹¤ë¥´ì§€ ì•ŠìŠµë‹ˆë‹¤. ë‹¨ì§€ ìœ„ì˜ PageRank equationì„ í–‰ë ¬ì‹ìœ¼ë¡œ ë°”ê¿”ì“°ë©´ êµ¬ê¸€ ë§¤íŠ¸ë¦­ìŠ¤ê°€ ë©ë‹ˆë‹¤. ê°ê°ì˜ í•­ë“¤ì´ ì˜ë¯¸í•˜ëŠ” ë°”ëŠ” ìœ„ì—ì„œ ì„¤ëª…ëœ ê²ƒê³¼ ë™ì¼í•˜ë©°, ë‘ë²ˆì§¸ í•­ì˜ $ left[ frac{1}{N} right]_{N times N}$ëŠ” í–‰ë ¬ì˜ ëª¨ë“  ì›ì†Œê°€ $ frac{1}{N}$ìœ¼ë¡œ ì±„ì›Œì§„ $N times N$ì°¨ì›ì˜ í–‰ë ¬ì„ ë§í•©ë‹ˆë‹¤. . G=Î²M+(1âˆ’Î²)[1N]NÃ—NG= beta M+(1- beta) left[ frac{1}{N} right]_{N times N}G=Î²M+(1âˆ’Î²)[N1â€‹]NÃ—Nâ€‹ . Random Teleports ($ beta=0.8$) . ì•„ë˜ì˜ $ beta=0.8$ì¼ ë•Œ Random Teleports ì˜ˆì‹œì—ì„œ ê²€ì€ìƒ‰ ì„ ë“¤ì€ teleportsë¥¼ ì ìš©í•˜ì§€ ì•Šì•˜ì„ ë•Œì˜ ê·¸ë˜í”„ì˜ directed linksë¥¼ í‘œí˜„í•˜ë©° ì´ˆë¡ìƒ‰ ì„ ë“¤ì€ 0.2í™•ë¥ ì˜ teleportsê°€ ì¶”ê°€ëœ ë¶€ë¶„ì„ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤. Power iterationì„ í†µí•´ ê³„ì‚°ë˜ë©´ í˜ì´ì§€ $y, a, m$ì´ ê°ê° $7/33, 5/33, 21/33$ìœ¼ë¡œ ìˆ˜ë ´í•˜ëŠ” ê²ƒì„ ì•Œ ìˆ˜ ìˆê³  spider trapì¸ í˜ì´ì§€ mì´ ëª¨ë“  importanceë¥¼ í¡ìˆ˜í•˜ì§€ ì•ŠëŠ” ê²ƒì„ í™•ì¸í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. . . Solving PageRank ì •ë¦¬ . PageRank $ mathbf{r} = mathbf{G} mathbf{r}$ì„ power iteration methodë¡œ í’€ ìˆ˜ ìˆë‹¤. | PageRankì—ì„œ ìƒê¸¸ ìˆ˜ ìˆëŠ” ë¬¸ì œë“¤ì¸ Dead Endsì™€ Spider Trapsë¥¼ Random Uniform Teleportationìœ¼ë¡œ í•´ê²°í•  ìˆ˜ ìˆë‹¤. | . . Original Lecture Video : CS224W: Machine Learning with Graphs 2021 Lecture 4.2 - PageRank: How to Solve? .",
            "url": "https://cs224w-kor.github.io/blog/matrix/pagerank/spider%20trap/dead%20ends/teleports/2022/07/13/lecture-0402.html",
            "relUrl": "/matrix/pagerank/spider%20trap/dead%20ends/teleports/2022/07/13/lecture-0402.html",
            "date": " â€¢ Jul 13, 2022"
        }
        
    
  
    
        ,"post16": {
            "title": "Lecture 4.1 - PageRank",
            "content": ". Lecture 4. Graph as Matrix . Lecture 4.1 - PageRank | Lecture 4.2 - PageRank, How to Solve? | Lecture 4.3 - Random Walk with Restarts | Lecture 4.4 - Matrix Factorization and Node Embeddings | . . 4ê°•ì—ì„œëŠ” Graphë¥¼ ë§¤íŠ¸ë¦­ìŠ¤(ì„ í˜•ëŒ€ìˆ˜) ê´€ì ìœ¼ë¡œ ë°”ë¼ë³´ëŠ” ê²ƒì— ëŒ€í•´ ì´ì•¼ê¸° í•©ë‹ˆë‹¤. . . ë‹¤ìŒ 3ê°€ì§€ í‚¤ì›Œë“œ, Random walk(Node Importance), Matrix Factorization, Node embeddingë¥¼ ì¤‘ì‹¬ìœ¼ë¡œ ê³µë¶€í•©ë‹ˆë‹¤. ê°•ì˜ëŠ” ì´ 4íŒŒíŠ¸ë¡œ ë‚˜ëˆ„ì–´ì ¸ ì§„í–‰ë©ë‹ˆë‹¤. . The Web as a Directed Graph . . ì›¹ì„ ê±°ì‹œì ì¸ ê´€ì ìœ¼ë¡œ ë³´ê²Œë˜ë©´, í•˜ë‚˜ì˜ ì›¹ í˜ì´ì§€ â†’ Nodeë¡œ í•˜ì´í¼ë§í¬ â†’ Edgeë¡œ ìƒê°í•˜ì—¬ í•˜ë‚˜ì˜ ê±°ëŒ€í•œ Graphë¡œ ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤. . Side issue . ë‹¤ì´ë‚˜ë¯¹í•˜ê²Œ ìƒˆë¡œ í˜ì´ì§€ë“¤ì´ ìƒê¸¸ ìˆ˜ ìˆìŠµë‹ˆë‹¤. | ë‹¤í¬ì›¹ê³¼ ê°™ì€ ì ‘ê·¼í•  ìˆ˜ ì—†ëŠ” í˜ì´ì§€ë“¤ë„ ìˆì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤. | . ì ì‹œ Side issueëŠ” ë‚´ë ¤ë†“ê³ , ìƒˆë¡œ í˜ì´ì§€ë“¤ì´ ìƒê¸°ì§€ë„ ì•Šê³  ê¸°ì¡´ì˜ í˜ì´ì§€ë“¤ì´ ì‚¬ë¼ì§€ì§€ë„ ì•ŠëŠ” Static pages ìƒí™©ì„ ê°€ì •í•´ë´…ì‹œë‹¤. ì•„ë˜ì˜ ê·¸ë¦¼ì—ì„œì²˜ëŸ¼ í˜ì´ì§€ë“¤ì€ í•˜ì´í¼ë§í¬ë“¤ë¡œ ì„œë¡œ ì—°ê²°ë˜ì–´ ìˆê³ , ìœ ì €ëŠ” í˜ì´ì§€ë“¤ì— ë‹¬ë ¤ìˆëŠ” í•˜ì´í¼ ë§í¬ë“¤ë¡œ ì´ë£¨ì–´ì§„ ì—°ê²°ë§ì„ ê¸°ë°˜ìœ¼ë¡œ í•­í•´í•˜ë“¯ì´ Navigational í•˜ê²Œ page to page ì´ë™ì„ í•˜ê²Œ ë©ë‹ˆë‹¤. (ì˜¤ëŠ˜ë‚ ì—ëŠ” post, comment, like ë“±ì˜ ê¸°ë°˜ì˜ transactionalí•œ ì›¹ì—ì„œì˜ ìƒí˜¸ì‘ìš©ì´ ì¼ì–´ë‚˜ì§€ë§Œ ì´ëŠ” ìš°ì„  ë…¼ì™¸ë¡œ í•˜ê² ìŠµë‹ˆë‹¤.) . . ìœ„ì˜ ê·¸ë¦¼ì²˜ëŸ¼ ì›¹ ê·¸ë˜í”„ëŠ” ë°©í–¥ì„±ì´ ìˆëŠ” ìœ í–¥ ê·¸ë˜í”„(Directed graph)ì„ì„ ì•Œ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ìœ„í‚¤í”¼ë””ì•„ì™€ ê°™ì€ ì›¹ ì‚¬ì „ í˜ì´ì§€ë“¤ ê°„ì˜ ê´€ê³„ì„±ì´ë‚˜ ë…¼ë¬¸ì˜ ì¸ìš© ê´€ê³„ ê·¸ë˜í”„ ë“±ì—ì„œ ì˜ˆì‹œë¥¼ ì‰½ê²Œ ì°¾ì•„ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤. . . Ranking Nodes on the Graph . ì›¹ì„ í•˜ë‚˜ì˜ ê±°ëŒ€í•œ ìœ í–¥ ê·¸ë˜í”„ë¡œ ìƒê°í•  ë•Œ í•œê°€ì§€ ì¤‘ìš”í•œ insightê°€ ìˆìŠµë‹ˆë‹¤. . ğŸ’¡ ëª¨ë“  ì›¹ í˜ì´ì§€ë“¤ì´ ë˜‘ê°™ì´ ì¤‘ìš”í•˜ì§€ëŠ” ì•Šë‹¤ . ë°”ë¡œ ê° í˜ì´ì§€ì˜ ì¤‘ìš”ì„±ì´ ë˜‘ê°™ì§€ ì•Šë‹¤ëŠ” ì´ì•¼ê¸°ëŠ” ê·¸ë˜í”„ì—ì„œ ê° ë…¸ë“œì˜ ì¤‘ìš”ì„±(importance)ê°€ ë‹¤ë¥´ë‹¤ëŠ” ë§ë¡œ ë°”ê¿” ìƒê°í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì•„ë˜ ì‚¬ì§„ì„ ë³´ë©´ ì§ê´€ì ìœ¼ë¡œ íŒŒë€ìƒ‰ ë…¸ë“œê°€ ë¹¨ê°„ìƒ‰ ë…¸ë“œë³´ë‹¤ ë” ì¤‘ìš”í•  ê²ƒ ê°™ë‹¤ë¼ê³  ìƒê°í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì™œ ê·¸ë ‡ê²Œ ë³´ì¼ê¹Œìš”? ì•„ì§ ë…¸ë“œì˜ ì¤‘ìš”ì„±ì— ëŒ€í•´ ì •ì˜í•˜ì§€ ì•Šì•˜ì§€ë§Œ ê·¸ë˜í”„ì—ì„œ ê° ë…¸ë“œë¥¼ ì¤‘ì‹¬ìœ¼ë¡œ ë»—ì–´ìˆëŠ” edge(link)ì˜ ìˆ˜ê°€ í•œëˆˆì— ë¹„êµë˜ê¸° ë•Œë¬¸ì— ì§ê´€ì ìœ¼ë¡œ íŒŒì•…í•  ìˆ˜ ìˆëŠ” ê²ƒì…ë‹ˆë‹¤. ì´ì²˜ëŸ¼ ì›¹ ê·¸ë˜í”„ì˜ link structureë¥¼ ê°€ì§€ê³  ìš°ë¦¬ëŠ” ê° í˜ì´ì§€ë“¤(node)ì˜ rankingì„ ë§¤ê¸¸ ìˆ˜ ìˆìŠµë‹ˆë‹¤. . . Link Analysis Algorithms . ê° í˜ì´ì§€ë“¤ì˜ ì¤‘ìš”ì„±(importance)ë¥¼ íŒŒì•…í•˜ê¸° ìœ„í•´ Link Analysisê°€ í•„ìš”í•©ë‹ˆë‹¤. . ë³¸ ìˆ˜ì—…ì—ì„œ ë‹¤ë£° Link Analysis ì•Œê³ ë¦¬ì¦˜ë“¤ì€ ì•„ë˜ ì´ 3ê°œì— ëŒ€í•´ì„œ ë‹¤ë£° ì˜ˆì •ì…ë‹ˆë‹¤. . PageRank | Personalized PageRank (PPR) | Random Walk with Restarts | . Links as Votes . ë§í¬ê°€ íˆ¬í‘œìš©ì§€ë¼ê³  ìƒê°í•´ë´…ì‹œë‹¤. ì—¬ê¸°ì„œ ìœ í–¥ ê·¸ë˜í”„ì¸ ì›¹ ê·¸ë˜í”„ì—ì„œ ë§í¬ëŠ” 2ê°€ì§€ ì¢…ë¥˜ê°€ ìˆë‹¤ëŠ” ê²ƒì„ ë‹¤ì‹œí•œë²ˆ ìƒê°í•´ë´ì•¼ í•©ë‹ˆë‹¤. . in-comming links(in-links): ê¸°ì¤€ í˜ì´ì§€ë¡œ ë“¤ì–´ì˜¤ëŠ” ë°©í–¥ì˜ ë§í¬ | out-going links(out-links): ê¸°ì¤€ í˜ì´ì§€ì—ì„œ ë‚˜ê°€ëŠ” ë°©í–¥ì˜ ë§í¬ | . ì´ë ‡ê²Œ ë°©í–¥ê¹Œì§€ ê³ ë ¤í•˜ì—¬ ë§í¬ë¥¼ íˆ¬í‘œë¼ê³  ìƒê°í•  ë•Œ, ì—„ë°€íˆ ë§í•˜ìë©´ in-linkë¥¼ íˆ¬í‘œë¼ê³  ìƒê°í•´ì•¼ í•  ê²ƒ ì…ë‹ˆë‹¤. í•œ ê°€ì§€ ë” ìƒê°í•´ë³¼ ë¬¸ì œëŠ” ëª¨ë“  in-linkë“¤ì„ ë™ë“±í•˜ê²Œ ìƒê°í•  ìˆ˜ ìˆëŠ”ê°€?ë¼ëŠ” ë¬¸ì œì…ë‹ˆë‹¤. ì–´ë–¤ ë§í¬ë“¤ì€ ë‹¤ë¥¸ ë§í¬ë“¤ì— ë¹„í•´ ì¢€ ë” ì¤‘ìš”í•œ í˜ì´ì§€ë¡œë¶€í„°(from) ê¸°ì¤€í˜ì´ì§€ë¡œ(to) ì˜¨ ë§í¬ì¼ ìˆ˜ë„ ìˆê¸° ë•Œë¬¸ì— countì— ì°¨ë“±ì„ ë‘¬ì•¼ í•˜ì§€ ì•Šì„ê¹Œë¼ê³  ìƒê°í•  ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤. ì´ëŸ° ê³ ë¯¼ë“¤ì€ ê²°êµ­ í˜ì´ì§€ë“¤ì´ ì„œë¡œ ì—°ê²°ë˜ì–´ ìˆì–´ì„œ recursiveí•œ ë¬¸ì œë¡œ ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤. . â• recursiveí•œ ë¬¸ì œë€, ë¬¼ë¦¬ê³  ë¬¼ë¦¬ëŠ” ë¬¸ì œë¡œ ìƒê°í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. Aâ†’B ë§í¬ì—ì„œ Aê°€ ì¤‘ìš”í•œ í˜ì´ì§€ë¼ëŠ” ì‚¬ì‹¤ì„ ê¸°ë°˜ìœ¼ë¡œ Bê°€ ì¤‘ìš”í•´ì§€ê³ , ì´ì–´ì§€ëŠ” Bâ†’C ë§í¬ì—ì„œ ì´ ì˜í–¥ì„ ì´ì–´ë°›ì•„ Cê¹Œì§€ ì¤‘ìš”í•œ í˜ì´ì§€ë¼ê³  íŒë‹¨í•˜ê²Œ ë˜ê¸° ë•Œë¬¸ì…ë‹ˆë‹¤. PageRank . The â€œFlowâ€ Model . ìœ„ì—ì„œ ì„¤ëª…í•œ recursiveí•œ íŠ¹ì„±ì„ ê¸°ë°˜ìœ¼ë¡œ ì¤‘ìš”ì„±ì´ í˜ëŸ¬ê°€ëŠ”(flow) ëª¨ë¸ì„ ìƒê°í•´ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì¤‘ìš”ì„±ì„ $r$ì´ë¼ëŠ” ë³€ìˆ˜ë¡œ ë‘ê³  ê¸°ì¤€ ë…¸ë“œ jì˜ importanceê°€ ì–´ë–»ê²Œ flowë˜ëŠ”ì§€ ì‚´í´ë³´ê² ìŠµë‹ˆë‹¤. . jë¡œ in-linkë˜ì–´ìˆëŠ” i, k ì˜ importance $r_i$, $r_k$ë¥¼ ê° ë…¸ë“œì˜ out-linkì˜ ìˆ˜ë§Œí¼ ë‚˜ëˆ„ì–´ì„œ jë¡œ ì „ë‹¬ë©ë‹ˆë‹¤. i ë…¸ë“œì˜ out-linkëŠ” ì´ 3ê°œ ì´ë¯€ë¡œ $ frac{r_i}{3}$, kë…¸ë“œì˜ out-linkëŠ” ì´ 4ê°œ ì´ë¯€ë¡œ $ frac{r_k}{4}$ë¡œ ê³„ì‚°ë˜ì–´ ë‘ ê°’ì˜ í•©ì´ $r_j$ê°€ ë©ë‹ˆë‹¤. | $r_j$ëŠ” jë…¸ë“œì˜ out-linkë¥¼ í†µí•´ flowí•˜ê²Œ ë˜ëŠ”ë° out-linkì˜ ìˆ˜, ì¦‰ 3ìœ¼ë¡œ ë‚˜ëˆ„ì–´ì ¸ $ frac{r_j}{3}$ ê°’ì´ ê°ê°ì˜ ë‹¤ìŒ ë…¸ë“œë“¤ë¡œ $r_j$ê°’ì´ ì „ë‹¬ë˜ê²Œ ë©ë‹ˆë‹¤. | . ì´ì²˜ëŸ¼ importanceê°€ ë†’ì€ í˜ì´ì§€ë¡œë¶€í„° in-linkëœ í˜ì´ì§€ëŠ” ì˜í–¥ì„ ë°›ì•„ importanceê°€ ë†’ì•„ì§ì„ ì•Œ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ë…¸ë“œ $j$ì˜ rank, $r_j$ë¥¼ ì •ì˜í•˜ë©´ ë‹¤ìŒê³¼ ê°™ì´ ìˆ˜ì‹ìœ¼ë¡œ ë‚˜íƒ€ë‚¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤. (ì´ë•Œ $d_i$ëŠ” ë…¸ë“œ iì˜ out-degreeë¥¼ ë§í•©ë‹ˆë‹¤.) . rj=âˆ‘iâ†’jridir_{j}= sum_{i rightarrow j} frac{r_{i}}{d_{i}}rjâ€‹=iâ†’jâˆ‘â€‹diâ€‹riâ€‹â€‹ . ë‹¤ìŒê³¼ ê°™ì€ ì˜ˆì‹œì—ì„œ ê° ê¸°ì¤€ ë…¸ë“œë¥¼ ê°€ì§€ê³  in-linkë“¤ì„ ê³ ë ¤í•˜ì—¬ â€œFlow equationâ€ì„ ê³„ì‚°í•´ë³´ë©´ ë‹¤ìŒê³¼ ê°™ë‹¤. . . ë…¸ë“œ y ë…¸ë“œ a ë…¸ë“œ m . yì—ì„œ ì˜¤ëŠ” ë§í¬ + aì—ì„œ ì˜¤ëŠ” ë§í¬ | yì—ì„œ ì˜¤ëŠ” ë§í¬ + mì—ì„œ ì˜¤ëŠ” ë§í¬ | aì—ì„œ ì˜¤ëŠ” ë§í¬ | . $r_y = frac{r_y}{2} + frac{r_a}{2}$ | $r_a = frac{r_y}{2} + r_m$ | $r_m = frac{r_a}{2}$ | . â• 3 Unknowns, 3 Equations ì´ê¸° ë•Œë¬¸ì— 4ë²ˆì§¸ constraintë¡œ $r_y + r_a + r_m =1$ë¡œ scaleê´€ë ¨ constraintë¥¼ ì¶”ê°€í•˜ì—¬ Gaussian eliminationì„ ì‚¬ìš©í•˜ì—¬ ì„ í˜•ë°©ì •ì‹ìœ¼ë¡œ í’€ë ¤ê³  í•˜ëŠ” ìƒê°ì€ ì¢‹ì§€ ì•Šë‹¤. ì™œëƒí•˜ë©´ importanceëŠ” ì´ëŸ°ì‹ìœ¼ë¡œ scalableí•˜ì§€ ì•Šê¸° ë•Œë¬¸ì´ë‹¤. (Itâ€™s not scalable) ì¢€ ë” ì •êµí•œ ì„¤ê³„ê°€ í•„ìš”í•˜ë‹¤. Matrix Formulation . Stochastic Adjacency Matrix $ mathbf{M}$ . $ mathbf{M}$ì€ $(nodeì˜ ìˆ˜) times (nodeì˜ ìˆ˜)$ì°¨ì›ì˜ ë§¤íŠ¸ë¦­ìŠ¤ ì…ë‹ˆë‹¤. | $i$â†’$j$ ë§í¬ì—ì„œ ë§¤íŠ¸ë¦­ìŠ¤ ìš”ì†Œ $M_{ji}$ëŠ” $ frac{1}{d_i}$ê°€ ë©ë‹ˆë‹¤. ($d_i$ë¥¼ ë…¸ë“œ $i$ì˜ out-degreeë¼ê³  ì •ì˜í•©ë‹ˆë‹¤.) . Mji=1diM_{ji} = frac{1}{d_i}Mjiâ€‹=diâ€‹1â€‹ ì˜¤ë¥¸ìª½ ì˜ˆì‹œì—ì„œì²˜ëŸ¼ ë…¸ë“œ $i$ë¥¼ ê¸°ì¤€ìœ¼ë¡œ ì´ 3ê°œì˜ out-linkë“¤ì´ ìˆë‹¤ë©´ ê°ê°ì˜ ê°’ì€ $1/3$ì´ ë©ë‹ˆë‹¤. . | column ê¸°ì¤€ stochastic : ì—´ ë°©í–¥ì˜ ëª¨ë“  ê°’ë“¤ì„ ë”í•˜ë©´ 1ì´ ë˜ëŠ” í™•ë¥ ê°’ì´ ë©ë‹ˆë‹¤. | . . Rank Vector $r$ . $ mathbf{r}$ì€ ê° í˜ì´ì§€ì˜ entry ê°’ì„ ê°€ì§€ëŠ” $(nodeì˜ ìˆ˜) times 1$ ì°¨ì›ì˜ ë²¡í„°ì…ë‹ˆë‹¤. | ê° í˜ì´ì§€ì˜ importance scoreë¥¼ $r_i$ë¡œ ì •ì˜í•©ë‹ˆë‹¤. | ëª¨ë“  ë…¸ë“œì˜ importance scoreì˜ í•©ì€ 1ì…ë‹ˆë‹¤. ë”°ë¼ì„œ ì´ ë˜í•œ í™•ë¥ ê°’ìœ¼ë¡œ ìƒê°í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. | . âˆ‘iri=1 sum_ir_i = 1iâˆ‘â€‹riâ€‹=1 . Flow Equations . ì´ì „ì— ì •ì˜í–ˆë˜ ë…¸ë“œì˜ rank ìˆ˜ì‹ì„ ìƒˆë¡­ê²Œ ì •ì˜í•œ ë§¤íŠ¸ë¦­ìŠ¤ $M$ê³¼ ë²¡í„° $r$ë¡œ ë‹¤ì‹œ ì¨ë³´ë©´ Flow Equationì„ ì™„ì„±í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. . r=Mâ‹…r mathbf{r}= mathbf{M} cdot mathbf{r}r=Mâ‹…r . ì•ì„œ ì‚´í´ë³¸ ê°„ë‹¨í•œ ê·¸ë˜í”„ ì˜ˆì‹œë¥¼ ê°€ì ¸ì™€ì„œ flow equationì„ ë§¤íŠ¸ë¦­ìŠ¤ ì—°ì‚°ìœ¼ë¡œ í‘œí˜„í•´ë³´ë©´ ì•„ë˜ì™€ ê°™ìŠµë‹ˆë‹¤. (flow equationì€ ì•ë‚´ìš©ì„ ì°¸ê³ ) . . . Connection to Random Walk . ë‹¤ìŒê³¼ ê°™ì€ ì¡°ê±´ì„ ë§Œì¡±í•˜ë©° ëœë¤í•˜ê²Œ ì›¹í˜ì´ì§€ë“¤ì„ ëŒì•„ë‹¤ë‹ˆê³  ìˆëŠ” ìœ ì €ë¥¼ ìƒê°í•´ë³´ê² ìŠµë‹ˆë‹¤. . ì‹œì  $t$ì— í˜ì´ì§€ $i$ì— ìˆìŠµë‹ˆë‹¤. | ë‹¤ìŒ ì‹œì  $t+1$ì— í˜ì´ì§€ $i$ë¡œë¶€í„° ë‚˜ê°€ëŠ” ë°©í–¥ì˜ out-linkë“¤ ì¤‘ì— uniformí•˜ê²Œ ì„ íƒí•˜ì—¬ ì„œí•‘ì„ í•©ë‹ˆë‹¤. | ì•ì„œ ì„ íƒëœ out-linkë¥¼ í†µí•´ $i$ì™€ ì—°ê²°ëœ $j$ í˜ì´ì§€ì— ë„ë‹¬í•©ë‹ˆë‹¤. | ì´ ê³¼ì •(1~3)ì„ ë¬´í•œìœ¼ë¡œ ë°˜ë³µí•©ë‹ˆë‹¤. | . ì—¬ê¸°ì—ì„œ ìš°ë¦¬ëŠ” ì‹œì  ì˜ ê°œë…ì„ ê³ ë ¤í•˜ì—¬ ìƒˆë¡œìš´ ê°œë… ì •ì˜ë¥¼ í•˜ë‚˜ í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. . p(t) mathbf{p(t)}p(t) . $ mathbf{p(t)}$ëŠ” í™•ë¥  ë²¡í„°(probability distribution)ë¡œ, ì´ ë²¡í„°ì˜ $i$ë²ˆì§¸ ìš”ì†ŒëŠ” ì•ì„œ ê°€ì •í•œ ìœ ì €ê°€ ì‹œì  $t$ì— í˜ì´ì§€ $i$ì— ìˆì„ í™•ë¥ ì„ ë‚˜íƒ€ëƒ…ë‹ˆë‹¤. . The Stationary Distribution . ì•ì„œ ì •ì˜í•œ $ mathbf{p(t)}$ë¥¼ ê°€ì§€ê³  ì´ ìœ ì €ê°€ ì‹œì  $t+1$ì— ìˆì„ í™•ë¥ ë¶„í¬ëŠ” ë‹¤ìŒê³¼ ê°™ì´ ê³„ì‚°í•©ë‹ˆë‹¤. . p(t+1)=Mâ‹…p(t) mathbf{p(t+1)}= mathbf{M} cdot mathbf{p(t)}p(t+1)=Mâ‹…p(t) . ğŸ’¡ ë§Œì•½ì— ìœ ì €ê°€ ì›¹ ì„œí•‘ì„ ê³„ì†í•˜ë‹¤ê°€ $ mathbf{p(t+1)} = mathbf{p(t)}$ ê°™ì€ ìƒí™©ì´ ë˜ë©´ ì–´ë–¨ê¹Œìš”? . p(t+1)=Mâ‹…p(t)=p(t) mathbf{p(t+1)}= mathbf{M} cdot mathbf{p(t)} = mathbf{p(t)}p(t+1)=Mâ‹…p(t)=p(t) . ì´ëŸ¬í•œ ìƒí™©ì—ì„œëŠ” ë” ì´ìƒ ìœ ì €ê°€ íŠ¹ì • í˜ì´ì§€ì— ìˆì„ í™•ë¥ ì´ ë³€í•˜ì§€ ì•Šê³  ìœ ì§€ë˜ëŠ” ê²½ìš°ê°€ ë˜ë©°, ì´ë¥¼ stationary distribution of a random walk ë¼ê³  í•©ë‹ˆë‹¤. . ì´ëŸ¬í•œ í˜•íƒœëŠ” ë‚®ì„¤ì§€ê°€ ì•Šì€ë°, ì•ì„œ rank vector $ mathbf{r}$ê°€ ë§¤íŠ¸ë¦­ìŠ¤ $ mathbf{M}$ê³¼ flow equationì„ êµ¬ì„±í•  ë•Œ ì´ëŸ¬í•œ ê¼´ì´ì—ˆìœ¼ë©°, ë”°ë¼ì„œ $ mathbf{r}$ì€ stationary distribution of a random walk ì…ë‹ˆë‹¤. . Eigenvector Formulation . ì´ì „ Lecture 2ì—ì„œ ì ì‹œ ë°°ì› ë˜ eigenvectorì™€ eignvalueë¥¼ ìƒê°í•´ë³´ë©´ ë‹¤ìŒ ìˆ˜ì‹ì„ ë– ì˜¬ë ¤ë³¼ ìˆ˜ ìˆìŠµë‹ˆë‹¤. . Î»c=Ac lambda mathbf{c} = mathbf{A} mathbf{c}Î»c=Ac . ì—¬ê¸°ì—ì„œ flow equationì„ ë‹¤ì‹œ ìœ„ì™€ ê°™ì€ ê¼´ë¡œ ì‘ì„±í•´ë³´ë©´, ì•„ë˜ì™€ ê°™ì´ eigenvalueê°€ 1ì´ê³  eigenvectorê°€ $ mathbf{r}$ì¸ ìˆ˜ì‹ìœ¼ë¡œ í•´ì„ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤. . 1â‹…r=Mâ‹…r1 cdot mathbf{r}= mathbf{M} cdot mathbf{r}1â‹…r=Mâ‹…r . ë”°ë¼ì„œ $ mathbf{r}$ì€ ë§¤íŠ¸ë¦­ìŠ¤ $ mathbf{M}$ì˜ principle eigenvector(eigenvalue 1)ì´ë©°, ì„ì˜ì˜ ë²¡í„° $ mathbf{u}$ì—ì„œ ì‹œì‘í•´ì„œ ê³„ì† ë§¤íŠ¸ë¦­ìŠ¤ $ mathbf{M}$ì„ ê³±í•˜ì—¬ ê·¹í•œ $ mathbf{M}( mathbf{M}(â€¦( mathbf{M}( mathbf{M} mathbf{u}))))$ìœ¼ë¡œ ë„ë‹¬í•˜ê²Œë˜ëŠ” long-term distributionì´ ë©ë‹ˆë‹¤. ì´ëŸ¬í•œ ë°©ì‹ìœ¼ë¡œ $ mathbf{r}$ì„ êµ¬í•˜ëŠ” ë°©ë²•ì„ Power iteration ì´ë¼ê³  í•©ë‹ˆë‹¤. . PageRank ì •ë¦¬ . ì›¹ êµ¬ì¡°ì—ì„œ ë³¼ ìˆ˜ ìˆëŠ” linkë“¤ì„ ê¸°ë°˜ìœ¼ë¡œ nodeë“¤ì˜ importanceë¥¼ ì¸¡ì •í•  ìˆ˜ ìˆë‹¤. | ëœë¤í•˜ê²Œ ì›¹ ì„œí•‘í•˜ëŠ” ìœ ì € ëª¨ë¸ì€ stochastic advacency matrix $ mathbf{M}$ìœ¼ë¡œ ë‚˜íƒ€ë‚¼ ìˆ˜ ìˆë‹¤. | PageRank ìˆ˜ì‹ì€ $ mathbf{r} = mathbf{M} mathbf{r}$ ì´ë©°, $ mathbf{r}$ì€ (1) ë§¤íŠ¸ë¦­ìŠ¤ $ mathbf{M}$ì˜ principle eigenvector, (2) stationary distribution of a random walk 2ê°€ì§€ë¡œ í•´ì„ë  ìˆ˜ ìˆë‹¤. | . . Original Lecture Video : CS224W: Machine Learning with Graphs 2021 Lecture 4.1 - PageRank .",
            "url": "https://cs224w-kor.github.io/blog/web/matrix/pagerank/rank/flow%20model/flow%20equations/eigenvector/stationary%20distribution/power%20iteration/2022/07/13/lecture-0401.html",
            "relUrl": "/web/matrix/pagerank/rank/flow%20model/flow%20equations/eigenvector/stationary%20distribution/power%20iteration/2022/07/13/lecture-0401.html",
            "date": " â€¢ Jul 13, 2022"
        }
        
    
  
    
        ,"post17": {
            "title": "Lecture N.1 - ê°•ì˜ ì†Œì œëª©(ë¶„í• ë˜ì–´ ìˆëŠ” ë™ì˜ìƒ ì œëª©)",
            "content": "ìœ„ì— titleì— íŠ¹ìˆ˜ë¬¸ì :ë‚˜ []ë¥¼ ì‚¬ìš©í•˜ì§€ ë§ˆì„¸ìš”! . .md íŒŒì¼ì˜ ì œëª©ì€ ë‚ ì§œ-lecture-0N0n.mdë¡œ ì‘ì„±í•´ì£¼ì„¸ìš”.(2022-07-13-lecture-0401) . Lecture N. ê°•ì˜ ì œëª© . N ê°•ì˜ì˜ ì²«ë²ˆì§¸ í¬ìŠ¤íŠ¸ì— ê°•ì˜ì— ëŒ€í•œ ì „ë°˜ì ì¸ ì†Œê°œë¥¼ ì‘ì„±í•©ë‹ˆë‹¤. &gt; í‘œì‹œë¥¼ ì•ì— ì‘ì„±í•©ë‹ˆë‹¤. . Imgurë¡œ ì´ë¯¸ì§€ë„ ë„£ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤.(ì•„ë˜ì— Imgurì— ëŒ€í•´ ì„¤ëª…ìˆìŠµë‹ˆë‹¤.) ì„¤ëª…ì„¤ëª… . ê°•ì˜ê°€ ì„¸ë¶„í™”ë˜ì–´ ìˆì–´ì„œ ë‚˜ëˆ„ì–´ì§„ í¬ìŠ¤íŒ…ë“¤ì„ ëª¨ì•„ì„œ í•œë²ˆì— ë³´ì—¬ì£¼ê¸° ìœ„í•´ ë§í¬ë¥¼ ë‹µë‹ˆë‹¤. . Lecture N.1 - ì†Œì œëª©1 | Lecture N.2 - ì†Œì œëª©2 | Lecture N.3 - ì†Œì œëª©3 | Lecture N.4 - ì†Œì œëª©4 | . . ì œëª© . ì œëª©ê°™ì€ ê²½ìš°ëŠ” ììœ ë¡­ê²Œ ì‘ì„±í•˜ì…”ë„ ë˜ë‚˜ ì €ëŠ” ê°•ì˜ ìë£Œ pptë¥¼ ê¸°ë°˜ìœ¼ë¡œ í•´ì„œ ì„¹ì…˜ ì œëª©(ì•½ 80% ì •ë„)ì„ ê°€ì ¸ì™”ìŠµë‹ˆë‹¤. . ì†Œì œëª© . #, ## ë“± ì œëª©ì„ ì„¸ë¶„í™”í•˜ë©´ ì¢‹ê² ì§€ë§Œ ë„ˆë¬´ ë§ì´ ì œëª©ì„ ì“°ë©´ ìœ„ì— TOCê°€ ê¸¸ì–´ì ¸ì„œ ë³´ê¸° ì¢‹ì§€ ì•Šì„ ê²ƒ ê°™ì•„ ì œëª©ì€ #ë§Œ ì“°ê³  í•˜ìœ„ ì œëª©ìœ¼ë¡œëŠ” **ì†Œì œëª©**ì„ ì‚¬ìš©í•´ì„œ ì‘ì„±í–ˆìŠµë‹ˆë‹¤. . ì´ë¯¸ì§€ . ì´ë¯¸ì§€ëŠ” ê¹ƒí—™ì— ì§ì ‘ì—…ë¡œë“œí•œ í›„ markdownìœ¼ë¡œ ì‘ì„±í•  ìˆ˜ ìˆìœ¼ë‚˜ ê·¸ëŸ¬ë©´ ì¶”í›„ì— ìš©ëŸ‰ë¬¸ì œê°€ ìƒê¸´ë‹¤ê³  í•´ì„œ Imgurë¥¼ ì´ìš©í•´ì„œ í¬ìŠ¤íŒ…í•©ë‹ˆë‹¤. . ê³„ì •ì„ ë§Œë“¤ì–´ì•¼ í•©ë‹ˆë‹¤. | ê³„ì • ì•„ì´ì½˜ì˜ ì•„ë˜ì— ìˆëŠ” Imagesë¥¼ ëˆ„ë£¹ë‹ˆë‹¤. . . | ì—…ë¡œë“œ í•˜ê³ ì í•˜ëŠ” ì´ë¯¸ì§€ë¥¼ ë“œë˜ê·¸ ì•¤ ë“œëì„ í•©ë‹ˆë‹¤. | ì—…ë¡œë“œ ëœ ì´ë¯¸ì§€ ìœ„ë¡œ ì»¤ì„œë¥¼ ê°€ì ¸ê°€ë©´ í¸ì§‘ ì•„ì´ì½˜ì´ ëœ¨ê³  ì´ê±¸ ëˆŒëŸ¬ì„œ ì´ë¯¸ì§€ í¬ê¸°ë¥¼ ì¡°ì ˆí•©ë‹ˆë‹¤. (ì´ë¯¸ì§€ í¬ê¸°ë¥¼ ì¡°ì ˆí• ë•Œë§ˆë‹¤ ì´ë¯¸ì§€ ì£¼ì†Œë§í¬ê°€ ë‹¬ë¼ì§€ê¸° ë•Œë¬¸ì— ê¸€ì— markdown ë§í¬ë¥¼ ê°€ì ¸ì˜¤ê¸° ì „ì— ì¡°ì •í•˜ëŠ” ê²ƒì´ ì¢‹ìŠµë‹ˆë‹¤. ì—¬ëŸ¬ê°œ ì´ë¯¸ì§€ë“¤ì„ ëˆŒëŸ¬ì„œ ì¼ì¼ì´ ëˆ„ë¥´ì§€ ì•Šê³  ì—°ì†ì ìœ¼ë¡œ í¸ì§‘í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.) . . ì €ëŠ” ê°œì¸ì ìœ¼ë¡œ ê°€ë¡œ 800í”½ì…€ì´ ë„˜ì–´ê°€ì§€ ì•ŠëŠ” ì„ ì—ì„œ widthë§Œ ì¡°ì ˆí•˜ë©´ ë‚˜ë¨¸ì§€ heightëŠ” ë¹„ìœ¨ì— ë§ê²Œ ì €ì¥ ë©ë‹ˆë‹¤. í¸ì§‘í›„ ìœ„ì— ìˆëŠ” saveë¥¼ ëˆŒëŸ¬ ì €ì¥í•´ì£¼ì„¸ìš”. . | í¬ê¸° í¸ì§‘ì„ ë‹¤ í•œ í›„ ì´ë¯¸ì§€ë¥¼ í´ë¦­í•˜ë©´ markdown linkë¥¼ ë³µì‚¬í•  ìˆ˜ ìˆê³  ì´ë¥¼ md íŒŒì¼ì— ë¶™ì—¬ë„£ìœ¼ë©´ ë©ë‹ˆë‹¤. | ì´ë•Œ `!`ì—†ì´ ë§í¬ê°€ ë³µì‚¬ë˜ëŠ”ë° md íŒŒì¼ì— ê°€ì ¸ì˜¬ë•Œ ì•ì— `!`ë¥¼ ë¶™ì—¬ì£¼ì„¸ìš”. (ì´ ë¶€ë¶„ì€ ë…¸ì…˜ì—ì„œ `callout`ìœ¼ë¡œ ì‘ì„±í•˜ë©´ ìƒê¸°ëŠ” ë¶€ë¶„ì…ë‹ˆë‹¤. **raw text**ë§Œ ì¸ì‹ë˜ë©° êµµì€ ê¸€ì”¨, ì½”ë“œ í¬ë§· ë“± ë‹¤ ì§€ì›ë˜ì§€ ì•ŠìŠµë‹ˆë‹¤. ìˆ˜ì‹ì€ ì§€ì›ë©ë‹ˆë‹¤. $a$) í‘œ . title 1 2 3 . ì¼ë°˜ì ì¸ | ì‘ì„± ì–‘ì‹ì„ | ë”°ë¦…ë‹ˆë‹¤. | :) | . í† ê¸€ . htmlì„ ì´ìš©í•˜ì—¬ í† ê¸€ì„ í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤. í•˜ì§€ë§Œ ì´ëŠ” ë…¸ì…˜ì˜ calloutê³¼ ë§ˆì°¬ê°€ì§€ë¡œ ì•ˆì— ë‚´ìš©ì€ raw textë§Œ ì§€ì›ë©ë‹ˆë‹¤. . í† ê¸€ëœ ì œëª© ìˆ¨ê²¨ì§„ ë‚´ìš©ì€ raw textë§Œ ë©ë‹ˆë‹¤. ë” ë‹¤ì–‘í•œ ì‘ì„± í˜•ì‹ . ì€ 2020-01-14-test-markdown-post.md ê³µì‹ ì˜ˆì‹œê¸€ì„ ì°¸ê³ í•´ì£¼ì„¸ìš”. ì—¬ê¸°ì— ì—†ëŠ” ë‹¤ë¥¸ ê¸°ëŠ¥ë“¤ì€ ë³´í†µ html í˜•ì‹ì„ ë§ì¶°ì£¼ë©´ ì ìš©ì´ ë˜ëŠ” ê²ƒ ê°™ìŠµë‹ˆë‹¤. . ì‘ì„± íë¦„ . ì‘ì„±í•˜ëŠ” íë¦„ì€ íŒê³µìœ í•˜ëŠ” ì°¨ì›ìœ¼ë¡œ ì œê°€ í•˜ê³  ìˆëŠ” ë°©ì‹ì„ ë§ì”€ë“œë¦¬ê² ìŠµë‹ˆë‹¤. . ë…¸ì…˜ì— ê°•ì˜ ë‚´ìš© ì •ë¦¬ | ë…¸ì…˜ì—ì„œ exportë¥¼ markdownìœ¼ë¡œ í•´ì„œ zip íŒŒì¼ë¡œ ë°›ì€ í›„ ì••ì¶•í•´ì œí•˜ë©´ ì•ˆì— md íŒŒì¼ê³¼ ì´ë¯¸ì§€ë“¤ì´ ëª¨ì•„ì ¸ ìˆëŠ” í´ë”ê°€ ìƒê¹ë‹ˆë‹¤. | blog repositoryì— ë‹¤ìš´ë°›ì€ md íŒŒì¼ì„ ë³µë¶™í•´ì„œ ì¡°ê¸ˆì˜ í¸ì§‘ì„ í•©ë‹ˆë‹¤.(ìœ„ì— ì‘ì„±í•´ì•¼ í•˜ëŠ” í•­ëª©(title, description, tag ë“±), ìˆ˜ì‹, ì¸í„°ë„· ë§í¬í™•ì¸) | Imgurì— ì ‘ì†í•´ì„œ 2ë²ˆì—ì„œ ë°›ì€ ì´ë¯¸ì§€ë“¤ì„ í•œë²ˆì— ì—…ë¡œë“œí•˜ê³  í¸ì§‘í•œ í›„ markdown linkë“¤ì„ md íŒŒì¼ì— ê°€ì ¸ì˜µë‹ˆë‹¤. | í¬ìŠ¤íŒ… ë§¨ ì•„ë˜ ì›ë˜ ë™ì˜ìƒ ë§í¬ë¥¼ ì²¨ë¶€í•©ë‹ˆë‹¤. | ê¹ƒí—™ì— commit í•©ë‹ˆë‹¤. fastpagesì—ì„œ ë§Œë“¤ì–´ë†“ì€ actionì´ ì‹¤í–‰ëœ í›„ ì¡°ê¸ˆ ì‹œê°„ì´ íë¥¸ í›„ í¬ìŠ¤íŒ…ì´ ì˜¬ë¼ì˜µë‹ˆë‹¤. | ì›í•˜ëŠ” ëŒ€ë¡œ í¬ìŠ¤íŒ…ì´ ì œëŒ€ë¡œ ì˜¬ë¼ì™”ëŠ”ì§€ í™•ì¸í•©ë‹ˆë‹¤. ì´ê³¼ì •ì„ ì—¬ëŸ¬ë²ˆ commití•˜ë©´ì„œ í™•ì¸í•´ì•¼ í•  ìˆ˜ë„ ìˆìŠµë‹ˆë‹¤. | ê¼­ ì´ ìˆœì„œëŒ€ë¡œ ì‘ì„±í•˜ì‹¤ í•„ìš”ëŠ” ì—†ìœ¼ë©° ì²˜ìŒì— ì¢€ ìµìˆ™í•´ì§€ëŠ”ë° ì‹œê°„ì´ ê±¸ë¦½ë‹ˆë‹¤. . ì´ëª¨ì§€ ì‘ì„±ê°€ëŠ¥í•©ë‹ˆë‹¤. https://getemoji.com/ ì œê°€ ì£¼ë¡œ ì´ìš©í•˜ëŠ” ì‚¬ì´íŠ¸ì…ë‹ˆë‹¤. | . ğŸ’¡ ì´ì™¸ì— í˜¹ì‹œ ì˜ ëª¨ë¥´ê² ëŠ” ë¶€ë¶„ì€ ë§ì”€í•´ì£¼ì„¸ìš”! ğŸ˜€ . . Original Lecture Video : ì›ë˜ ìœ íŠœë¸Œ ë™ì˜ìƒ ì œëª©ì„ ê·¸ëŒ€ë¡œ ë³µë¶™í•˜ë˜ |ì€ ì œì™¸í•˜ê³  ì‘ì„±í•´ì£¼ì„¸ìš”. ì € ë¬¸ìê°€ ìˆì„ ê²½ìš° í‘œë¡œ ì¸ì‹ë©ë‹ˆë‹¤. .",
            "url": "https://cs224w-kor.github.io/blog/tag1/tag2/keyword1/2022/07/01/template-0000.html",
            "relUrl": "/tag1/tag2/keyword1/2022/07/01/template-0000.html",
            "date": " â€¢ Jul 1, 2022"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "Questions",
          "content": "ìŠ¤í„°ë””ì—ì„œ ê°™ì´ ê³µë¶€í•˜ë©° ë‚˜ì™”ë˜ ì§ˆë¬¸ë“¤ì— ëŒ€í•´ ì •ë¦¬í•´ë³´ì•˜ìŠµë‹ˆë‹¤. í† ì˜í•˜ë©° ì •ë¦¬í•œ ê²ƒì´ë‹ˆ ì ˆëŒ€ì ì¸ ë‹µì€ ì•„ë‹˜ì„ ì£¼ì˜í•´ì£¼ì„¸ìš”. . Q: QUESTION | . A: blabla | . Contributers . Alphabetical order . JungYeon Lee | person2 | person3 | person4 | person5 | person6 | person7 | .",
          "url": "https://cs224w-kor.github.io/blog/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  

  
  

  
      ,"page11": {
          "title": "",
          "content": "Sitemap: {{ â€œsitemap.xmlâ€ | absolute_url }} | .",
          "url": "https://cs224w-kor.github.io/blog/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}