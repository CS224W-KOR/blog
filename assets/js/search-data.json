{
  
    
        "post0": {
            "title": "Fastpages Notebook Blog Post",
            "content": "About . This notebook is a demonstration of some of capabilities of fastpages with notebooks. . With fastpages you can save your jupyter notebooks into the _notebooks folder at the root of your repository, and they will be automatically be converted to Jekyll compliant blog posts! . Front Matter . The first cell in your Jupyter Notebook or markdown blog post contains front matter. Front matter is metadata that can turn on/off options in your Notebook. It is formatted like this: . # &quot;My Title&quot; &gt; &quot;Awesome summary&quot; - toc:true- branch: master - badges: true - comments: true - author: Hamel Husain &amp; Jeremy Howard - categories: [fastpages, jupyter] . Setting toc: true will automatically generate a table of contents | Setting badges: true will automatically include GitHub and Google Colab links to your notebook. | Setting comments: true will enable commenting on your blog post, powered by utterances. | . The title and description need to be enclosed in double quotes only if they include special characters such as a colon. More details and options for front matter can be viewed on the front matter section of the README. . Markdown Shortcuts . A #hide comment at the top of any code cell will hide both the input and output of that cell in your blog post. . A #hide_input comment at the top of any code cell will only hide the input of that cell. . The comment #hide_input was used to hide the code that produced this. . put a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it: . import pandas as pd import altair as alt . . put a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it: . cars = &#39;https://vega.github.io/vega-datasets/data/cars.json&#39; movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; sp500 = &#39;https://vega.github.io/vega-datasets/data/sp500.csv&#39; stocks = &#39;https://vega.github.io/vega-datasets/data/stocks.csv&#39; flights = &#39;https://vega.github.io/vega-datasets/data/flights-5k.json&#39; . . place a #collapse-output flag at the top of any cell if you want to put the output under a collapsable element that is closed by default, but give the reader the option to open it: . print(&#39;The comment #collapse-output was used to collapse the output of this cell by default but you can expand it.&#39;) . The comment #collapse-output was used to collapse the output of this cell by default but you can expand it. . . Interactive Charts With Altair . Charts made with Altair remain interactive. Example charts taken from this repo, specifically this notebook. . Example 1: DropDown . # use specific hard-wired values as the initial selected values selection = alt.selection_single( name=&#39;Select&#39;, fields=[&#39;Major_Genre&#39;, &#39;MPAA_Rating&#39;], init={&#39;Major_Genre&#39;: &#39;Drama&#39;, &#39;MPAA_Rating&#39;: &#39;R&#39;}, bind={&#39;Major_Genre&#39;: alt.binding_select(options=genres), &#39;MPAA_Rating&#39;: alt.binding_radio(options=mpaa)} ) # scatter plot, modify opacity based on selection alt.Chart(df).mark_circle().add_selection( selection ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=&#39;IMDB_Rating:Q&#39;, tooltip=&#39;Title:N&#39;, opacity=alt.condition(selection, alt.value(0.75), alt.value(0.05)) ) . Example 2: Tooltips . alt.Chart(df).mark_circle().add_selection( alt.selection_interval(bind=&#39;scales&#39;, encodings=[&#39;x&#39;]) ).encode( alt.X(&#39;Rotten_Tomatoes_Rating&#39;, type=&#39;quantitative&#39;), alt.Y(&#39;IMDB_Rating&#39;, type=&#39;quantitative&#39;, axis=alt.Axis(minExtent=30)), # y=alt.Y(&#39;IMDB_Rating:Q&#39;, ), # use min extent to stabilize axis title placement tooltip=[&#39;Title:N&#39;, &#39;Release_Date:N&#39;, &#39;IMDB_Rating:Q&#39;, &#39;Rotten_Tomatoes_Rating:Q&#39;] ).properties( width=500, height=400 ) . Example 3: More Tooltips . label = alt.selection_single( encodings=[&#39;x&#39;], # limit selection to x-axis value on=&#39;mouseover&#39;, # select on mouseover events nearest=True, # select data point nearest the cursor empty=&#39;none&#39; # empty selection includes no data points ) # define our base line chart of stock prices base = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)), alt.Color(&#39;symbol:N&#39;) ) alt.layer( base, # base line chart # add a rule mark to serve as a guide line alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), # add circle marks for selected time points, hide unselected points base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), # add white stroked text to provide a legible background for labels base.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=&#39;price:Q&#39; ).transform_filter(label), # add text labels for stock prices base.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=&#39;price:Q&#39; ).transform_filter(label), data=stocks ).properties( width=500, height=400 ) . Data Tables . You can display tables per the usual way in your blog: . df[[&#39;Title&#39;, &#39;Worldwide_Gross&#39;, &#39;Production_Budget&#39;, &#39;Distributor&#39;, &#39;MPAA_Rating&#39;, &#39;IMDB_Rating&#39;, &#39;Rotten_Tomatoes_Rating&#39;]].head() . Title Worldwide_Gross Production_Budget Distributor MPAA_Rating IMDB_Rating Rotten_Tomatoes_Rating . 0 The Land Girls | 146083.0 | 8000000.0 | Gramercy | R | 6.1 | NaN | . 1 First Love, Last Rites | 10876.0 | 300000.0 | Strand | R | 6.9 | NaN | . 2 I Married a Strange Person | 203134.0 | 250000.0 | Lionsgate | None | 6.8 | NaN | . 3 Let&#39;s Talk About Sex | 373615.0 | 300000.0 | Fine Line | None | NaN | 13.0 | . 4 Slam | 1087521.0 | 1000000.0 | Trimark | R | 3.4 | 62.0 | . Images . Local Images . You can reference local images and they will be copied and rendered on your blog automatically. You can include these with the following markdown syntax: . ![](my_icons/fastai_logo.png) . . Remote Images . Remote images can be included with the following markdown syntax: . ![](https://image.flaticon.com/icons/svg/36/36686.svg) . . Animated Gifs . Animated Gifs work, too! . ![](https://upload.wikimedia.org/wikipedia/commons/7/71/ChessPawnSpecialMoves.gif) . . Captions . You can include captions with markdown images like this: . ![](https://www.fast.ai/images/fastai_paper/show_batch.png &quot;Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/&quot;) . . Other Elements . GitHub Flavored Emojis . Typing I give this post two :+1:! will render this: . I give this post two :+1:! . Tweetcards . Typing &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this: Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 . Youtube Videos . Typing &gt; youtube: https://youtu.be/XfoYk_Z5AkI will render this: . Boxes / Callouts . Typing &gt; Warning: There will be no second warning! will render this: . Warning: There will be no second warning! . Typing &gt; Important: Pay attention! It&#39;s important. will render this: . Important: Pay attention! It&#8217;s important. . Typing &gt; Tip: This is my tip. will render this: . Tip: This is my tip. . Typing &gt; Note: Take note of this. will render this: . Note: Take note of this. . Typing &gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs: . Note: A doc link to an example website: fast.ai should also work fine. . Footnotes . You can have footnotes in notebooks, however the syntax is different compared to markdown documents. This guide provides more detail about this syntax, which looks like this: . For example, here is a footnote {% fn 1 %}. And another {% fn 2 %} {{ &#39;This is the footnote.&#39; | fndetail: 1 }} {{ &#39;This is the other footnote. You can even have a [link](www.github.com)!&#39; | fndetail: 2 }} . For example, here is a footnote 1. . And another 2 . 1. This is the footnote.↩ . 2. This is the other footnote. You can even have a link!↩ .",
            "url": "https://cs224w-kor.github.io/blog/jupyter/2022/09/29/test.html",
            "relUrl": "/jupyter/2022/09/29/test.html",
            "date": " • Sep 29, 2022"
        }
        
    
  
    
        ,"post1": {
            "title": "Lecture 17.4 - Scaling up by Simplifying GNNs",
            "content": ". Lecture 17. Scaling Up Graph Neural Networks to Large Graphs . Lecture 17.1 - Scaling up Graph Neural Networks | Lecture 17.2 - GraphSAGE Neighbor Sampling | Lecture 17.3 - Cluster GCN: Scaling up GNNs | Lecture 17.4 - Scaling up by Simplifying GNNs | . . 이때까지 SGD training에서 mini-batch를 구성하는 방식을 바꿔가며 큰 그래프를 학습할 수 있도록 만들었던 것에 반해, 지금부터는 mini-batch는 랜덤 샘플링 그대로 두고, 반대로 GNN 구조를 바꾸어 큰 그래프를 학습하는 방법에 대해서 알아보도록 하겠습니다. 어떻게 보면 큰 그래프를 학습하려고 하는 목적은 같지만 이전에 배운 방법들에 대해 orthogonal 한 방법이라고 보시면 되겠습니다. . Roadmap of Simplifying GCN . 먼저 대표적인 GNN 모델 중 하나인 GCN으로부터 시작해서 모델을 simplify 해 나가 보겠습니다. Wu et al.에 따르면 단순하게 GCN에서 비선형 활성 함수를 제거함으로써 모델 디자인을 굉장히 scalable하게 만들 수 있다고 합니다. 더욱 놀라운 점은, 활성 함수 제거라는 simplification을 하더라도 모델의 성능에는 큰 차이가 없다는 것인데요, 이 놀라운 발견에 대해 이제부터 차차 설명 드리도록 하겠습니다. . 💡 GCN에서 ReLU 활성 함수를 뺌으로써 구조를 단순화하자! . Recall: GCN (mean-pool) . 설명에 앞서 먼저 GCN을 되짚어 보도록 하죠. 이 부분은 귀가 닳도록 이전 강의들에서 반복한 내용이니 간단하게만 언급하고 넘어갈게요. 여기서 그래프 $G=(V,E)$가 주어졌고, 엣지 집합 $E$는 각 노드에 대한 self-loop을 포함한다고 가정해보겠습니다. 이 때, GCN layer $k$에 대한 이웃 노드 aggregation과 transform 과정은 아래와 같은 벡터식으로 표현됩니다. . . Layer 0에서의 초기 노드 임베딩을 $h_v^{(0)} = X_v$로 두고 총 $K$개의 GCN layer을 거치며 반복적으로 노드 임베딩을 업데이트 하면, 마지막 layer에서 최종 노드 임베딩 $z_v = h_v^{(K)}$를 얻을 수 있습니다. . 이처럼 벡터식으로 표현된 전 과정을 더욱 간단하게 행렬식으로도 표현할 수 있는데요, 우선 반복적으로 사용할 notation 부터 정리하고 가겠습니다. . Layer $k$에서의 노드 임베딩 행렬 : $H^{(k)} = [h_1^{(k)}, …, h_{ | V | }^{(k)}]^T$ | . | Self-loop를 포함한 그래프의 인접 행렬 : $A$ | $D$를 $D_{v,v} = Deg(v) = | N(v) | $의 대각선 행렬로 정의한다면, | . $D$의 역행렬인 $D^{-1}$ 또한 $D_{v,v}^{-1} = 1/ | N(v) | $를 만족하는 대각선 행렬로 정의 가능 | . | . 위와 같은 notation을 통해 GCN의 ‘Mean-pooling’ 부분을 아래와 같이 행렬식으로 변환할 수 있습니다. . . 최종적으로 GCN의 벡터식과 행렬식을 이렇게 정리해 볼 수 있겠네요. 행렬식을 통해 계산하면 모든 노드에 대해 한번에 계산이 가능하다는 장점이 있습니다. . . Simplifying GCN . 자, 이제 아까 말했듯이 GCN의 수식에서 ReLU 활성 함수를 빼보도록 하겠습니다. 활성 함수를 뺀 GCN 행렬식은 아래와 같은데요, 이를 길게 전개해서 layer $K$에서의 최종 노드 임베딩을 구하는 식으로 바꿔보겠습니다. . . 여기서 여러 선형 변환 행렬 $W_k$의 곱 또한 선형 변환 행렬인 $W$로 쓸 수 있다는 점에 주의하십시오. 단숨에 $K$개의 GCN layer을 통과한 최종 노드 임베딩을 얻기 위해서는, 우선 0번째 layer에서의 초기 노드 임베딩 $X$에 인접 행렬을 $K$번 곱함으로써 $K$-hop 까지의 이웃 노드 정보를 aggregate 해야 합니다. 다만 simplified 된 GCN의 경우 비선형 함수가 제외되어 각 GCN layer에서 행해지는 transformation을 하나의 선형 변환으로 표현할 수 있기 때문에 마지막에 한번 $W$를 곱해주기만 하면 최종 노드 임베딩을 구할 수 있는 것이죠. 최종 행렬식을 직관적으로 이해하는 것도 어렵지 않죠? . 이처럼 GCN에서 단순히 비선형 함수를 없애는 것 만으로도 모델 구조를 아주 단순화 시킬 수 있습니다. 특히, $ tilde{A}^KX$는 학습되지 않는 부분이기 때문에 처음에 먼저 계산해 놓고 그냥 필요할 때 효율적으로 사용하면 된답니다. 간단하게 $ tilde{X} = tilde{A}^KX$라고 하고 다시 한번 식을 쓴다면 아래와 같습니다. 즉, 최종 노드 임베딩은 그저 미리 연산된 행렬 $ tilde{X}$에 선형 변환을 하여 쉽게 계산할 수 있다는 것이죠. . . 💡 ReLU를 뺀 GCN의 행렬식 전개 결과 → $H^{(K)} = tilde{A}^KXW^T = tilde{X}W^T$ . Simplified GCN: Summary . 우리는 그대로 $M$개의 노드를 랜덤 추출하여 mini-batch를 구성하는 SGD training을 통해 학습을 진행할 것이기 때문에 전 학습 과정을 아래와 같이 표현할 수 있습니다. . Pre-processing . 먼저 미리 계산 가능한 $ tilde{X}= tilde{A}^KX$를 구해 놓습니다. 이는 CPU 위에서 빠르게 계산할 수 있습니다. . | Mini-batch training . 전체 노드 중에 $M$개의 노드를 랜덤으로 샘플링 하여 mini-batch를 구성합니다. 각 mini-batch는 ${v_1, v_2, …, v_M}$으로 표현할 수 있겠네요. | 각 노드에 대한 최종 노드 임베딩을 $h_{v_i}^{(K)} = W tilde{X}_{v_i}$ 를 통해 구합니다. | 나머지 과정은 동일합니다. 생성된 노드 임베딩을 갖고 적절한 loss 값을 계산하여 역전파 함으로써 모델 파라미터를 업데이트 하는 것이죠. | . | Comparison with Other Methods &amp; Potential Issue of Simplified GCN . Simplified GCN vs. Neighbor Sampling . Neighbor Sampling처럼 computation graph를 일일이 만들어서 mini-batch를 구성하지 않아도 되기 때문에 Simplified GCN이 학습 시간 측면에서는 더 효율적입니다. . | Simplified GCN vs. Cluster-GCN . Cluster-GCN에서 mini-batch가 특정 커뮤니티에 국한되어 전체 그래프를 대표하지 못하여 여러 커뮤니티를 굳이 굳이 붙여서 사용해야 하는데 반해, Simplified GCN에선 그냥 랜덤으로 노드를 샘플링해서 mini-batch를 구성하면 되므로 훨씬 간편합니다. 또한, 완전히 랜덤으로 mini-batch를 만들기 때문에 배치 마다의 편차도 작아져서 학습이 보다 안정적입니다. . | . 앞에서 배운 두 방법과 Simplified GCN을 비교해보면 Simplified GCN이 여러 측면에서 훨씬 효율적이라는 것을 볼 수 있습니다. 그렇다면 그냥 일괄적으로 Simplified GCN을 사용하여 큰 그래프를 학습하면 되는 것 아닐까요? . 하지만, Simplified GCN은 노드 임베딩을 만드는데 있어서 비선형성을 완전히 제거해 버렸기 때문에 표현력 측면에서는 다른 두 방법에 비해 크게 떨어진다는 단점이 있습니다. . Performance of Simplified GCN . 표현력이 떨어지는 단점에도 불구하고 GCN은 semi-supervised node classification 태스크에서 오리지널 GCN에 필적할만한 우수한 성능을 보이는데요, 대체 그 이유가 무엇일까요? . . Graph Homophily . 이전 강의에서도 다루었듯이, homophily란 유유상종을 나타내는 말입니다. 즉, 그래프 상에서 엣지로 연결된 노드들은 동일한 정답 라벨을 가질 가능성이 높다고 해석할 수 있겠네요. Graph homophily는 실생활의 그래프에서도 잘 드러나는데요, 예를 들어 논문-인용 네트워크에서 서로 인용 관계 (엣지)로 연결된 논문 노드들은 서로 비슷한 연구 주제를 다룰 가능성이 높고, 소셜 네트워크에서 서로 친구 관계인 사용자 노드끼리 영화 취향이 비슷할 가능성이 높은 것처럼 말이죠. . 반면, Simplified GCN에서 $ tilde{A}^KX$를 계산하는 과정을 다시 생각해 볼까요? 이를 위해 $K$회 반복하여 $X leftarrow tilde{A}X$를 연산하면 되는데, 이 과정에서 반복적으로 이웃 노드 feature의 평균을 활용하여 타겟 노드 feature를 업데이트 하기 때문에 결국 엣지로 연결된 노드 끼리 비슷한 $ tilde{A}^KX$를 갖게 됩니다. . . 우리의 Simplified GCN 모델은 결국 $ tilde{A}^KX$에 단순 선형 변환을 거친 최종 노드 임베딩을 활용하여 downstream task에서 예측값을 만들게 되는데, 엣지로 연결된 노드끼리는 최종 노드 임베딩이 비슷하므로 결국 비슷한 예측값을 가지게 될 것입니다. 따라서 graph homophily가 강하게 드러나는 그래프의 경우에는 node classification task에서 Simplified GCN의 예측이 정답 라벨과 비슷한 양상을 띠어 좋은 성능을 나타낼 수 밖에 없죠. . Summary: Simplified GCN . 💡 1. GNN 구조 : ReLU 활성 함수를 빼서 간소화함2. Mini-batch 구성 방식: 일반적인 랜덤 샘플링으로 구성함3. 학습 방식: SGD training . Simplified GCN은 오리지널 GCN에서 ReLU 활성 함수를 빼서 노드 임베딩 생성 과정을 단순한 pre-processing 연산으로 바꿈으로써, 효율적인 임베딩 생성을 가능하게 함 (이 과정은 CPU에서 수행할 수 있을 정도로 가볍고 효율적임!) | $ tilde{A}^KX$를 미리 계산해두면, scalable하게 mini-batch를 구성하여 SGD training을 할 수 있음 | Graph homophily가 잘 드러나는 그래프에 대해 node classification task에서 우수한 성능을 보임 | . .",
            "url": "https://cs224w-kor.github.io/blog/gnn/scalable/large%20graph/graphsage/cluster%20gcn/simplified%20gcn/2022/09/07/lecture-1704.html",
            "relUrl": "/gnn/scalable/large%20graph/graphsage/cluster%20gcn/simplified%20gcn/2022/09/07/lecture-1704.html",
            "date": " • Sep 7, 2022"
        }
        
    
  
    
        ,"post2": {
            "title": "Lecture 17.3 - Cluster GCN - Scaling up GNNs",
            "content": ". Lecture 17. Scaling Up Graph Neural Networks to Large Graphs . Lecture 17.1 - Scaling up Graph Neural Networks | Lecture 17.2 - GraphSAGE Neighbor Sampling | Lecture 17.3 - Cluster GCN: Scaling up GNNs | Lecture 17.4 - Scaling up by Simplifying GNNs | . . Issues with Neighbor Sampling . 17.2에서 말했듯이 Neighbor Sampling 방식으로 mini-batch를 구성하는데는 여러 문제점이 있습니다. . Computation graph의 크기가 GNN layer의 갯수에 대해 지수적으로 증가하여 계산 효율성이 크게 떨어지게 됩니다. | 샘플링 된 이웃 노드들이 많이 겹치는 경우, redundant한 계산이 많이 일어나기 때문에 효율적이지 않습니다. 아래 그림에서 보다시피 노드 C와 D는 두 computation graph에 모두 포함되어 여러번 불필요하게 연산에 참여하게 됩니다. | . . Insight from Full-batch GNN . 이러한 비효율성을 어떻게 해결하면 좋을까요? 해답은 full-batch GNN에서 얻을 수 있었습니다. 다시 full-batch를 떠올려보죠. . . Full-batch training으로 학습할 때, 한 GNN layer에서 위 식을 통해 모든 노드의 임베딩을 동시에 업데이트 합니다. 일괄적으로 $l-1$번째 layer의 노드 임베딩을 한번씩만 사용하여 $l$번째 layer의 노드 임베딩을 생성하기 때문에 Neighbor Sampling에서처럼 redundant한 연산은 일어나지 않게 됩니다. 이를 더 자세히 들여다 보면 아래와 같이 동작하게 됩니다. . . 하나의 GNN layer을 기준으로 computation graph를 만들고 자시고 할 것 없이 전체 그래프에 대해 일괄적으로 노드 임베딩을 생성합니다. 이 때 모든 엣지에 대해 양방향으로 두 번의 message passing이 일어납니다. → 시간 복잡도 $ propto O(2*no.(edges))$ | $K$-layer GNN을 기준으로 하면, → 시간 복잡도 $ propto O(2K*no.(edges))$ | . 따라서 full-batch training의 시간 복잡도는 엣지의 갯수와 GNN layer의 갯수에 선형 비례하기 때문에 이론적으론 굉장히 빨라야 합니다. 하지만, 우리가 다루는 그래프가 큰 그래프인만큼 엣지의 수가 굉~장히 크고 전체 그래프를 GPU 메모리 상에 로딩하는 것이 불가능하기 때문에 full-batch training을 사용할 수 없는 것이었죠. . Subgraph Sampling . 그렇다면 전체 그래프 말고 더 작은 subgraph를 추출하여 subgraph에 대해 full-batch training을 적용하면 어떨까요? 즉, 이 아이디어는 곧 mini-batch를 전체 그래프에서 추출한 subgraph로 구성하여 SGD training을 하면 보다 계산 redundancy를 줄이고, 더 효율적으로 큰 그래프를 학습할 수 있다는 말과 같습니다. 그러려면 아래 그림과 같이 전체 그래프로부터 GPU에 올려질 수 있을 법한 작은 subgraph들을 잘 추출해내야 합니다. . . 💡 Subgraph로 mini-batch를 구성하자! . 그럼 여기서, 어떻게 전체 그래프를 잘 설명할 수 있는 subgraph들을 추출할 수 있을까요? Full-batch training에서는 엣지를 따라 일어나는 message passing을 통해 노드 임베딩을 업데이트하기 때문에, 좋은 subgraph란 전체 그래프의 엣지 연결 구조를 잘 드러내는 모양새여야 할 것입니다. . . 위 그림의 왼쪽, 오른쪽 subgraph 중에 어떤 subgraph가 더 좋을까요? 왼쪽 subgraph는 원본 그래프의 커뮤니티 구조를 잘 유지하고 있어서 좋은 subgraph가 될 수 있는 반면, 오른쪽 subgraph는 원본 그래프의 중요한 엣지들을 많이 버린 구조기 때문에 안 좋은 subgraph라 볼 수 있습니다. 특히나 오른쪽 subgraph에서 생긴 고립된 노드들은 어떠한 엣지로도 연결 되어 있지 않기 때문에 노드 임베딩을 제대로 만들 수 없을 것입니다. . Exploiting Community Structure . . 13장에서 설명하였듯이 실생활에 쓰이는 대부분의 그래프는 위와 같이 커뮤니티 구조를 가집니다. 따라서 좋은 subgraph로 mini-batch를 구성하기 위해 원본 그래프의 국소적 엣지 연결 구조를 잘 보존하고 있는 커뮤니티가 subgraph로 사용되는 것이 좋아 보입니다. . 💡 원본 그래프의 커뮤니티 구조를 subgraph로 사용하자! . Cluster-GCN . Cluster-GCN은 mini-batch로 subgraph를 쓰자는 아이디어로부터 제안되었으며 두 단계를 통해 학습이 수행됩니다. . . Pre-processing | . 원본의 큰 그래프 $G$가 주어졌을 때, 아무 scalable한 community detection 알고리즘 (e.g., Louvain, METIS, BigCLAM 등)을 사용해서 위 그림과 같이 $G_i$로 커뮤니티를 분리해냅니다. 이 때, 각 커뮤니티 $G_i$가 subgraph로 사용될 것이기 때문에 커뮤니티 내부의 엣지는 보존하되 커뮤니티 사이의 엣지는 버립니다. 여기서 subgraph를 랜덤하게 뽑아서 mini-batch로 사용할 것이기 때문에 각 subgraph는 GPU에 통째로 올릴 수 있을 크기여야겠죠? . Mini-batch training | . 여러 subgraph 중 랜덤하게 뽑은 하나 (편의상 $G_c$라고 하겠습니다!) 를 mini-batch로 사용합니다. 이제 이 온전한 그래프에 대하여 full-batch training을 통해 노드 임베딩을 생성하면 됩니다. 이후로는 동일하게 downstream task에 걸맞는 mini-batch의 loss 값이 계산되고, 역전파를 통해 한번 GNN 모델의 파라미터가 업데이트 되겠네요. 계속 full-batch training의 계산상 효율성 때문에 Cluster-GCN이 제안되었다면서 교안에는 mini-batch training이라는 용어가 쓰여서 혹여나 헷갈리실까봐 덧붙여 설명드리겠습니다. 결국 subgraph를 mini-batch로 활용하여 학습을 진행하는 것이기 때문에 크게 보면 SGD, 즉 mini-batch training의 범주에 포함되는 것이 맞습니다. 다만, mini-batch 자체가 Neighbor Sampling에서 처럼 따로따로 구분된 computation graph로 구성되는 것이 아니고 하나의 온전한 그래프 모양을 갖는 subgraph이기 때문에, 한번에 노드 임베딩을 생성해버리는 full-batch training처럼 학습할 수 있는 것입니다! . Issues with Cluster-GCN . . 하지만 Cluster-GCN에도 문제는 있습니다. 그래프 community detection은 비슷한 노드들을 같은 커뮤니티에 집어넣기 때문에 추출되는 subgraph들은 원본 그래프를 대표한다고 볼 수 없을 뿐더러 커뮤니티 사이의 엣지들이 소실되며 중요한 메세지들을 잃을 수 있습니다. 13강의 Granovetter의 실험을 되짚어보면 커뮤니티 사이의 연결로부터 직업 소개와 같은 중요한 정보들이 많이 전달된다는 거 기억나시죠? 중요한 엣지들이 소실됨에 따라 subgraph 마다 마다 계산된 gradient 값의 편차가 심할 것이고, 이는 학습의 불안정성을 야기하며 수렴 속도를 더디게 합니다. . Advanced Cluster-GCN: Overview . . 이러한 문제는 여러 커뮤니티 (subgraph)를 하나의 mini-batch에 같이 넣음으로써 해결할 수 있습니다. 이런 방식으로 mini-batch를 구성한다면, subgraph 내에 로컬 커뮤니티 구조도 보존하면서 동시에 커뮤니티 간 엣지도 포함하게 되면서 보다 원본 그래프를 잘 대표할 수 있는 subgraph를 만들 수 있습니다. 다만 community detection시에 커뮤니티 사이즈를 좀 더 작게 함으로 여러 커뮤니티를 합쳐서 subgraph를 만들 때에도 subgraph가 GPU에 잘 올라가도록 해야 함에 주의하세요. . 💡 여러 개의 커뮤니티 구조를 합쳐서 subgraph로 사용하자! . Advanced Cluster-GCN . Advanced Cluster-GCN의 두 단계 또한 subgraph를 생성하는 부분 이외에는 vanilla Cluster-GCN과 동일하기 때문에 간략하게만 언급하고 넘어가도록 하겠습니다. . Pre-processing . Community detection 알고리즘을 활용하여 원본 그래프를 여러 커뮤니티로 분리합니다. 이 때, 커뮤니티 사이즈를 작게 하여 detection을 진행하도록 합니다. . | Mini-batch training . 분리된 여러 커뮤니티 중 몇 개를 랜덤하게 골라 aggregate 함으로써 subgraph, 즉 mini-batch를 구성할 것입니다. 이 때, 커뮤니티 사이의 엣지 또한 보존됨에 주목하세요! . | Comparison of Time Complexity . 자, 그럼 지금까지 큰 그래프를 GNN으로 학습하기 위해 배운 두 가지 방법, Neighbor Sampling과 Cluster-GCN을 시간 복잡도 측면에서 비교해보도록 하겠습니다. 두 방법에서 모두 머신러닝 모델은 $K$개의 GNN layer을 가지며, mini-batch의 사이즈는 $M$이라고 가정하겠습니다. . Neighbor Sampling (Sampling factor = $H$일 때) . . 이 방법의 경우 한 mini-batch가 $M$개의 computation graph들로 구성되며, 각 computation graph에서 한 노드 당 $H$개의 이웃 노드만이 샘플링되며 $H^K$의 크기를 갖게 됩니다. 따라서 전체 시간 복잡도는 $M cdot H^K$ 입니다. . 시간 복잡도 = $O(M cdot H^K)$ . | Cluster-GCN . 한 mini-batch에 대해 계산되는 시간 복잡도는 subgraph 내 엣지의 수에 비례합니다. 만약 전체 그래프에 대한 평균 차수가 $D_{avg}$이라면, mini-batch로 사용되는 subgraph 내의 노드 갯수가 $M$개이기 때문에 전체 엣지 수는 $M cdot D_{avg}$가 됩니다. 따라서 전체 $K$ layer에 대한 시간 복잡도는 $K cdot M cdot D_{avg}$ 입니다. . 시간 복잡도 = $O(K cdot M cdot D_{avg})$ . | . 일반적으로 시간 복잡도가 GNN layer의 갯수 $K$에 지수적으로 비례하는 Neighbor Sampling에 비해 선형 비례하는 Cluster-GCN이 계산 측면으로는 더 효율적이라고 합니다. 다만, 만약 $K$를 작게 두는 경우에 보통 현업에서는 Neighbor Sampling 방식이 더 우세하다고 하는데요, 이는 Cluster-GCN 방법이 Neighbor Sampling 방식에 비해 원본 그래프를 잘 대표할 수 있는 mini-batch를 구성하지 못하기 때문이라고 합니다. 어쨌거나 Cluster-GCN 방법은 community detection을 통해 추출된 커뮤니티 구조에 크게 의존하고 있으니까요 🙂 . Summary: Cluster-GCN . 💡 1. GNN 구조 : 그대로 사용2. Mini-batch 구성 방식: 커뮤니티 구조를 포함하는 subgraph로 구성3. 학습 방식: SGD training . Cluster-GCN은 먼저 community detection을 통해 원본 그래프를 여러 커뮤니티로 분리함 | 여러 커뮤니티가 합쳐진 subgraph가 mini-batch로 사용됨 | 온전한 그래프 모양을 띠는 mini-batch에 대해 full-batch training이 수행됨 | $K$가 클 때 Cluster-GCN은 Neighbor Sampling에 비해 더 효율적임 | 하지만 일반적으로 Cluster-GCN은 편향된 gradient를 만들게 되는데, $K$가 커진다 하더라도 동일 구조의 subgraph에 대해서만 aggregation이 일어나므로 진짜 원본 그래프를 $K$-hop만큼 explore하는 효과를 낼 수 없기 때문임 | . .",
            "url": "https://cs224w-kor.github.io/blog/gnn/scalable/large%20graph/graphsage/cluster%20gcn/simplified%20gcn/2022/09/07/lecture-1703.html",
            "relUrl": "/gnn/scalable/large%20graph/graphsage/cluster%20gcn/simplified%20gcn/2022/09/07/lecture-1703.html",
            "date": " • Sep 7, 2022"
        }
        
    
  
    
        ,"post3": {
            "title": "Lecture 17.2 - GraphSAGE Neighbor Sampling",
            "content": ". Lecture 17. Scaling Up Graph Neural Networks to Large Graphs . Lecture 17.1 - Scaling up Graph Neural Networks | Lecture 17.2 - GraphSAGE Neighbor Sampling | Lecture 17.3 - Cluster GCN: Scaling up GNNs | Lecture 17.4 - Scaling up by Simplifying GNNs | . . Recall: Computational Graph . . GNN이 노드 임베딩을 어떻게 만드는지 다시 떠올려 볼까요? 타겟 노드가 0번 노드라고 가정하면, 0번 노드를 기점으로 하여 computation graph가 생성됩니다. 오른쪽 그림은 타겟 노드로부터 2-hop 떨어진 이웃 노드까지 aggregate하여 타겟 노드의 임베딩을 생성하는데요, 이는 2개의 GNN layer로 표현됩니다. 이를 일반화하면, $K$-layer의 GNN은 결국 $K$-hop만큼 떨어진 이웃 노드까지 본다는 얘기가 되겠죠. 다시 말해, 타겟 노드의 임베딩을 업데이트 하기 위해 $K$-hop까지 떨어진 이웃 노드를 제외한 나머지 모든 노드들은 신경 쓰지 않아도 된다는 말입니다. . Computing Node Embeddings . 위에서 얻은 인사이트를 바탕으로 생각해보면, 결국 아래 왼쪽 그림처럼 mini-batch를 노드 단위로 구성해서 이웃 노드 정보가 다 끊기도록 만들기 보다, 오른쪽 그림과 같이 mini-batch를 computation graph 단위로 구성한다면 노드 간 연결된 모양새를 유지하며 성공적으로 SGD를 수행할 수 있으리라 기대할 수 있습니다. 다시 말해, $M$개의 랜덤 샘플링된 노드 자체 대신, 그 노드들로부터 뻗어나가는 $M$개의 computation graph로 mini-batch를 구성한다는 것입니다. 물론 전체 그래프 말고 mini-batch를 사용하기 때문에 GPU에 한꺼번에 올려 계산하는 것이 가능한 선에서 $M$을 설정해야겠죠! Mini-batch를 적당하게 만들었다면, 이후에 진행될 SGD 학습은 동일한 방식으로 일어납니다. . . 💡 Computation graph로 mini-batch를 구성하자! . Issue with Stochastic Training . 하지만 computation graph로 mini-batch를 만드는 데는 여러 문제가 있습니다. . Computation graph의 크기는 GNN layer 수 $K$에 대해 지수적으로 증가함 | Hub 노드가 존재하면 computation graph의 크기가 폭증함 여기서 Hub 노드란 SNS 그래프의 연예인 노드와 같이 차수가 높은 노드를 일컫습니다. 만약 이웃 중 하나라도 이런 hub 노드가 포함된다면 이에 연결된 엄청 많은 이웃 노드들까지 고려해야 하기 때문에 computation graph의 사이즈가 폭증합니다. | 따라서 단순하게 computation graph를 그대로 갖다 mini-batch에 넣어버리기 보다, computation graph 자체를 더욱더 작게 만들어줄 필요가 생겼습니다! . Neighbor Sampling . . 따라서 우리는 하나의 뻗어나간 가지에서 최대로 랜덤하게 $H$개의 이웃 노드만을 내버려 두고, 나머지 노드들은 버리는 방법을 취합니다. 만약 $H=2$로 둔다면, 위 그림처럼 한 노드의 이웃 노드를 갖고 computation graph를 그릴때, 딱 2개의 이웃 노드들만 사용하는 것이죠. 이렇게 가지치기된 (pruned) computation graph는 감당 가능한 사이즈로 유지되기 때문에, 이를 활용해서 mini-batch를 구성한다면 더욱 효율적으로 큰 그래프를 SGD로 학습할 수 있습니다. . 💡 최대 $H$개의 랜덤한 이웃 노드만 사용하여 computation graph를 생성하자! . 따라서 결국에 $K$ layer의 GNN은 매 hop마다 $H$개의 이웃 노드로 뻗어나가는 computation graph를 생성할 것이기 때문에 computation graph 내에 총 $ prod_{k=1}^K H_k$ 개의 leaf 노드를 가지게 될 것입니다. 즉, 여전히 $K$에 대해 지수적으로 computation graph의 크기가 증가하긴 하지만, fanout에 상한계 (upper bound)가 생김으로써 너무 커지지 않게 조절할 수 있게 된다는 말입니다. . Remarks on Neighbor Sampling . 논점 1 . $H$와 학습 안정성의 trade-off 관계 . 작은 $H$는 computation graph의 크기를 작게 만들어 효율적이고 빠른 학습이 가능하게 합니다 ↔ 하지만 한 노드를 업데이트 하기 위해 너무 일부의 이웃 노드만 활용하기 때문에 aggregation 단계의 variance를 크게 하고, 결국 불안정한 학습을 야기합니다. | . 논점 2 . 학습 과정의 시간 복잡도 . 앞에서 언급했듯이 Neighbor Sampling을 하더라도 computation graph의 크기는 여전히 $K$에 대해 지수적으로 증가합니다. | . 논점 3 . 어떤 이웃 노드들을 샘플링 할 것인지? . 랜덤하게 $H$개의 이웃 노드를 샘플링하는 방식은 매우 빠르지만, 대다수의 중요치 않은 (차수가 낮은) 노드들이 선택될 가능성이 높기 때문에 최적의 방식은 아닙니다. | Random Walk with Restarts 방식을 활용하여 중요한 노드들을 위주로 샘플링 해봅시다. . . 초록색 노드가 타겟 노드라고 할 때, 초록색 노드의 이웃 노드들 중 $H=3$개의 노드만 샘플링 해봅시다 (이해의 편의를 위해 1-hop만 고려하겠습니다~) | 먼저 초록색 노드로부터 시작하는 Random Walk를 수행한 후, 각 이웃 노드들이 random walk상 방문될 score $R_i$를 계산합니다. | 총 9개의 이웃 노드 중, 가장 큰 $R_i$를 갖는 3개의 파란색 노드만 샘플링하여 computation graph를 구성합니다. | 보통 차수가 높은 중요한 노드들이 random walk상 등장할 가능성이 더 높기 때문에 이 방법으로 더 중요한 이웃 노드들을 성공적으로 샘플링 할 수 있습니다! | . | 이웃 노드들을 샘플링하는 방식들은 여전히 연구가 진행되고 있는 주제라고 하네요~ | . Summary: Neighbor Sampling . 💡 1. GNN 구조 : 그대로 사용2. Mini-batch 구성 방식: $M$개의 Computation graph로 구성3. 학습 방식: SGD training . $M$개의 노드에 대해 computation graph를 생성하여 mini-batch에 넣음 | Neighbor Sampling 단계에서 computation graph가 가지치기되어 계산상의 효율성을 높임 (일종의 dropout처럼 생각할 수 있음!) | 하지만, GNN layer의 수가 많다면 여전히 computation graph의 크기가 커지기 때문에 가지치기할 노드 수 $H$ 또는 배치 사이즈를 줄여야 함. 이는 결국 불안정한 학습으로 이어지게 됨 | Neighbor Sampling 방법은 현업에서 많이 사용됨 | . .",
            "url": "https://cs224w-kor.github.io/blog/gnn/scalable/large%20graph/graphsage/cluster%20gcn/simplified%20gcn/2022/09/07/lecture-1702.html",
            "relUrl": "/gnn/scalable/large%20graph/graphsage/cluster%20gcn/simplified%20gcn/2022/09/07/lecture-1702.html",
            "date": " • Sep 7, 2022"
        }
        
    
  
    
        ,"post4": {
            "title": "Lecture 17.1 - Scaling up Graph Neural Networks",
            "content": ". Lecture 17. Scaling Up Graph Neural Networks to Large Graphs . Lecture 17.1 - Scaling up Graph Neural Networks | Lecture 17.2 - GraphSAGE Neighbor Sampling | Lecture 17.3 - Cluster GCN: Scaling up GNNs | Lecture 17.4 - Scaling up by Simplifying GNNs | . . Graphs in Modern Applications . 오늘날 그래프는 어떠한 도메인에서 널리 활용될까요? 예를 들어 살펴보겠습니다. . 추천 시스템 (Recommender systems) | . 아마존, 유튜브, 핀터레스트와 같은 기업들에서는 user-item의 이분 그래프 (bipartite graph)를 활용하여 유저들에게 아이템을 추천하거나 (link prediction), 유저 또는 아이템을 분류하는 (node classification) 서비스를 제공하고 있습니다. 보통 대기업 서비스를 이용하는 유저가 매우 많고 판매하는 아이템 또한 굉장히 다양하기 때문에 활용되는 이분 그래프의 크기는 매우 큽니다. . 소셜 네트워크 (Social networks) | . 페이스북, 트위터, 인스타그램과 같은 기업들에서는 사용자 그래프를 활용하여 친구를 추천하거나 (link-level), 성별과 같은 사용자의 특징을 파악하는 (nodel-level) 서비스를 제공하고 있습니다. 대형 SNS를 활용하는 사용자가 매우 많기 때문에 해당 기업들이 다루어야 하는 그래프의 크기 또한 매우 큽니다. . 학술 그래프 (Academic graph) | . 마이크로소프트 학술 그래프는 약 120M 편의 논문과 120M 명의 저자로 구성되어 있는 매우 큰 그래프로써, 이를 활용하여 논문 분류 (node classification), 협력 연구자 추천 및 논문 인용 추천 (link prediction) 등과 같은 서비스를 제공합니다. . 지식 그래프 (Knowledge Graphs (KGs)) | . 위키피디아, 프리베이스 등에서 제공하는 지식 그래프를 활용하여 KG completion이나 reasoning과 같은 머신러닝 태스크를 수행합니다. 이 때, 지식 그래프에 포함된 entity의 갯수는 약 80M-90M로 또한 매우 큰 그래프를 다루게 됩니다. . What is in Common? . 앞서 설명하였듯이 그래프는 다양한 도메인에서 널리 활용되고 있습니다. 여기서, 도메인에 무관하게 활용되는 그래프는 아래와 같은 공통점을 갖습니다. . 매우 큰 스케일 노드의 수는 대략 10M ~ 10B 입니다. | 엣지의 수는 대략 100M ~ 100B 입니다. | . | 풀고자 하는 태스크 Node-level : 유저/아이템/논문 분류 | Edge-level : 추천, completion | . | . 우리는 오늘 포스트에서 이러한 큰 그래프에 적용할 수 있도록 GNN을 scalable하게 만드는 방법들에 대해서 배워보겠습니다! . Why is it Hard? . 일반적으로 우리는 머신러닝 모델을 학습하기 위해서 전체 데이터 포인트에 대한 loss 함수를 최소화하는 방향으로 파라미터를 업데이트 해 나갑니다. 만약 데이터셋에 총 $N$개의 데이터 포인트가 있다면 수식은 아래와 같습니다. . . 하지만 만약 $N$이 굉장히 크다면, 모든 데이터 포인트에 대해 loss 값을 계산하는 데 시간이 너무 많이 들기 때문에 우리는 full-batch training 대신 mini-batch를 활용하는 Stochastic Gradient Descent (SGD)를 수행하게 됩니다. 6강에서 SGD를 자세히 설명하였으니 자세한 설명은 넘어가도록 하겠습니다. 간략하게 말해서, $M( ll N)$ 개의 데이터 포인트를 반복적으로 랜덤하게 샘플링하여 추출된 mini-batch에 대해서 gradient descent를 각각 수행하는 방법입니다. SGD는 full-batch training에 비해 빠르게 수렴하며, 계산상의 효율성을 가집니다. . 그렇다면 효율적인 SGD를 GNN에 적용해볼까요? | . Mini-batch를 구성할 때, $M$개의 노드가 랜덤하게 샘플링되기 때문에 위 그림과 같이 각 샘플링된 노드들은 서로 연결되지 않고 고립되어 있을 가능성이 큽니다. GNN이 이웃 노드의 feature을 aggregate하여 타겟 노드의 feature을 업데이트 한다는 것을 생각한다면 고립된 노드로는 올바르게 노드 feature을 업데이트 할 수 없게 됩니다. 따라서 SGD를 단순히 GNN에 적용하는 것은 좋은 생각이 아닙니다. . 그렇다면 그냥 full-batch training 방식으로 GNN을 학습시켜 볼까요? | . Full-batch training 방식으로 GNN을 학습시키려면, 먼저 전체 그래프와 초기 노드 feature를 로드한 후, 매 GNN layer 마다 이전 layer에서 얻은 노드 임베딩을 활용하여 전체 노드 임베딩을 한꺼번에 업데이트 해야 합니다. 따라서, 매 GNN layer 마다의 전체 노드 임베딩을 모두 GPU 메모리에 올려야 위와 같은 계산이 가능하게 됩니다. 하지만 GPU는 빠른 연산 속도를 갖는 반면 수용할 수 있는 메모리의 양이 10GB ~ 20GB으로 굉장히 작기 때문에 full-batch training으로 GNN을 학습할 수 없습니다. . SGD도 안된다, full-batch training 학습 방식도 안된다. 그럼 대체 큰 그래프를 어떻게 GNN으로 학습할 수 있을까요? 오늘 포스트에서 이를 다뤄보겠습니다. . Today’s Lecture . 오늘 다룰 내용을 간단하게 요약하자면, GNN을 scalable하게 만드는 세 가지 방법을 차례로 소개할 것입니다. . Mini-batch를 작은 subgraph로 두고 message passing을 수행하는 방법 Neighbor Sampling [Hamilton et al. NeurIPS 2017] | Cluster-GCN [Chiang et al. KDD 2019] | . → 매번 mini-batch로 샘플링된 subgraph만 GPU에 올려서 loss를 계산해주면 됩니다. . → 일반적인 GNN 구조 그대로 사용 &amp; Mini-batch를 만드는 다양한 방식 &amp; SGD training . | GNN 구조 자체를 단순화 시키는 방법 Simplified GCN [Wu et al. ICML 2019] | . → 그냥 CPU만 써서도 효율적으로 계산할 수 있습니다. . → GNN 구조를 변경 &amp; Mini-batch는 원래대로 랜덤하게 추출 &amp; SGD training . | . .",
            "url": "https://cs224w-kor.github.io/blog/gnn/scalable/large%20graph/graphsage/cluster%20gcn/simplified%20gcn/2022/09/07/lecture-1701.html",
            "relUrl": "/gnn/scalable/large%20graph/graphsage/cluster%20gcn/simplified%20gcn/2022/09/07/lecture-1701.html",
            "date": " • Sep 7, 2022"
        }
        
    
  
    
        ,"post5": {
            "title": "Lecture 13.4 - Detecting Overlapping Communities",
            "content": ". Lecture 13. Community Detection in Networks . Lecture 13.1 - Community Detection in Networks | Lecture 13.2 - Network Communities | Lecture 13.3 - Louvain Algorithm | Lecture 13.4 - Detecting Overlapping Communities | . . Overlapping Communities . . . 여기까지 잘 따라오셨나요? 여지껏 우리는 위 그림의 제일 왼쪽 그래프에서 보시는 것과 같이 서로 겹치지 않는 커뮤니티에 대해서만 다뤄왔습니다. 하지만, 실생활의 네트워크의 커뮤니티는 서로 겹칠 가능성이 훨씬 높기 때문에 이런 서로 겹치는 커뮤니티를 detect하는 방법도 공부해야 합니다. 서로 겹치는 커뮤니티는 그래프를 인접 행렬로 표현하면 시각적으로 더욱 명확하게 확인할 수 있는데요, 상단 오른쪽 그림과 같이 인접 행렬의 밀집된 부위 일부가 겹치는 것을 볼 수 있습니다. 들어가기에 앞서 먼저 실제 네트워크 데이터의 커뮤니티 구조를 직접 확인해보는 시간을 갖도록 하겠습니다. . . &lt; SNS 소셜 네트워크의 커뮤니티 구조 &gt; . . &lt; 단백질 상호 작용 네트워크의 커뮤니티 구조 &gt; . Plan of Action . 이렇게 서로 겹치는 커뮤니티 구조 또한 detect하기 위해 어떻게 할 것인지 전체적인 계획을 간단히 설명한 후, 각 부분의 세부 사항으로 넘어가겠습니다. . Step 1 여러 노드가 각각 어떤 커뮤니티에 소속되어 있는지 나타내는 node community affiliation을 가지고 전체 그래프를 생성하는 생성 모델인 Community Affiliation Graph Model(AGM)을 정의합니다. 쉽게 말해 노드 집합 $V$에 소속된 노드 간에 엣지를 생성하는 모델이라고 생각하면 됩니다. | . 💡 AGM: Community Affiliation → Network . | Step 2 반대로 그래프 $G$가 주어졌을 때 어떤 AGM 모델이 어떤 인풋 community affiliation을 가지고 그래프 $G$를 생성했는지 역으로 찾아나갑니다. | $G$를 생성했을 법한 가장 그럴싸한 AGM 모델을 찾음으로써 각 노드의 커뮤니티 소속 정보를 알 수 있게 됩니다. | . | AGM: Community-Affiliation Graph Model . . 모델 파라미터 $F = (V,C,M,{p_c})$ $V$: 노드 집합 | $C$: 커뮤니티 집합 | $M$: 소속 여부 | $p_c$: 각 커뮤니티마다 갖고 있는 하나의 확률 값으로, 동일 커뮤니티 내에 속한 노드 사이에 엣지가 형성될 확률을 의미함 | . | 네트워크 생성 과정 동일 커뮤니티 $c$ 내에 있는 노드 사이에 $p_c$의 확률로 엣지를 형성합니다. | 만약 두 노드가 여러 커뮤니티에 동시에 소속되어 있다면(”overlapping community”), 적어도 하나의 커뮤니티로부터 $p_c$가 충족되어 엣지가 형성되었으면 두 노드는 연결되었다고 봅니다. → 즉, 여러 커뮤니티에 동시에 소속되어 있는 노드들은 그만큼 엣지로 연결된 가능성이 높다는 말이지요! 🤩 | 따라서, 노드 $u$와 $v$ 사이에 엣지가 형성될 확률은 $p(u,v) = 1- prod_{c in M_u cap M_v} (1-p_c)$로 표현할 수 있습니다. AGM은 이러한 방식으로 non-overlapping, overlapping, hierarchical(nesting) 등 다양한 커뮤니티 구조를 다룰 수 있다고 합니다! . | . . 💡 만약 노드 $u$와 $v$가 서로 공통으로 소속된 커뮤니티가 없다면, 두 노드는 절대 엣지로 연결될 수 없을 것입니다. 이러한 문제를 해결하기 위해 $p_{background}= epsilon$를 갖는 background community를 정의합니다. background community에는 모든 노드가 소속되어 있으므로, 노드 $u$와 $v$도 $ epsilon$의 확률로 연결될 수 있겠죠? . Graph Fitting . . 이제 우리는 반대로 네트워크 구조가 주어졌다고 했을 때 이를 생성했을 법한 AGM 모델 $F$를 구해볼 것입니다. 즉 그래프 $G$를 알고 있는 조건에서, . Community Affiliation 그래프 $M$ | 커뮤니티의 집합 $C$ | 각 커뮤니티에 대응되는 엣지 생성 확률 $p_c$ | . 를 찾아야 합니다. . . 가장 그럴싸한 AGM 모델 $F$는 Maximum Likelihood Estimation으로 구할 수 있는데요, 수식은 위 그림과 같이 적을 수 있습니다. 즉, 모델 $F$로 부터 생성된 합성 그래프가 실제 그래프 $G$와 유사할 확률을 최대화하는 모델 파라미터 $F$를 구하는 것이 목적이라고 해석해 볼 수 있겠죠. 위 수식을 풀기 위해 우리는 다음 두 가지를 정의해야 합니다. . **$P(G | F)$를 전개하여 목적 함수(Objective function)를 수학적으로 정리해야 합니다!** | . | 1단계에서 만들어진 목적 함수를 모델 파라미터 $F$에 대해 최대화하기 위해 최적화 기법을 선택해야 합니다! (예. Gradient ascent) | Graph Likelihood $P(G|F)$ . . 만약 모델 $F$와 결과 그래프 $G$가 각각 위의 그림과 같이 주어졌다고 가정해봅시다. 여기서 $F$ 옆의 행렬은 community affiliation 그래프로부터 각 노드 쌍에 대해 엣지가 생성될 확률을 구한 것이고, $G$는 알고 있는 그래프 구조를 활용하여 인접 행렬을 구한 것입니다. 이 경우 $P(G | F)$는 그림 아래의 식과 같이 전개할 수 있습니다. 즉, **그래프 상 존재하는 엣지들에 대해서는 엣지가 생성될 확률을, 존재하지 않는 엣지들에 대해서는 엣지가 생성되지 않을 확률을 각각 구한 후 모두 곱함으로써 $P(G | F)$를 구할 수 있습니다.** | . 💡 $P(G | F)= prod_{(u,v) in G} P(u,v) prod_{(u,v) notin G} 1-P(u,v)$ | . 하지만 문제가 있습니다! 🙄 우리는 현재 community affiliation 그래프를 모르는 상황입니다. 즉, 어떤 노드가 어떤 커뮤니티에 소속되어 있는지, 또 커뮤니티마다 갖는 $p_c$의 확률이 무엇인지 전혀 알 수 없습니다. 따라서 우리는 $p(u,v)$를 위와 같이 구할 수 없게 되는데요, 그렇기 때문에 AGM의 조건을 “Relaxing”하여 $p(u,v)$를 구해보도록 하겠습니다. . . 일단 노드의 커뮤니티 소속(membership) 여부를 몰라도 되도록, 기본적으로 모든 노드가 각각 모든 커뮤니티에 소속되어 있고, 각 membership에는 0 이상의 강도가 부여된다고 가정합시다. 예를 들어 $F_{uA}$는 노드 $u$가 커뮤니티 $A$에 소속되는 강도가 됩니다. 위 그림에서 노드 $v$는 커뮤니티 $A$의 소속이 아닌 것처럼 그려졌는데, 사실상 소속되어 있지만 강도가 0, 즉 $F_{vA}=0$으로 해석하는 것이 올바릅니다. . 이 경우 기존에 AGM 모델에서 사용하던 notation $p_c$는 $p_c(u,v)=1-exp(-F_{uC} cdot F_{vC})$ 로 바꿔 쓸 수 있습니다. $F$값이 항상 0보다 크거나 같기 때문에 $0 le p_c(u,v) le 1$의 조건이 항상 만족되며, 아래와 같이 해석할 수 있습니다. . . 두 노드 $u$와 $v$가 각각 커뮤니티 $C$에 높은 강도로 소속되어 있다면 → $p_c(u,v)$ 값이 큼 (1에 가까워짐) | 두 노드 $u$와 $v$중 하나 이하만이 커뮤니티 $C$에 소속되어 있다면 → $p_c(u,v)=0$로 두 노드는 서로 연결되지 않음 | . 존재하는 모든 커뮤니티의 집합을 $ Gamma$라고 하였을 때, 기존에 $p(u,v) = 1- prod_{c in M_u cap M_v} (1-p_c)$로 표현되던 식은 비슷하게 $p(u,v)=1- prod_{C in Gamma} (1-P_c(u,v))$로 바꾸어 쓸 수 있습니다. . 💡 $p_c(u,v)=1-exp(-F_{uC} cdot F_{vC})$ ····· (1) $p(u,v)=1- prod_{C in Gamma} (1-P_c(u,v))$ ····· (2) . 자, 이제 다왔습니다. 위 (2)식에 (1)식을 대입하여 최종적으로 식을 전개하면 최종적으로 아래와 같은 수식을 얻을 수 있습니다. . . BigCLAM Model (1) — Objective Function . 📢 $P(G|F)= prod_{(u,v) in G} P(u,v) prod_{(u,v) notin G} 1-P(u,v)$ $p(u,v)=1-exp(-F_u^TF_v)$ . 앞에서 다룬 두 수식을 이용하여 최종적으로 $P(G | F)$ 수식을 전개해보도록 하겠습니다. | . . 사실 $1-exp(-F_u^TF_v)$와 $exp(-F_u^TF_v)$로 표현되는 likelihood 값은 0과 1 사이의 작은 값으로 서로 연속해서 곱하다보면 굉장히 작은 수가 되는 등 수치적으로 불안정한 값을 만들어 냅니다. 따라서 우리는 로그 함수를 도입하여 곱을 합으로 바꿈으로써 이 문제를 해결할 수 있습니다. . . **최종적으로 우리의 목적 함수는 $log(P(G | F))$로 우리는 이를 최대화 함으로써 그래프 $G$를 만들었을 법한 AGM 모델 $F$의 파라미터를 추정할 수 있게 됩니다.** 앞서 다룬 목적 함수를 최대화함으로써 overlapping 커뮤니티를 detect하는 이 모델은 BigCLAM이라고 합니다. | . 💡 $l(F) = sum_{(u,v) in E} log(1-exp(-F_u^TF_v))- sum_{(u,v) notin E} F_u^TF_v$ . BigCLAM Model (2) — Optimization . 지금까지 BigCLAM의 목적 함수를 알아보았습니다. 그렇다면 이 목적 함수를 최적화(최대화)하는 방법은 무엇일까요? 여느 최적화 과정과 다를 바 없이 목적 함수 $l(F)$의 최적화는 아래 단계를 반복하며 일어납니다. . 랜덤한 소속 강도 $F$로 초기화 합니다. | 모든 노드 $u$에 대해 각각, 목적 함수의 gradient을 갖고 Gradient Ascent를 진행합니다. 이 말인즉슨, $F_u$ 값이 커지는 방향으로 이동한다는 것으로 생각할 수 있습니다. 노드 $u$의 소속 강도 $F_u$에 대해 업데이트를 진행할 때, 다른 모든 노드의 $F$값은 고정해 놓음에 주의하십시오! | | 상기 과정을 수렴 시까지 반복합니다. | . . 이 때, 위에서 정의한 목적 함수에 대한 gradient는 위 식과 같지만, 이를 바로 사용하여 최적화를 진행하기에는 무리가 있습니다. 전체 그래프 상 노드 $u$의 이웃 노드는 한정적인 반면 이웃 노드가 아닌 노드에 대해서 뒷 term을 계속 반복하여 계산하는 것은 연산 부담이 매우 크기 때문입니다. . . 여기서 살짝 트릭을 사용하여, 뒷 term을 위와 같이 바꿔줍니다. 이렇게 바꿀 경우, 모든 노드에 대해 계산하는 첫 번째 term은 처음에 한번만 계산해 놓으면 계속해서 재사용 할 수 있기 때문에 효율적이며, 뒤따르는 두,세 번째 term은 노드 $u$와 노드 $u$의 이웃 노드들에 대해서만 계산하면 되기 때문에 훨씬 빨리 계산할 수 있습니다. 이렇게 변형하여 gradient ascent를 진행하면 한번의 가중치 업데이트에 노드 $u$의 차수에 선형 비례하는 시간 복잡도를 가지게 됩니다. . BigCLAM: Summary . 마지막으로 지금까지 다룬 BigCLAM 모델을 정리하고 오늘 포스트를 마무리 하도록 하겠습니다! 수고하셨습니다~ 🤗 . . BigCLAM은 overlapping 커뮤니티 구조를 파악하기 위한 모델($F$)을 우선 정의합니다. | 어떤 그래프 $G$가 주어졌을 때, 모델 $F$로 $G$를 생성할 log likelihood를 최대화함으로써 각 노드가 각 커뮤니티에 소속될 강도 ($F_u$)를 추정할 수 있습니다. | . .",
            "url": "https://cs224w-kor.github.io/blog/gnn/community/community%20detection/louvain/bigclam/2022/08/24/lecture-1304.html",
            "relUrl": "/gnn/community/community%20detection/louvain/bigclam/2022/08/24/lecture-1304.html",
            "date": " • Aug 24, 2022"
        }
        
    
  
    
        ,"post6": {
            "title": "Lecture 13.3 - Louvain Algorithm",
            "content": ". Lecture 13. Community Detection in Networks . Lecture 13.1 - Community Detection in Networks | Lecture 13.2 - Network Communities | Lecture 13.3 - Louvain Algorithm | Lecture 13.4 - Detecting Overlapping Communities | . . 지금까지 community detection의 objective이 modularity의 최대화라는 것을 배웠습니다. 하지만 아직 HOW라는 중요한 문제가 빠져있죠. 13.3 파트에서는 바로 이 HOW, 즉 어떻게 modularity를 최대화 할 것인지에 대해 알아보도록 하겠습니다. . Louvain Algorithm: At High Level . Louvain 알고리즘은 community detection을 위한 탐욕적 알고리즘으로 잘 알려져 있습니다. 간단하게 Louvain 알고리즘의 특징을 짚고 넘어가도록 하겠습니다. . 낮은 시간 복잡도 &amp; 빠른 수렴: 시간 복잡도가 $O(nlogn)$로써 굉장히 빠르기 때문에 큰 그래프에도 쉽게 적용할 수 있습니다. | 높은 modularity를 갖는 output: Community detection을 ‘잘’합니다! | 계층적 커뮤니티 구조를 파악할 수 있습니다. . . | . Louvain 알고리즘은 반복적인 pass를 통해 탐욕적으로 modularity를 최대화합니다. 각 pass는 2 phase로 구성되어 있는데, 각 phase는 다음과 같이 작동합니다. . Phase 1 - Partitioning 노드-커뮤니티 소속을 국소적으로 바꾸어 modularity 값의 변화를 살핌으로써 최종적으로 modularity가 극대화되는 방향으로 노드-커뮤니티 소속을 적합시킴 | Phase 2 - Restructuring 현재 적합된 노드-커뮤니티 소속을 기반으로 동일 커뮤니티 속 노드를 super-node로 aggregate하여 새로운 상위 단계의 네트워크를 생성함 → 이를 통해 계층적 커뮤니티 구조 파악 가능 | 다시 Phase 1으로 돌아감 | Pass는 더 이상 modularity를 증가시킬 수 없을 때까지 반복적으로 일어남 | . . 원래 그래프가 unweighted 그래프였을지라도 phase 2에서 weighted 그래프가 생성되기 때문에 기본적으로 Louvain 알고리즘은 인풋 그래프를 weighted 그래프로 간주하고 진행됩니다. 하지만 앞에서 언급했듯이 unweighted 그래프는 그저 특별한 weighted 그래프의 인스턴스로 볼 수 있기 때문에 큰 문제가 되지 않습니다! . Louvain: 1st phase (Partitioning) . 이제 Louvain의 각 phase를 좀 더 심도 있게 다뤄볼까요? 첫번째 phase에서는 주어진 그래프에 대해 modularity의 local maxima를 찍도록 커뮤니티를 분리하기 때문에 Partitioning 단계라고 불립니다. Partitioning은 다음과 같은 방식으로 일어납니다. . 먼저 그래프 상 모든 노드를 각기 다른 커뮤니티에 소속시킴 (한 커뮤니티 당 하나의 노드가 소속된 형태) | 각 노드 $i$에 대해서, 노드 $i$를 이웃 노드 $j$의 커뮤니티로 소속 이동시켰을 때 modularity gain($ triangle Q$)를 구함 (이 또한 노드 $i$의 모든 이웃들에 대해서 다 구함) | 노드 $i$의 이웃 노드 중, 노드 $i$의 소속 변경 시 가장 큰 modularity gain이 생기는 노드 $j$의 커뮤니티로 $i$를 이동시킴 | | 사실 어떤 노드를 먼저 보냐에 따라서 알고리즘의 결과가 바뀔 수 있긴 하지만, 여러 문헌에 따르면 그 차이가 굉장히 미미하기 때문에 노드 순서는 크게 중요치 않다고 합니다. | . Louvain: Modularity Gain . 그렇다면 본래 커뮤니티 $D$에 소속되어 있던 노드 $i$를 커뮤니티 $C$로 옮길 때 발생하는 modularity gain은 어떻게 계산할 수 있을까요? 전 과정에 대한 modularity gain은 먼저 노드 $i$를 커뮤니티 $D$로부터 제거하고, 그 후 노드 $i$를 커뮤니티 $C$에 소속 시키는 두 단계의 modularity gain 합으로 구할 수 있습니다. 이를 각각 식과 그림으로 표현하면 아래와 같습니다. . . 사실상 $ triangle Q(D rightarrow i)$는 단순히 $ triangle Q(i rightarrow C)$의 역과정으로써 비슷한 방법으로 구할 수 있기 때문에 여기서는 $ triangle Q(i rightarrow C)$만 수식적으로 도출해보도록 하겠습니다. . Deriving $ triangle Q(i rightarrow C)$ . 도출에 앞서서 먼저 커뮤니티 $C$의 modularity를 구하기 위해 $ sum_{in}$과 $ sum_{tot}$를 정의하겠습니다. . . 💡 Recall: Modularity . 앞서 정의한 $ sum_{in}$과 $ sum_{tot}$를 바탕으로 커뮤니티 $C$의 modularity $Q(C)$를 구하면 아래와 같습니다. . . 이제 더 나아가 노드 $i$를 커뮤니티 $C$에 새로 소속 시키는 상황까지 고려하기 위해 $k_{i,in}$과 $k_i$를 추가로 정의하겠습니다. . . 최종적으로 $ triangle Q(i rightarrow C)$를 구하기 위해서는 노드 $i$가 커뮤니티 $C$에 소속되지 않았을 때의 modularity $Q_{before}$과 소속된 후의 modularity $Q_{after}$을 각각 따로 구하여 그 차이를 계산해야 합니다. 따라서 먼저 $Q_{before}$을 구해보도록 하겠습니다. 이 때 노드 $i$는 혼자 따로 떨어진 타 커뮤니티에 소속되어 있기 때문에 $Q_{before}$은 $Q(C)$와 $Q({i})$의 합으로 구할 수 있습니다. 이 때 노드 $i$만으로 구성된 커뮤니티 ${i}$는 내부의 엣지가 존재하지 않기 때문에 $Q$의 앞 term이 0이 됨에 주의하세요! . . 다음으로 $Q_{after}$을 구해보도록 하겠습니다. 확장된 커뮤니티 $C+{i}$에 대한 modularity는 아래 수식을 통해 구할 수 있습니다. . . 최종적으로 $ triangle Q(i rightarrow C)$는 $Q_{after}$과 $Q_{before}$의 차이로 구할 수 있습니다. 최종 수식은 다음과 같습니다. . . 💡 Louvain 1st Phase Summary1. 현재 커뮤니티 $C$에 속해있는 모든 노드 $i$에 대해 이웃 노드의 커뮤니티로 소속을 변경함에 따르는 modularity gain을 구한다.2. 소속 변경에 따르는 modularity gain이 가장 큰 community $C’$를 구한다.$C’ = argmax_C, triangle Q(C rightarrow i rightarrow C’)$3. 만약 $ triangle Q(C rightarrow i rightarrow C’)$이 0보다 크다면, 노드 $i$의 소속을 $C$에서 $C’$으로 옮긴다.$C leftarrow C - {i}$$C’ leftarrow C’ + {i}$* 모든 노드가 최적의 커뮤니티에 속해 있을 때까지 위 과정을 반복한다. . Louvain: 2nd phase (Restructuring) . . Phase 1이 끝난 후, 네트워크의 각 노드들은 특정 커뮤니티에 소속되게 됩니다. 이 후, phase 2에서는 동일 커뮤니티에 속한 노드끼리 aggregate 되어 super-node를 형성함으로써 더 축약된 형태의 상위 단계 네트워크가 생성됩니다. 각 커뮤니티에 속한 노드 간에 적어도 하나 이상의 엣지가 존재하는 경우 대응되는 super-node 사이에도 엣지가 형성되며, 축약된 엣지의 weight는 원본 그래프의 edge weight를 합산함으로써 구할 수 있다. Phase 2가 종료된 후, 비로소 1번의 pass가 종결되며 이어 다시 phase 1이 축약된 네트워크에 수행된다. . 💡 Q) 축약된 오른쪽 그래프에서 self-edge는 무엇인가요? A) 축약되기 이전 원본 그래프에서 각 커뮤니티에 속하는 노드 간 edge weight를 합산하여 전달하기 위해 축약 그래프에서 self-edge를 만들게 됩니다! . Louvain Algorithm 정리 . 다시 정리하는 느낌으로 지금까지 배운 Louvain 알고리즘을 하나의 도식으로 나타내면 아래 그림과 같습니다. . . Louvain 알고리즘은 탐욕적 탐색 방법으로써 시간 복잡도가 낮아 큰 그래프에도 쉽게 적용할 수 있으며, modularity를 효과적으로 최대화하기 때문에 community detection에서 우수한 성능을 보입니다. 실제 task에 적용해보면 Louvain 알고리즘의 우수성을 더욱 명확하게 알 수 있는데요, 프랑스어와 네덜란드어를 주로 사용하는 벨기에 인구에 대해 제공된 통화 네트워크를 Louvain 알고리즘으로 분석하면 사용 언어에 따라 정확하게 커뮤니티가 분리되는 것을 볼 수 있습니다. . . .",
            "url": "https://cs224w-kor.github.io/blog/gnn/community/community%20detection/louvain/bigclam/2022/08/24/lecture-1303.html",
            "relUrl": "/gnn/community/community%20detection/louvain/bigclam/2022/08/24/lecture-1303.html",
            "date": " • Aug 24, 2022"
        }
        
    
  
    
        ,"post7": {
            "title": "Lecture 13.2 - Network Communities",
            "content": ". Lecture 13. Community Detection in Networks . Lecture 13.1 - Community Detection in Networks | Lecture 13.2 - Network Communities | Lecture 13.3 - Louvain Algorithm | Lecture 13.4 - Detecting Overlapping Communities | . . 13.1 파트를 통해 우리는 네트워크에 서로 밀집되어 연결된 노드 커뮤니티가 존재한다는 사실을 알게 되었습니다. 그렇다면 그래프가 주어졌을 때, 이런 노드 커뮤니티를 어떻게 자동으로 찾을 수 있을까요? . . 💡 How do we automatically find such densely connected groups of nodes? . Examples of Network Communities . Zachary’s Karate Club Network . . GNN을 다뤄보신 분이라면 익숙할 Zachary’s Karate Club 네트워크에도 국소적인 고밀집 커뮤니티가 존재합니다. Zachary가 대학 내 태권도 동아리의 부원들을 대상으로 데이터를 수집하던 당시, 부원 사이에 갈등이 생겨 자연스레 두 무리로 나뉘었다고 합니다. 친구들이 다퉈서 무리가 나뉘는 경우를 생각해보면, 자연스럽게 그 중 더 끈끈한 친구들끼리 각 무리를 형성하게 됩니다. 이러한 현상이 뚜렷하게 나타난 위 그래프에서도 커뮤니티 분리는 엣지를 최소한으로 자르는 형태로 일어납니다. . | Micro-Markets in Sponsored Search . . Query-to-Advertiser 그래프를 분할함으로써 국소 마켓(커뮤니티)들을 찾을 수 있습니다. 위 그림에 표시된 파란색 박스는 gambling에 관심이 있는 인터넷 사용자들에게 광고하길 원하는 advertiser의 커뮤니티라고 볼 수 있겠군요. . | NCAA Football Network . . 위의 왼쪽 그래프는 미식축구 팀들과 팀 사이 경기를 나타낸 그래프입니다. 뚜렷한 고밀집 부위를 육안으로 판단하기 힘든 왼쪽 그래프에서도 노드 커뮤니티를 찾을 수 있을까요? 네, 가능합니다 😊 NCAA(전미 대학 체육 협회)의 division에 따라 그래프를 좀 정리해본다면 동일 division에 속한 팀들끼리 크게 밀집되어 있음을 쉽게 확인할 수 있습니다! . | Modularity-based Community Detection: Concept Overview . 하지만 앞에서 살펴본 커뮤니티 split 방법들은 너무 domain-specific하고 일반화하기 어렵습니다. 따라서 우리는 그래프가 주어졌을 때 자동으로 커뮤니티를 탐지하고 분리하는 방법을 알아보겠습니다. . 들어가기에 앞서 Modularity라는 새로운 metric을 정의하고 가겠습니다. . Modularity Q 네트워크가 커뮤니티로 얼마나 잘 분리되었는지 판단하는 측정치 (cf. 커뮤니티: 밀도 높게 연결된 노드의 집합을 의미함) . 우리가 주어진 네트워크를 서로 나뉜 subgraph $s$의 집합 $S$로 분리했다고 가정해봅시다($s in S$). 그렇다면 우리는 modularity를 구함으로써 우리가 주어진 그래프를 얼마나 잘 분리했는지 정량적으로 측정할 수 있습니다. Modularity Q를 구하는 식은 아래와 같습니다. . . 식을 그대로 해석해보자면 modularity는 각각의 subgraph $s$에 대해 subgraph 내부에 실제로 존재하는 엣지 수와 null model을 통해 예측된 엣지 수의 차이를 구한 후, $S$ 내의 모든 subgraph에 대해 이 값을 총합함으로써 구할 수 있습니다. 만약 우리가 네트워크를 기가 막히게 커뮤니티 별로 분리해냈다면, 분리된 subgraph $s$는 tightly-connected, 즉 내부 엣지의 수가 많을 것이기 때문에 modularity가 커질 것입니다. 따라서 커뮤니티를 잘 찾아낸다는 것은 곧 modularity가 크다는 의미이고, 그렇기 때문에 우리는 modularity를 최대화하는 방향으로 커뮤니티 분리를 진행할 것입니다. . Null Model: Configuration Model . 여기서 잠깐! Modularity를 정의할 때 subgraph $s$내에 몇 개의 엣지가 존재할 지 예측하는 부분에서 null model을 활용해야 한다고 했는데요. 여기서 나온 null model은 사실 지난 12강 subgraph mining 강의에서 다뤘던 내용이지만 이해를 위해 여기서도 간략히 설명하고 넘어가도록 하겠습니다. . Null model은 $n$개의 노드를 가지고, $m$개의 엣지로 이루어진 그래프 $G$를 노드 단위로 다 자르고 엣지 연결을 재배치한 그래프 $G’$를 만듭니다. 이 때 각 노드의 차수를 유지하기 위해 노드의 연결부들은 보존한채로 그래프 $G$를 아래 그림처럼 노드 단위로 분해합니다. 엣지 재배치는 단순하게 하나의 연결부를 여러 가능한 연결부 중 랜덤하게 접합함으로써 이루어집니다. 이 때 계산상의 편의성을 위해 $G’$는 두 노드 사이에 여러 엣지가 존재할 수 있는 multigraph로 간주합니다. . . 이제 엣지 재배치를 좀 더 심도있게 살펴보겠습니다. 위 그림에서 노드 $i$와 노드$j$사이에 몇 개의 엣지가 생성될지는 $k_i cdot frac{k_j}{2m} = frac{k_ik_j}{2m}$을 통해 구할 수 있습니다. 노드 $i$에서 뻗어 나가는 연결부 $k_i$에 대해서 각 연결부가 노드 $j$로 접합될 확률은 전체 연결부 $2m$개 중에 $k_j$개의 연결부로 도달할 때 뿐이기 때문에 $ frac{k_j}{2m}$으로 나타낼 수 있습니다. . 💡 The expected number of edges between nodes $i$ and $j$ of degrees $k_i$ and $k_j$ equals $ frac{k_ik_j}{2m}$ . 그렇다면 재배치 된 그래프 $G’$ 전체에 대해서 예상되는 엣지의 수는, . . 다음과 같이 구할 수 있습니다. 여기서 주목할 점은 null model을 사용하여 엣지를 싹 재배치 한다고 하더라도, 전체 노드 차수 분포나 총 엣지 수는 보존 된다는 것입니다. . Modularity-based Community Detection . 이제 다시 modularity 설명으로 돌아와서, null model의 수식적 표현을 활용하여 modularity를 다시 표현해보면 아래와 같습니다. . . 즉, 모든 분리된 subgraph $s$에 대해서 $s$내의 모든 가능한 노드 쌍에 대해 실제 두 노드의 연결 여부($A_{ij}$)와 null model로써 예측된 두 노드 사이의 엣지 갯수($ frac{k_ik_j}{2m}$)의 차이를 총합함으로써 modularity $Q$를 구할 수 있습니다. 여기서 계수 $ frac{1}{2m}$은 단순히 $Q$값을 -1과 1 사이로 만들기 위해 붙었다고 보시면 됩니다. . 만약 우리가 밀도 높은 커뮤니티들을 잘 분리해냈다면 modularity $Q$가 양수의 값을 가지게 될 것이고, 이 값이 1에 가까워질 수록 더욱 더 잘 분리했다고 생각할 수 있습니다. 일반적으로 $Q$가 0.3-0.7 사이의 값을 가질 때 커뮤니티 구조를 잘 파악했다고 본다고 합니다. 따라서 앞으로 다룰 community detection은 모두 이 modularity 값을 최대화 하는 objective을 가지고 이야기가 진행될 것입니다. . 💡 Community Detection ↔ Maximizing Modularity . 앗! 참고로 지금까지는 unweighted 그래프 기준으로 설명드렸지만, 단순하게 이진 인접행렬을 weighted 인접행렬, 그리고 노드 차수를 weighted 차수(edge weights의 합)로 바꿈으로써 weighted 그래프에도 확장 적용할 수 있습니다 🤭 (unweighted 그래프 = edge weight가 모두 1인 weighted 그래프로 생각할 수 있으므로…) . .",
            "url": "https://cs224w-kor.github.io/blog/gnn/community/community%20detection/louvain/bigclam/2022/08/24/lecture-1302.html",
            "relUrl": "/gnn/community/community%20detection/louvain/bigclam/2022/08/24/lecture-1302.html",
            "date": " • Aug 24, 2022"
        }
        
    
  
    
        ,"post8": {
            "title": "Lecture 13.1 - Community Detection in Networks",
            "content": ". Lecture 13. Community Detection in Networks . Lecture 13.1 - Community Detection in Networks | Lecture 13.2 - Network Communities | Lecture 13.3 - Louvain Algorithm | Lecture 13.4 - Detecting Overlapping Communities | . . Networks &amp; Communities . 우리는 직관적으로 네트워크를 아래 그림과 같은 형태로 생각하곤 합니다. 가장 단적인 예로, 자신의 인간관계 그래프를 떠올려 보십시오. 나의 가족과 친구로 구성된 최측근 집단이 있는 반면, 직장 혹은 학교에서 이름과 얼굴 정도만 아는 좀 더 먼 관계들도 있을 수 있습니다. . . Flow of Job Information . 1960년도에 Granovetter은 이런 소셜 네트워크 상 사람들이 어떤 경로로 직업을 소개 및 추천 받는지에 대한 분석을 수행하였습니다. 분석 결과, 사람들은 주로 주변 사람들로부터 직업을 소개 받지만, 놀랍게도 가까운 지인(close-friend)보다 가벼운 친분이 있는 사람들(acquaintance)로 부터 소개 받는 경우가 더 많았습니다. 왜 이런 결과가 도출되었을까요? . . Granovetter’s Explanation . 위 의문점에 대한 해답으로 Granovetter은 그래프 상 엣지, 즉 사람들 사이에 형성된 관계들을 구조적인 관점과 사회적인 관점으로 동시에 봐야 한다고 이야기 했습니다. 더 나아가 Granovetter은 엣지의 구조적인 역할과 사회적인 역할을 결부하려고 하였는데요, 그 요점은 아래와 같습니다. . 구조적으로 밀도 높게 모여있는 노드들 간의 엣지 ↔사회적으로 더 가까운 관계를 나타냄 (Tightly-connected edges ↔ Socially strong) | 구조적으로 멀리 떨어진 노드들 사이를 잇는 엣지 ↔ 사회적으로 더 먼 관계를 나타냄 (Long-range edges ↔ Socially weak) | . 이를 정보 흐름의 관점에서 재해석하면 왜 사람들이 사회적으로 친분이 덜한 사람들에게서 직업 소개를 더 많이 받는지 알 수 있습니다 🙂 . . 위 그래프 상 Weak/ W로 표시된 long-range 엣지를 통해 네트워크의 다른 부분들에서 유의미한 정보가 흘러 들어올 수 있습니다. | 위 그래프 상 Strong/ S로 표시된 tightly-connected 엣지를 통해 흘러 들어오는 정보들은 보다 불필요하거나 중복된 정보일 확률이 높습니다. | . 여러분들도 정기적으로 만나는 가장 친한 친구들과 대화하다 보면 항상 대화가 거기서 거기라고 느껴본 적 있으신가요? 아무래도 가장 자주 소통하는 친구들이다 보니까 자연스레 서로 알고 있는 정보가 비슷해지는 반면, 오랜만에 만나는 사람들과 얘기해보면 생각치 못했던 새로운 정보나 사실을 알게 되곤 하죠. 이런 실생활의 예시로도 위와 같은 현상이 설명되는 것 같네요! . Triadic Closure . 그렇다면 네트워크에 존재하는 밀도 높은 군집들은 어떻게 해서 생겨나는 것일까요? . . 그 이유를 알아보기에 앞서서 위와 같은 그래프에서 노드 a와 b가 엣지로 연결될 확률이 더 높을까요 혹은 노드 a와 c가 연결될 확률이 더 높을까요? 아마 공통된 이웃 노드를 갖고 있는 a와 b가 연결될 확률이 더 높을 것입니다. 이렇게 서로 완전 연결된 삼각형 구조를 triadic(3인조의) closure이라고 합니다. 네트워크 상 triadic closure이 많다는 것은 곧 우리가 2강에서 다뤘던 clustering coefficient가 높다는 것과 동일한 의미를 가집니다. . 💡 Clustering Coefficient (2강 참조) . 네트워크에서 triadic closure이 생겨서 결국 밀도 높은 군집들이 형성되는 이유는 다음과 같습니다. . 만약 B와 C라는 사람이 A라는 공통인 친구가 있다고 가정한다면, . . B와 C는 둘 다 A와 시간을 보낼 것이기 때문에 서로 만날 확률이 높습니다. | B와 C는 공통 지인이 있기 때문에 서로를 신뢰할 수 있을 것입니다. | A는 B와 C를 따로 만나 시간을 보내기 보다, B와 C를 서로 소개 시켜준 후 다같이 함께 만난다면 시간적, 금전적으로 이득이 생깁니다. 따라서 A는 B와 C를 서로 소개 시켜주려는 경향이 있을 것입니다. | 실제로 연구 결과에 따르면 소셜 네트워크 상 triadic closure을 많이 갖고 있지 않은 십대 소녀들은 사회적인 군집을 이루고 있는 학생들에 비해 자살을 생각하는 경우가 더 많다고 합니다. 이렇게 triadic closure을 만들어 사회적으로 어떤 군집 혹은 커뮤니티에 소속되고자 하는 것은 인간의 본성인가 봅니다 😂 . Edge Strength in Real Data (1) - 실험 세팅 . 수년동안 큰 소셜 네트워크의 부재로 인해 Granovetter의 이론은 검증되지 못했습니다. 하지만 오늘날에는 이메일, 메신저, 휴대폰, SNS를 통해 사람들 사이에 폭넓은 소통이 가능해지게 됨으로써 비로소 Granovetter의 이론을 테스트해볼 수 있게 되었죠. . 우리는 EU 국가 인구 일부의 통화 네트워크를 활용하여 Granovetter 이론의 타당성을 검증해보도록 하겠습니다(Onnela et al. 2007). Granovetter의 주장에 따르면 네트워크 상에서 밀도 높게 모여 있는 노드들이 곧 사회적으로 더 가까운 관계를 나타내기 때문에, 과연 정말로 밀집된 노드 간 실제 통화량(edge weight/ edge strength)이 더 높은지 직접 확인해볼까요? . 구조적인 노드 밀집도 (Tightly-connected edges) 이어진 두 노드가 구조적으로 얼마나 밀도 높게 모여 있는지 확인하기 위해 Edge overlap이라는 개념을 새로 도입하겠습니다. Edge overlap은 아래의 식으로 구할 수 있으며, 0과 1 사이의 값으로 도출됩니다. Edge overlap이 1에 가까워질수록 두 노드는 한 커뮤니티에 속하는 노드일 가능성이 높으며, 0에 가까워질수록 두 노드를 잇는 엣지는 커뮤니티와 커뮤니티를 잇는 local bridge에 가까워집니다. . . | 사회적인 밀접도 (Socially strong) 사회적인 밀접도는 통화량을 나타내는 edge weight(edge strength)로 판단할 것입니다. . | . 💡 Q ) Edge Overlap $ propto$ Edge Strength(# phone calls) . Edge Strength in Real Data (2) - 실험 결과 . Edge Overlap vs. Strength 각 엣지에 대한 edge overlap 값과 edge strength를 나타낸 그래프는 아래와 같습니다. 파란색으로 표현된 우리 데이터에 대해서 예상 그대로 edge overlap과 edge strength가 정비례하는 관계를 보입니다. 빨간색으로 표현된 선은 실험의 베이스라인으로써, 우리 데이터의 네트워크 구조는 그대로 유지하되 edge strength 값만 랜덤하게 재배치한 그래프에 대한 결과입니다. . . 데이터를 직접 시각화 해보면 이러한 경향이 더 확연하게 보입니다. 아래의 왼쪽 그래프가 우리의 실제 통화량 그래프이며, 여기서 엣지의 색깔이 edge weight를 나타냅니다. 보다시피 밀집되어있는 노드들 사이가 붉은색 엣지로 연결되어 상호 통화량이 많다는 사실을 확인할 수 있습니다. 반면, 오른쪽에 제시된 랜덤 edge weight의 베이스라인 네트워크에선 구조적인 밀집도와 관계 없이 붉은색 엣지가 산재되어 있는 것을 볼 수 있습니다. . . | Edge Removal 더 나아가 그래프의 엣지를 삭제해가면서 나뉘는 그래프의 sub-components 중 가장 큰 subgraph의 크기를 확인해보도록 하겠습니다. 여기서 각각 edge strength와 edge overlap을 기준으로 하여 그래프의 엣지를 삭제할텐데, 만약 Granovetter의 주장대로 edge strength와 edge overlap이 비례하는 관계라면, 두 경우 모두 엣지 삭제 결과의 경향성이 비슷해야 할 것입니다. . 아래 결과 그래프에서 볼 수 있다시피, 두 경우에서 모두 edge strength와 edge overlap의 값이 작은 엣지부터 삭제했을 때 가장 큰 subgraph의 크기가 더 작아진다는 것을 확인할 수 있습니다. 이는 edge strength와 edge overlap값이 작은 엣지는 커뮤니티 간 local bridge일 확률이 높기 때문에 비교적 더 빠르게 전체 네트워크를 subgraph로 나눌 수 있기 때문입니다. . . | Conceptual Picture of Networks . . 지금까지 Granovetter의 이론의 검증을 마쳤습니다. 그의 이론에 따르면 결론적으로 그래프는 1) 국소적으로 밀집되어 strong 엣지로 연결된 노드 커뮤니티와, 2) 서로 다른 커뮤니티를 잇는 weak 엣지들로 구성됩니다. . .",
            "url": "https://cs224w-kor.github.io/blog/gnn/community/community%20detection/louvain/bigclam/2022/08/24/lecture-1301.html",
            "relUrl": "/gnn/community/community%20detection/louvain/bigclam/2022/08/24/lecture-1301.html",
            "date": " • Aug 24, 2022"
        }
        
    
  
    
        ,"post9": {
            "title": "Lecture 12.3 - Finding Frequent Subgraphs",
            "content": ". Lecture 12 . Lecture 12.1 - Fast Neural Subgraph Matching &amp; Counting | Lecture 12.2 - Neural Subgraph Matching | Lecture 12.3 - Finding Frequent Subgraphs | . . Lecture 12.3 - Finding Frequent Subgraphs . Problem . . Enumerating all size-k connected subgraphs | Counting #(occurrences of each subgraph type) | 가장 빈도수가 높은 size-k의 Motifs를 찾기 위해서는 다음 2가지를 해결해야 한다. 그런데 이렇게 Enumerating하고 Counting하는 것은 가능한 모든 패턴들을 조합시켜서 Combinatorial explosion을 가져오기 때문에 매우 hard computational problem이다. 따라서 우리는 이러한 문제를 Representation learning을 통해서 해결한다. GNN을 이용하여 그래프의 임베딩을 서로 비교하면서 두 그래프의 관계를 찾아서 해결한다. . SPMinier . . 가장 빈도수가 높은 size-k의 Motifs를 찾는 하나의 neural model이 바로 SPMiner이며, $G_T$ 그래프를 분해하여 order embedding space로 보낸 뒤 임베딩 공간에 나타난 뒤에 주어진 Subgraph $G_Q$를 모두 비교하며 Subgraph 빈도 수를 구하는 것이다. 여기서 Subgraph의 집합 = size-k의 Motifs의 후보 . . Order embedding space에서는 subgraph의 여부를 쉽게 알 수 있으며 위 그림에서 붉은색 영역 내의 모든 노란 점들은 $G_Q$를 포함하는 모든 $G_T$의 neighborhoods가 된다. . . SPMiner 목표는 k step 마다 가장 많은 eighborhood embeddings를 포함하는 Motif를 찾는 것이고, 학습은 무작위로 한개의 노드를 초기의 값으로 선택한 후$(S = u)$에 각 step 마다의 subgraph를 저장하는 과정($S$의 이웃 노드들을 골라 점진적으로 늘려 Motif의 사이즈를 키워간다 = 스텝을 진행하면서 motif를 성장시켜 더 큰 motif를 찾는 것이 목표이며, 위의 그림의 빨간 점에 속하는 neighborhoods의 수를 최대화하는 것이 목표)으로 이루어지고, 지정한 $k$(원하는 mofit 크기)에 도달하면 학습을 멈추며 Subgraph를 도출한다. . Summary . Subgraphs and motifs are important concepts that provide insights into the structure of graphs. (Subgraph와 Motif는 그래프의 구조에 대한 insights를 제공하는 중요한 개념) . Their counts can be used as features for nodes and graphs. (이를 노드 및 그래프의 기능으로 사용할 수 있다.) | . | We covered neural approaches to prediction subgraph isomorphism relationship. (Subgraph를 예측하기 위해서 neural apporaches를 적용하였다.) | Order embeddings have desirable properties and can be used to encode subgraph relations (Order embeddings의 속성을 사용하여서 Subgraph의 관계를 encode에 사용할 수 있다.) | Neural embedding-guided search in order embedding space can enable ML model to identify motifs much more frequent than existing methods (order embedding space을 통해서 ML 모델이 기존 방법보다 훨씬 더 Motif를 식별할 수 있습니다.) — Reference . | . CS224W: Machine Learning with Graphs 2021 Lecture 12.3 - Finding Frequent Subgraphs . Lecture 12. Frequent Subgraph Mining with GNNs . 12. Frequent Subgraph Mining with GNNs .",
            "url": "https://cs224w-kor.github.io/blog/graph%20neural%20network/gnn/graph%20convolution%20network/gcn/2022/08/17/lecture-1203.html",
            "relUrl": "/graph%20neural%20network/gnn/graph%20convolution%20network/gcn/2022/08/17/lecture-1203.html",
            "date": " • Aug 17, 2022"
        }
        
    
  
    
        ,"post10": {
            "title": "Lecture 12.2 - Neural Subgraph Matching",
            "content": ". Lecture 12 . Lecture 12.1 - Fast Neural Subgraph Matching &amp; Counting | Lecture 12.2 - Neural Subgraph Matching | Lecture 12.3 - Finding Frequent Subgraphs | . . Lecture 12.2 - Neural Subgraph Matching . Subgraph Matching . Subgraph matching이란: query 그래프가 target 그래프의 subgraph isomorphism인지 확인하는 task . . 우리는 다음과 같이 Query와 Target 그래프가 주어졌을때 Query가 Target의 subgraph 인지 판단하기 위해서 embedding space의 geometric shape을 활용하며, 여기서 embedding space의 geometric shape을 구하기 위해서 GNN을 활용합니다. . . Node anchor를 활용하여 query의 노드 $v$와 target의 노드 $u$의 임베딩이 동일한지 확인하며, Query의 anchor node가 k-hop을 가질 때 k-hop 내에 있는 이웃노드들의 임베딩 또한 비교한다. . Order Embedding Space . . 다음과 같이 embedding space 안에 ${ color{red} square}$과 ${ color{green} bigcirc}$, ${ color{yellow} bigcirc}$를 Representation했을 때 ${ color{green} bigcirc}$은 ${ color{green} bigcirc}⩽ leqslant⩽{ color{red} square}$의 관계를 가지기 때문에 Target의 Subgraph라고 할 수 있다. . 그러나 Query2는 ${ color{yellow} bigcirc}⋠ npreceq⋠{ color{red} square}$이기 때문에 Target과 다른 그래프라고 할 수 있다. . . Subgraph isomorphism relationship 은 3종류로 다음과 같이 embedding space에 Representation 된다. . Transitivity는 ${ color{yellow} square}$ $ preccurlyeq$ ${ color{green} square}$, ${ color{green} square}$ $ preccurlyeq$ ${ color{red} square}$ 인 경우 ${ color{yellow} square}$ $ preccurlyeq$ ${ color{red} square}$의 관계를 가지는 형태로 ${ color{yellow} square}$ 은 ${ color{green} square}$의 Subgraph이고 ${ color{green} square}$은 ${ color{red} square}$의 Subgraph이고 ${ color{yellow} square}$ 은 ${ color{red} square}$의 Subgraph인 관계이다. . Anti-symmetry는 ${ color{yellow} square}$ == ${ color{green} square}$ 의 관계를 가지는 형태로 ${ color{yellow} square}$ 과 ${ color{green} square}$이 서로 동일한 그래프인 관계이다. . Closure under intersection은 ${ color{yellow} square}$ 이 ${ color{red} square}$과 ${ color{green} square}$의 부분적인 Subgraph인 관계이며, 음수를 가지는 임베딩은 없으며, 여기서 ${ color{yellow} square}$ 유효한 값을 가진다. . Loss Function . . Subgraph 속성이 순서 임베딩 공간에서 보존되도록 순서 제약 조건을 지정하며, 이를 위해서 max-margin loss를 사용하게되며 이를 GNN에 학습하여 embedding space 구합니다. 해당 loss function은 아래와 같습니다. . E(Gq,Gt)=∑i=1D(max⁡(0,zq[i]−zt[i]))2E left(G_{q}, G_{t} right)= sum_{i=1}^{D} left( max left(0, z_{q}[i]-z_{t}[i] right) right)^{2}E(Gq​,Gt​)=i=1∑D​(max(0,zq​[i]−zt​[i]))2 . 위의 수식을 그래프 $G_{q}$와 $G_{t}$ 사이의 “margin”로 정의합니다. . $E left(G_{q}, G_{t} right)=0$ (왼쪽 그래프)이면 $G_{q}$ 는 $G_{t}$ 의 Subgraph가 이고 $E left(G_{q}, G_{t} right)&gt;0$ (오른쪽 그래프)이면 $G_{q}$ 는 $G_{t}$ 의 Subgraph가 아님을 나타냅니다. . Training . 학습 데이터셋 $ left(G_{q}, G_{t} right)$는 $G_{t}$ 의 subgraph인 $G_{q}$가 반, 그렇지 않은 것이 반이 되도록 구성해야 한다. | Positive sample에 대해서는 $E left(G_{q}, G_{t} right)$ 를 최소화하도록 negative smaple에 대해서는 $ max left(0, alpha-E left(G_{q}, G_{t} right) right)$ 를 최소화하도록 학습하는데 이는 모델이 임베딩을 너무 멀리 이동시키는 것을 방지하기 위함이다. For positive examples: $G_{q}$가 $G_{t}$의 subgraph일 때 $E left(G_{q}, G_{t} right)$ 최소화 | For negative examples: $ max left(0, alpha-E left(G_{q}, G_{t} right) right)$ 최소화 | . | 데이터셋 $G$ 로부터 학습을 위한 $G_{T}$ 와 $G_{Q}$ 를 샘플링하는 과정이 필요하다. 매번 반복할 때마다 새로운 training pairs을 샘플링한다. | 이점: 반복할 때마다 모델은 다른 Subgraph example를 볼 수 있다. | 샘플링되는 Subgraph가 기하급수적으로 많기 때문에 성능은 향상되고, 과적합이 방지된다. | . | $G_{T}$는 무작위로 anchor 노드 $v$ 를 뽑은 뒤 거리가 $K$ 인 모든 노드를 포함시켜 만든다. | Positive example $G_{Q}$는 BFS 샘플링을 거친다. 일반적으로 데이터 집합의 크기에 따라 3-5를 사용한다. | 런타임과 성능을 절출하는 하이퍼파라미터 | . | . . $S=v, V= phi$ 로 초기화한다. | 매 스텝마다 $S$ 의 모든 이웃 노드 집합 $N(S)$의 $10 %$ 를 샘플링하여 $S$ 로 업데이트하며 나머지 노드들은 $V$ 로 업데이트 한다. | $K$ 스텝을 거치면 $G_{Q}$를 얻게 된다. | Negative example은 $G_{Q}$로부터 노드/엣지들은 제거하거나 추가하여 만든다. . Summary . Neural subgraph matching uses a machine learning based approach to learn the NP-hard problem of subgraph isomorphism (NP-hard 문제인 Subgraph matching를 해결하기 위해서 ML 기반의 접근법을 사용) Given query and target graph, it embeds both graphs into an order embedding space (쿼리와 그래프가 주어지면 모두 order embedding space에 포함시킨다.) | Using these embeddings, it then computes $E left(G_{q}, G_{t} right)$ to determine whether query is a subgraph of the target (이러한 임베딩을 사용하여 $E left(G_{q}, G_{t} right)$을 계산하여 Subgraph인지 확인한다.) | . | Embedding graphs within an order embedding space allows subgraph isomorphism to be efficiently represented and tested by the relative positions of graph embeddings (order embedding space를 통해 Subgraph를 효율적으로 표현하였으며, graph embedding의 상대적 위치에 의해 테스트 된다.) — Reference . | . CS224W: Machine Learning with Graphs 2021 Lecture 12.2 - Neural Subgraph Matching . Lecture 12. Frequent Subgraph Mining with GNNs . 12. Frequent Subgraph Mining with GNNs .",
            "url": "https://cs224w-kor.github.io/blog/graph%20neural%20network/gnn/graph%20convolution%20network/gcn/2022/08/17/lecture-1202.html",
            "relUrl": "/graph%20neural%20network/gnn/graph%20convolution%20network/gcn/2022/08/17/lecture-1202.html",
            "date": " • Aug 17, 2022"
        }
        
    
  
    
        ,"post11": {
            "title": "Lecture 12.1 - Fast Neural Subgraph Matching & Counting",
            "content": ". Lecture 12 . Lecture 12.1 - Fast Neural Subgraph Matching &amp; Counting | Lecture 12.2 - Neural Subgraph Matching | Lecture 12.3 - Finding Frequent Subgraphs | . . Lecture 12.1 - Fast Neural Subgraph Matching &amp; Counting . Subgraphs와 Motifs는 그래프의 구조적 insights를 제공해주는 중요한 concepts이기 때문에, Subgraphs와 Motifs의 개수를 셈으로써 우리는 노드나 그래프의 features로 사용할 수 있다. . 따라서 우리는 Subgraphs와 motifs의 개수를 세는 방법을 이번 목차에 다룰 것이며, Motifs를 Subgraphs에 포함되거나 동일한 개념으로 이해하시면 됩니다. . Subgraphs . . Subgraph는 네트워크의 구성 요소이며, 이를 통해서 네트워크를 구성할 수 있기 때문에 building blocks of networks이며, Subgraph를 레고 블럭으로 생각한다면, 레고들이 모여서 큰 레고를 형성하듯이 네트워크를 구성하게 됩니다. 이렇게 레고 블럭과 같이 많은 영역에서 반복되는 구조의 구성요소를 통하여 그래프의 기능 및 동작을 결정합니다. . 이러한 Subgraph 구성 기준에 있어 2가지 방법이 있는데, 첫번째는 노드를 중심으로 Subset을 구성하면 “Node-induced subgraph”이며, “induced subgraph”라고 부르기도 합니다. . 두번째로 엣지를 중심으로 구성하면 “Edge-induced subgraph”라고 하며, “non-induced subgraph” or just “subgraph”라고 부르기도 합니다. . 이러한 두 가지의 구성 기준은 도메인에 따라 두 방식 중 하나가 선택되어 사용되며 예를 들어 chemistry는 노드가 중요하므로 node-induced를 knowledge graph에서는 엣지가 중요하므로 edge-induced를 활용합니다. . Graph에서는 두 개의 그래프가 서로 같은 지를 아는 것이 중요한데, 이러한 문제를 Graph isomorphism problem 라고 하며, 이 문제는 NP-hard입니다. . . 예를 들어서 위와 같은 그래프가 주어졌을때, 우리는 두 개의 그래프가 완전히 다르다고 말할 수 있을까요? 우리는 대부분 $G_2$ 안 에 $G_1$ 이 포함되어 있다고 말할 것이며, 이러한 그래프의 isomorphism을 조금 더 확실하게 구분하기 위해서는 Node와 Edge를 가지고 서로 두 그래프를 비교해보는 것입니다. . . 위 처럼 두 그래프는 시각적으로 모두 다른 그래프 처럼 보이지만 첫번째 그래프의 경우에는 각 노드와 연결된 엣지를 기준으로 비교해본다면 서로 같은 그래프이다. 두번째 그래프의 경우 각 노드에 연결된 엣지가 다르니 다른 그래프이다. 이처럼 Node와 Edge를 가지고 서로 두 그래프를 비교하면 두 그래프의 isomorphism 판단할 수 있다. . . $G_1$과 $G_2$의 node를 각각 match 시키면 우리는 $G_1$과 $G_2$ 사이에 동일한 subgraph를 가진다는 것을 알 수 있다. 따라서 우리는 $G_1$과 $G_2$는 다른 그래프지만 $G_1$은 $G_2$의 subgraph 라고 할 수 있다. . . 왼쪽 이미지처럼 그래프의 크기가 같더라도 엣지의 수 및 방향에 따라 다양한 non-isomorphic 그래프가 존재합니다. 왼쪽 그림은 그래프의 크기가 4인 모든 비동형, 연결된 비방향 그래프의 예시이며, 오른쪽은, 크기가 3인 모든 비동형 연결 방향 그래프의 예시입니다. . Network Motifs . . Network motifs는 “recurring, significant patterns of interconnections”라고 정의하며 Network Motifs를 정의하기 위해서는 3가지 개념을 알아야 합니다. . 1. Pattern: 찾고자 하는 node-induced subgraph로 motif이라고도 부르며, 그림에 Induced subgraph of interest 와 같은 유도될 수 있는 subgraph의 패턴이다. . 2. Recurring: pattern이 전체 그래프에서 나타나는 빈도를 의미한다. . 3. Significant: real graph와 random으로 생성된 graph를 비교를 통해 랜덤하게 생성된 그래프에서 기대보다 빈번하게 있는가를 의미한다. . . Motif가 필요한 이유는 이러한 Motif을 통해 그래프의 작동방식을 이해할 수 있으며 예측하는데 도움이 되며, 예를 들어, feed-forward loop는 noise를 제거하는 용도로 뇌의 뉴런 네트워크에서 흔히 볼 수 있으며, parallel loops는 먹이사슬 관계에서 볼 수 있으며, Single-input module는 유전자 제어 네트워크에서 흔히 볼 수 있다고 합니다. . Subgraph Frequency . . 왼쪽 이미지처럼 Graph-level에서는 거대 그래프 $G_T$에 포함되는 작은 그래프 $G_Q$의 개수로 빈도를 정의할 수 있으며, 오른쪽 이미지처럼 Node-level에서는 $G_Q$의 anchor 노드를 $v$라 할 때 $G_Q$와 isomorphicg한 $G_T$의 subgraph들 중 $u$로 맵핑되는 노드 $v$의 개수를 빈도로 정의하며, 이러한 방식은 outlier에 더 robust하다고 합니다. . . 데이터 셋에 여러 개의 그래프가 포함되어 있고 데이터 세트에서 하위 그래프의 빈도를 계산하려는 경우에는 데이터 셋이 여러개의 그래프를 가진다면 연결되지 않은 부분을 가진 하나의 거대 그래프 $G_T$로 보고 빈도를 계산한다고 합니다. . Motif Significance . . Motif Significance는 랜덤으로 생성된 network와 비교하고자 하는 network(real)에서 주어진 motif의 발생 빈도를 비교하여 계산한다. . . Random Graphs를 정의하는 방법은 위의 그림 처럼 3가지로 정의가능하며, 첫번째 방법은 Erdős–Rényi random graphs라고 하며, n개의 노드를 정하고 각 노드를 연결하는 엣지를 생성할 확률 p를 정하여 random graph를 생성하는 방법이다. . 또 다른 방법은 Configuration model라고 하며, Configuration model은 주어진 degree sequence를 사용하여 random graph를 생성하는 방법이다. 하나의 노드가 주어지면 각 노드는 degree(=spokes)가 존재한다. 이 각 노드에 존재하는 edge의 개수, 즉 degree를 하나의 작은 node로 생각하여 각 노드를 무작위로 연결한다. 이 연결선이 1개 이상 존재하면 마지막과 같이 그래프의 Node를 연결하여 random graph를 생성한다. . 또 다른 방법은 Switching이라 하며, Switching은 그림과 같이 A-&gt;B / C-&gt;D 로 연결되는 edge를 무작위로 선택한 후, 두 edge의 endpoint를 서로 교환하여 A-&gt;D / C-&gt;B 로 연결되는 edge로 바꾸어 random graph를 생성하는 방법이다. 따라서 Switching은 random으로 생성된 그래프와 비교될 그래프의 node degree가 서로 같다는 특징을 가진다. Configuration model은 node degree가 달라질 수 있다. . Motif Significant를 계산하는 과정은 총 3 Step으로 이루어 진다. . 지금까지는 1, 2단계를 다뤘고 이제 각각의 모티브가 통계적으로 얼마나 유의한지에 대하여 측정하는 방법인 3단계를 배울 것임 . 1단계: 주어진 그래프 $ left(G^{ text {real}} right)$의 모티프 수 | 2단계: 유사한 통계(예: 노드 수, 에지 수, 정도 순서)를 가진 랜덤 그래프 생성 및 랜덤 그래프에서 모티브 수 계산 | 3단계: 통계적 측정을 사용하여 각 모티프가 얼마나 유의한지 평가합니다. | Z-점수 사용 | . Z-score for Statistical Significance . $Z_{i}$는 모티프 $i$의 통계적 중요성을 포착 : . Zi=(Nireal −Nˉirand )/std⁡(Nirand )Z_{i}= left(N_{i}^{ text {real }}- bar{N}{i}^{ text {rand }} right) / operatorname{std} left(N{i}^{ text {rand }} right)Zi​=(Nireal ​−Nˉirand )/std(Nirand ) . $N_{i}^{ text {real }}$은 graph $G^{ text {real }}$에서 motif $i$의 수 | $ bar{N}_{i}^{ text {rand }}$ random graph들의 motif $i$의 수의 평균이다. | . Network significance profile (SP): . SPi=Zi/∑jZj2S P_{i}=Z_{i} / sqrt{ sum_{j} Z_{j}^{2}}SPi​=Zi​/j∑​Zj2​ . ​ . $SP_i$는 정규화된 Z-점수의 벡터이다. | $SP_i$의 차원은 motifs의 수가 된다. | SP emphasizes relative significance of subgraphs: 크기가 다른 네트워크를 비교하는 데 중요 | 일반적으로 큰 그래프일 수록 Z-score의 값이 커진다. | . | . . Motif Significant는 Z-score와 비슷한 개념이다. 랜덤으로 생성된 network의 motif의 발생 빈도의 평균과 표준편차를 사용하고 여기서 랜덤으로 생성된 network를 모집단으로 사용하여 Z-score를 구하며, 우리는 Network significance profile를 정규화된 Z-score의 벡터로 구할 수 있다. 일반적으로 큰 그래프일 수록 Z-score의 값이 커진다. . 우리는 Z-score를 통해서 각 Subgraph에 Significance(음수 값은 표현이 부족함, 양수 값은 과표현)를 분류할 수 있으며, 이를 통해 network significance profile(모든 하위 그래프 유형에 대한 값이 있는 형상 벡터)을 만들 수 있으며, 다음으로는 여러 그래프의 프로파일을 랜덤 그래프와 비교합니다. . . 규제망(유전자 규제) | 뉴런 네트워크(시냅스 연결) | 언어 네트워크(단어 인접) | 월드 와이드 웹(페이지 간 하이퍼링크) | 소셜 네트워크(우정) | . 비교를 통해 서로 다른 도메인의 network는 서로 다른 network significance profile를 가진다는 것을 알 수 있고 서로 같은 도메인의 network는 서로 같은 network significance profile을 가진다는 것을 확인할 수 있다. 따라서 우리는 network significance profile을 통해서 해당 도메인의 고유한 특성에 대한 insight를 얻을 수 있습니다. . Summary . Subgraphs and motifs are the building blocks of graphs (motifs는 graphs의 일부분) Subgraph isomorphism and counting are NP-hard ( isomorphism and counting은 NP-hard 단계의 어려움을 같는다.) | . | Understanding which motifs are frequent or significant in a dataset gives insight into the unique characteristics of that domain (motifs의 significant를 통해서 우리는 해당 도메인의 고유한 특성에 대한 insight를 얻을 수 있음) | Use random graphs as null model to evaluate the significance of motif via Z-score (random graphs는 motif의 significance를 평가하기 위한 null model로써 사용됨) | . . Reference . CS224W: Machine Learning with Graphs 2021 Lecture 12.1-Fast Neural Subgraph Matching &amp; Counting . Lecture 12. Frequent Subgraph Mining with GNNs . 12. Frequent Subgraph Mining with GNNs .",
            "url": "https://cs224w-kor.github.io/blog/graph%20neural%20network/gnn/graph%20convolution%20network/gcn/2022/08/17/lecture-1201.html",
            "relUrl": "/graph%20neural%20network/gnn/graph%20convolution%20network/gcn/2022/08/17/lecture-1201.html",
            "date": " • Aug 17, 2022"
        }
        
    
  
    
        ,"post12": {
            "title": "Lecture 11.3 - Query2box Reasoning over KGs",
            "content": ". Lecture 11. Reasoning in Knowledge Graphs using Embeddings . Lecture 11.1 - Reasoning in Knowledge Graphs | Lecture 11.2 - Answering Predictive Queries | Lecture 11.3 - Query2box: Reasoning over KGs | . . Motivation . 11.2의 마지막 부분에서 다음과 같은 해결해야 할 질문들이 있었습니다. . 중간 노드들은 entities의 집합(set)이 될텐데 어떻게 나타낼 수 있을것인가? | latent space에서 교집합(intersection) 연산(operation)을 어떻게 나타낼 수 있을 것인가? | Box Embeddings . latent space에 query를 hyper-rectangles(boxes)로 나타낼 수 있습니다. 박스는 중심점과 크기를 나타내는 offset의 값으로 정의하고 나타낼 수 있습니다. 따라서 이를 수식으로 나타내면 아래와 같습니다. . . 임베딩 공간에서 나타내보면 아래 그림과 같을 것 입니다. 이렇게 box형식의 표현이 좋은 점은 앞서 motivation에서 살펴보았던 2가지 고민에 대한 좋은 해결책이 되기 때문입니다. (1) 해당 query에 속하는 node들의 set을 손쉽게 box안에 표현하면 되는 것이고 (2) intersection 또한 각 query를 나타내는 box area의 교집합 부분으로 나타낼 수 있기 때문입니다. . . query를 box로 나타내기로 한 다음 이제 다른 부분들은 자동으로 다음과 같이 embedding space에 표현됩니다. . Entity embedding: entity는 zero-volume box(하나의 점)로 나타냅니다. 파라미터 수는 $d|V|$ 입니다. | Relation embedding: 하나의 box에서 다른 새로운 box로 매칭하는 것으로 표현됩니다. 파라미터 수는 $2d|R|$ 입니다. | Intersection operator $f$: 인풋은 box들이고 아웃풋으로는 하나의 box가 나옵니다. 인풋으로 들어가는 box들의 교집합 부분을 구하는 모델입니다. | . Projection Operator . 하나의 query 영역에서 relation을 통해 다음의 query 영역으로 이동하는 것을 다음과 같이 Projection Operator를 이용해서 표현할 수 있습니다. 새로운 query $q’$의 center와 offset 모두 projection operator를 통해 이전 query $q$와 relation $r$로부터 구할 수 있습니다. . . 이를 앞서 conjunctive query에서 intersection을 구하기 전까지의 query plan 과정을 embedding space에서 나타내면 다음과 같습니다. ESR2와 Short of Breath에서 각각 출발하여 relation과정(projection 연산)을 통해 query box가 embedding space에서 움직이는 것을 볼 수 있습니다. . . Intersection Operator . query box들의 교집합 부분은 Intersection operator를 통해서 구할 수 있습니다. 이를 embedding space에서의 그림으로 나타내면 아래와 같습니다. Intersection operator는 앞서 살펴봤던 것처럼 인풋으로는 여러개의 box들을 받고 아웃풋으로는 하나의 box를 출력합니다. 따라서 아웃풋인 새로운 box의 center point와 offset을 어떻게 구할지 설계해야 합니다. . . Center point | embedding space에서 여러개의 box들의 intersection이 되는 부분의 중심점은 box들의 center point들과 가깝다는 insight를 활용합니다. 따라서 여러개 box들의 center point들로 만들어지는 경계 박스(아래 그림에서 분홍색 box) 안에 intersection box의 center point가 있을 것입니다. . . 따라서 수식적으로 새로운 intersection box의 중심점은 인풋이 되는 box들의 중심점들의 위치를 weighted sum을 하여 구합니다. 이때 weight $ mathbf{w}i$는 Neural network인 $f{cen}$을 통해 training되는 trainable한 값이며 각 center point들에 대한 self-attention score를 나타낸다고 볼 수 있습니다. 즉, 아래의 수식과 같이 box들의 center point인 $Cen(q_i)$가 NN $f_{cen}$을 통해 나온 아웃풋을 softmax를 통해 구한 값이 weight $ mathbf{w}_i$가 되는 것입니다. . . Offset (Box size) | 당연히 intersection이 되는 영역은 인풋이 되는 box들의 크기보다 작아질 것 입니다.(shrinking) intersection의 offset은 아래와 같은 수식으로 표현되는데, min()에서 아웃풋의 박스가 줄어드는 것을 보장하며 Neural Network $f_{off}$에 box들의 offset들을 넣어서 나온 값에다가 sigmoid 함수를 통과하여 나온 0~1 사이의 값을 곱하여 intersection box의 offset을 결정하게 됩니다. . . 이렇게 intersection operator까지 구하는 법을 배웠으니 앞서 진행한 conjunctive query plan을 embedding space에서의 연산 과정으로 나타내면 다음과 같습니다. 최종적으로 빗금쳐진 intersection이 answer 영역이 됩니다. . . Entity-to-Box Distance . Embedding 공간에서 앞으로 여러 연산과 계산을 하기 위해 거리를 정의할 수 있어야 할 것 입니다. Entity(하나의 점)과 Box(query)사이의 거리를 어떻게 구하는 것이 좋을까요? . 우선 entity는 zero-volume의 점이므로 하나의 위치로 나타낼 수 있습니다. 이와 같은 맥락으로 box는 중심점을 대표 위치로 나타낼 수 있습니다. 하지만 box는 점이 아닌 하나의 영역이므로 박스 안과 밖을 구분할 수 있고 따라서 이를 구분하여 점-박스 사이의 거리 $d_{box}$를 계산하게 됩니다. box 안에 있는 경우, 거리 $d_{in}$에 0~1사이의 값인 weight $ alpha$를 곱해주어 계산합니다. 수식으로 나타내면 아래와 같습니다. . . . 이렇게 정의한 점-박스 사이의 거리 $d_{box}$를 기반으로 score function $f_q(v)$는 다음과 같이 정의할 수 있습니다. 거리가 멀수록 bad score이어야 하므로 $-$를 붙여 음수로 만들어줍니다. . . Embedding AND-OR Queries . Union operation중 하나인 or 에 대해 생각해봅시다. 예를 들어 유방암 또는 폐암을 치료할 수 있는 약물은 무엇일까?와 같은 질문에 필요한 연산일 것 입니다. Conjunctive query와 disjunction을 결합한 것을 Existential Positive First-order (EPFO) query라고도 합니다. 여기에서는 앞으로 AND-OR query라고 지칭하겠습니다. 여기서 고민해야 할 부분은 다음과 같습니다. . disjunction operator를 어떻게 설계할 것인가? | 저차원의 vector space에서 AND-OR query들을 어떻게 나타낼 것인가? | 우선 2번째 고민에 대한 해결책은 없습니다. 임의의 query들의 합집합(union)을 나타내기 위해서는 고차원의 embedding이 필요하기 때문입니다. 아래의 예시를 보면서 이해해보겠습니다. . . 총 3개의 query가 주어져 있고 각 query에 해당하는 answer node들 $v_1$, $v_2$, $v_3$이 있다고 해봅시다. answer node들은 서로 중복되지(overlapping) 않습니다. . . 그러면 우선 각 query를 box로 나타내어 answer에 맞도록 그려보면 다음과 같을 것입니다. . . 다음으로 query의 합집합 연산 v에 대한 query box를 그려보면 다음과 같습니다. 아직까지는 answer(positive)와 answer가 아닌(negative) entity들을 구분하는 영역을 그리는 것이 가능합니다. . . 하지만 다음과 같이 entity가 4개인 경우가 된다면 answer와 answer가 아닌 것들을 구분하는 box embedding을 구할 수 없게 됩니다. 이처럼 non-overlapping answer들을 가지고 있는 query들의 갯수가 증가할 수록 이들의 OR 연산자를 다룰 수 있는 공간의 차원도 높아지게 되어 저차원에서의 AND-OR query를 나타낼 수 없는 것입니다. . . 하지만 저차원의 AND-OR 연산자를 나타낼 수 없다고 해서 이를 아예 해결할 방법이 없는 것은 아닙니다. 여기서 핵심 아이디어는 맨 마지막 step의 union만 제외하고 다른 union 연산들은 다 풀어서 표현해주면 된다는 것입니다. 아래의 그림에서 처럼 앞 step에 있던 union을 풀어서 더 여러개의 relation으로 표현해준 다음 맨 마지막 단계에서 union을 넣어주어 동치인 상황을 만들어줍니다. . . Disjunctive Normal Form . AND-OR query는 동치인 Disjunctive Normal form, DNF로 치환될 수 있습니다. . . Entity와 DNF $q=q_{1} vee q_{2} vee cdots vee q_{m}$ 사이의 거리는 다음과 같이 정의 됩니다. . . query q의 AND-OR embedding 과정 query $q$를 동치인 DNF $q=q_{1} vee q_{2} vee cdots vee q_{m}$로 만든다. | $q_1$~ $q_m$ 모두 embedding 한다. | (box) distance $d_{box}( mathbf{q}_i, mathbf{v})$를 계산한다. | 3번에서 구한 distance들 중에서 최솟값 min을 구한다. | final score인 $f_q(v)=-d_{box}( mathbf{q}, mathbf{v})$를 구한다. | | . Training . 학습으로 training할 수 있는 부분은 다음과 같습니다. . 파라미터 수가 $d|V|$ 인 Entity embedding | 파라미터 수가 $2d|R|$ 인 Relation embedding | Intersection operator $f$ | . 학습 과정 은 다음과 같습니다. . training graph인 $G_{train}$에서 학습할 query $ mathbf{q}$를 샘플링 합니다. 이때 정답이 되는 $v in llbracket q rrbracket_{G_{train}}$과 정답이 아닌 negative sample인 $v’ notin llbracket q rrbracket_{G_{train}}$ 모두 샘플링 합니다. | 샘플링 된 query $ mathbf{q}$를 embedding 합니다. | score function을 이용하여 $f_q(v)$와 $f_q(v’)$을 계산합니다. | 아래와 같이 정의된 loss function $l$을 이용하여 positive sample의 score는 최대화, negative sample의 score는 최소화하는 방향으로 학습합니다. | . Query Generation from Templates . training set에는 다음과 같이 다양한 multiple query template들을 생성해서 넣어줄 수 있습니다. . . answer node에서 시작해서 backward하는 방식으로 training sample들을 만들어 낼 수 있습니다. . . 위와 같은 KG에서 answer node인 Fulvestrant를 하나 선택합니다. 이 노드에서부터 TreatedBy, Assoc relation을 통해 backward를 하게 되면 [Anchor1, (Relation1, Realtion2)] template인 [ESR2, (Assoc, TreatedBy)]의 sample을 하나 얻게 되는 것입니다. 이와 같은 매커니즘으로 CausedBy relation으로 backward하게 되면 [Anchor1, (Relation1)] template의 sample 하나를 얻게 됩니다. . . 이러한 backward를 통해 positive sample인 $ llbracket q rrbracket_{G}$를 얻게 되고, negative sample은 KG에서 random 하게 샘플링하게 만들어지게 되는데 이때 random하게 만들어진 negative sample이 positive sample과 겹치지 않는지 주의할 필요가 있습니다. . Visualization Embedding Space . 실제로 box embedding이 무엇을 배우는지 확인해보기 위해 t-SNE를 이용하여 시각화 해봅시다. (고차원에서 box embedding이 이루어지고 2차원의 t-sne로 나타내는 것이므로 box 영역으로 나타내어지지 않습니다.) . . 11강 정리 . 큰 KG에서의 predictive query의 answer를 찾는 방법에 대해 알아보았습니다. | 핵심은 query를 embedding함으로써 latent space에서의 탐색을 가능하게 했다는 점입니다. embedding된 query들을 가지고 할 수 있는 다양한 operator들에 대해 알아보았습니다. | query box embedding을 통해 embedding space에서 answer를 구할 수 있습니다. | . | . . Original Lecture Video : CS224W: Machine Learning with Graphs 2021 Lecture 11.3 - Query2box: Reasoning over KGs .",
            "url": "https://cs224w-kor.github.io/blog/reasoning/knowledge%20graph/box%20embedding/projection%20operator/intersection%20operator/distance/query%20generation/score%20function/t-sne/2022/08/17/lecture-1103.html",
            "relUrl": "/reasoning/knowledge%20graph/box%20embedding/projection%20operator/intersection%20operator/distance/query%20generation/score%20function/t-sne/2022/08/17/lecture-1103.html",
            "date": " • Aug 17, 2022"
        }
        
    
  
    
        ,"post13": {
            "title": "Lecture 11.2 - Answering Predictive Queries",
            "content": ". Lecture 11. Reasoning in Knowledge Graphs using Embeddings . Lecture 11.1 - Reasoning in Knowledge Graphs | Lecture 11.2 - Answering Predictive Queries | Lecture 11.3 - Query2box: Reasoning over KGs | . . Traversing KG in Vector Space . 핵심 아이디어는 Query를 embedding하는 것입니다. 이전에 배운 TransE를 다시 복습해보면 $ mathbf{h}$에서 $ mathbf{t}$로 가기 위해 $ mathbf{r}$를 경유하여 갈 때의 score function $f_r(h, t)$는 다음과 같이 나타냈었습니다. . . . 이것을 Query Embedding 관점으로 다시 생각해보면, query embedding $ mathbf{q} = mathbf{h} + mathbf{r}$로 볼 수 있고 이때의 목표는 query embedding $ mathbf{q}$를 answer embeding인 $ mathbf{t}$와 가깝게 만드는 것입니다. 따라서 query를 임베딩 한다는 것은 TransE를 multi-hop reasoning으로 일반화 시키는 것과 같습니다. Query의 relation들이 여러개 적용되는 과정은 embedding space에서 vector addition 과정이며 이는 Knowledge Graph에서의 entity마다 독립적으로 일어나는 과정입니다. . . . . 💡 - ***TransE***는 **composition relation을 다룰 수 있었기 때문에** path query의 multi-hop를 **latent space에서의 relation addition으로** 표현할 수 있도록 했습니다. - 반면, *TransR, DistMult, ComplEx*는 composition relation을 처리할 수 없기 때문에 path query를 다룰 수 있도록 확장할 수 없습니다. Conjunctive Queries . . 지금까지 One-hop query와 이를 multi-hop으로 확장한 Path Query까지 알아보았습니다. 그렇다면 마지막으로 남은 더 복잡한 query들 간의 결합(conjunction) 연산도 가능할까요? 각 query의 anchor node에서 출발하여 traversing을 하고 난 뒤, 각 traversing의 결과인 node의 교집합(intersection)을 구하면 정답을 구할 수 있습니다. . . 하지만 여기서도 missing connection이 있다면 해당 connection과 관련된 정답을 놓칠 수 있습니다. 이런 경우 implicit하게 connection을 추론해볼 수 있습니다. 만약 (ESR2, Assoc, Breast Cancer)이 missing되었다면 (1) ESR2가 BRCA1과 ESR1 모두와 interaction 관계를 가지고 있고 (2)BRCA1과 ESR1 각각은 Breast cancer과 association 관계를 가지고 있기 때문에 (ESR2, Assoc, Breast Cancer)을 가질 수 있음을 추론해볼 수 있습니다. . . Traversing KG in Vector Space . 하지만 KG traversing을 벡터 공간에서 나타낼 때 몇가지 더 고민해볼 점들이 있습니다. 이에 대한 답은 다음 section 11.3에서 찾아보겠습니다. . conjunctive query에서 나타나는 중간과정의 노드(intermediate node)들은 집합(set)으로 나타나게 되고 이를 어떻게 표현할 것인지 | latent space에서의 Intersection을 어떻게 정의하고 어떤 연산과정으로 나타낼 것인지 | . . Original Lecture Video : CS224W: Machine Learning with Graphs 2021 Lecture 11.2 - Answering Predictive Queries .",
            "url": "https://cs224w-kor.github.io/blog/reasoning/knowledge%20graph/traversing/query%20embedding/transe/conjunctive%20query/2022/08/17/lecture-1102.html",
            "relUrl": "/reasoning/knowledge%20graph/traversing/query%20embedding/transe/conjunctive%20query/2022/08/17/lecture-1102.html",
            "date": " • Aug 17, 2022"
        }
        
    
  
    
        ,"post14": {
            "title": "Lecture 11.1 - Reasoning in Knowledge Graphs",
            "content": ". Lecture 11. Reasoning in Knowledge Graphs using Embeddings . Lecture 11.1 - Reasoning in Knowledge Graphs | Lecture 11.2 - Answering Predictive Queries | Lecture 11.3 - Query2box: Reasoning over KGs | . . 복습: Knowledge Graph Completion Task . 거대한 Knowledge Graph가 주어졌을 때 어떻게 KG를 완성(missing part를 찾는 것)시킬 수 있을까요? 이에 대한 해결방법은 head와 relation이 주어졌을때 tails를 예측하는 것으로 바꿔 생각해볼 수 있습니다. . . 아래의 문학 Knowledge Graph의 예시에서 “J.K. Rowling”라는 heading과 “genre”라는 relation이 주어졌을 때, “Science Fiction”이라는 tail을 예측하는 task를 생각해 볼 수 있습니다. . . Reasoning over Knowledge Graphs . KG에서의 추론(Reasoning)은 다음과 같은 목표로 생각해볼 수 있습니다. . Goal: KG에서 multi-hop(=그래프 상에서 여러번 traverse) 추론하기 | . 결국 multi-hop query를 예측하는 것이라고 생각할 수 있는데, 그렇다면 불완전하고(incomplete) 거대한(massive)한 KG에서 어떻게 multi-hop reasoning을 할 수 있을까요? 문제를 정의하기 위해 우선 query 타입들에 대해 알아보겠습니다. query 타입에는 아래의 그림과 같 3가지, one-hop, path(multi-hop), conjuntive query가 있습니다. . . 아래의 KG에서 각 query 타입의 예시 질문과 이에 따른 query 표기를 살펴보겠습니다. . . Query 타입 예시(자연어 질문) 예시(Query) . One-hop Queries | What adverse event is caused by Fulvestrant? F에 의해 일어나는 부작용은? | [e:Fulvestrant, (r:Causes)] | . Path Queries | What protein is associated with the adverse event caused by Fulvestrant? F에 의해 일어나는 부작용과 관련된 단백질은 무엇인가? | [e:Fulvestrant, (r:Causes, r:Assoc)] | . Conjunctive Queries | What is the drug that treats breast cancer and caused headaches? 유방암을 치료하는데 쓰이지만 두통을 일으킬 수 있는 약물은 무엇인가? | ([e:BreastCancer, (r:TreatedBy)], [e:Migraine, (r:CausedBy)]) | . *e: entity, r:relation . Predictive One-hop Queries → Path Queries . 사실 KG completion 문제는 One-hop query문제들로 나누어서 생각해볼 수 있습니다. One-hop보다 더 복잡해지면 One-hop을 더 여러개 붙인다고 생각하면 되기 때문입니다. . . 그래서 결국 N-hop query는 다음과 같이 “achor”인 $v_a$를 시작점으로 하는 여러개의 relation들이 이어지는 $(r_1, r_2, …, r_n)$으로 나타낼 수 있습니다. 이때 해당 KG completion에 맞는 정답 query는 여러개일 수 있기 때문에 $ llbracket q rrbracket_{G}$로 나타냅니다. . . . 예시를 들어 Notation을 살펴보겠습니다. . 아래의 KG에서 Fulvestrant에 의해 일어나는 부작용과 관련된 단백질은 어떤 것들이 있을까요? . . 이 task에서 anchor node는 Fulvestrant가 되고, 2번의 relation들을 거쳐서 답변을 할 수 있습니다. Notation과 매칭하여 정리해보면 다음과 같습니다. . $v_a$→ e:Fulvestrant | $(r_1, r_2)$→ (r: Causes, r:Assoc) | Query: [e:Fulvestrant, (r:Causes, r:Assoc)] | . . Query가 정리되었다면 이제 query에 맞는 정답 path를 찾아야 할 것 입니다. 어떻게 KG에서 정답 path들을 찾아볼 수 있을까요? . 간단하게 KG에서 query의 relation 순서에 맞춰 traverse를 하면 됩니다. Fulvestrant에서 시작하는 (1) Causes relation을 가지고 있는 모든 다음 노드들을 찾습니다. 그 다음으로 해당 노드에서 (2) Association relation을 가지고 있는 모든 노드들을 찾아 정답을 찾으면 됩니다. . . Predictive Queries with LIMITATION . 단순하게 traversing하면서 query에 응답하는 것으로 해결하면 좋겠지만 불행히도 KG는 incomplete한 경우가 많습니다. 즉, 관련된 지식이 충분하지 못해서 그래프에 나타내지 못했을 수도 있고 모든 사실들을 상당한 시간과 비용을 들여 표현하기가 사실상 거의 불가능에 가깝기 때문에 대다수의 relation들이 missing되어 있는 incomplete KG를 받아들일 수 밖에 없습니다. 따라서 이런 불완전함때문에 모든 answer를 찾을 수 없습니다. 예를 들어 앞선 예시에서 Fulvestrant↔Short of Breath 의 relation이 missing되어 있다면 BIRC2 를 찾을 수 없을 것입니다. . . 그렇다면 (probabilistic) complete한 KG를 만드는 것을 우선적으로 해야 할까요? . 하지만 complete한 KG는 매우 연결성이 약한 관계조차로 0이 아닌 매우 작은 값(확률)으로라도 표현이 될 것이기 때문에 매우 dense할 것입니다. 이렇게 dense한 KG에서 traversing의 시간복잡도는 path의 길이를 $L$이라고 했을 때 $d^L_{max}$가 될 것이기 지수적으로 증가할 것이기 때문에 구하기 어려워집니다. . . 따라서 incomplete한 KG에서 path-based queries를 구하는 task인 Predictive queries를 정의하고 문제를 풀게 됩니다. 이 task는 link prediction task가 좀 더 일반화된 것이라고 생각할 수 있으며 one-step prediction을 multi-step prediction으로 확장하는 부분이 중요한 포인트입니다. . . Original Lecture Video : CS224W: Machine Learning with Graphs 2021 Lecture 11.1 - Reasoning in Knowledge Graphs .",
            "url": "https://cs224w-kor.github.io/blog/reasoning/knowledge%20graph/one-hop%20query/path%20query/conjunctive%20query/predictive%20query/2022/08/17/lecture-1101.html",
            "relUrl": "/reasoning/knowledge%20graph/one-hop%20query/path%20query/conjunctive%20query/predictive%20query/2022/08/17/lecture-1101.html",
            "date": " • Aug 17, 2022"
        }
        
    
  
    
        ,"post15": {
            "title": "Lecture 6.3 - Deep Learning for Graphs",
            "content": ". Lecture 6. Graph Neural Networks (1) GNN Model . Lecture 6.1 - Introduction to Graph Neural Networks | Lecture 6.2 - Basics of Deep Learning | Lecture 6.3 - Deep Learning for Graphs | . . 이제 본격적으로 이번 강의의 메인 주제였던 딥러닝을 활용한 그래프 데이터 임베딩 방법에 대해 공부해 봅시다. 참고로 Deep Encoder은 그래프 뉴럴 네트워크(GNN)으로도 부르기 때문에 혼재해서 사용하더라도 헷갈리지 않으시길 바랍니다. . Setup . 들어가기에 앞서 반복적으로 활용할 notation에 대해 간략히 설명하고 시작하겠습니다. 앞으로 아래 기호를 쭉 사용하여 설명을 할 예정이니 잘 알아두시기 바랍니다. . $V$: 노드 집합 | $A$: 인접 행렬 (Adjacency matrix) | $X in mathbb{R}^{m times |V|}$: 노드 features | $v$: 노드 집합에 포함된 한 노드, $N(v)$: $v$의 이웃 노드 | . 그래프 구조는 엣지의 방향성 및 가중 여부에 따라 여러 종류로 분류할 수 있지만, 이해를 위해 여기서는 가장 간단한 undirected &amp; unweighted 그래프로 설명하겠습니다. 따라서 인접 행렬은 0과 1로 이루어진, 대각 방향으로 symmetric한 행렬이라고 생각하실 수 있습니다. . 또한, 이전의 Shallow Encoder과는 달리 이제 노드 feature도 함께 고려하여 노드 임베딩을 학습한다는 점에 유의하시기 바랍니다. . 사실 대부분의 그래프 데이터셋은 노드 feature을 포함하고 있지만, 만에 하나 없는 경우라면 다음과 같은 벡터/값을 노드 feature로 사용하기도 합니다. 노드의 one-hot 인코딩 벡터 | 상수 벡터 [1, 1, …, 1] | 노드 차수(degree) | . | . Naive Approach . 딥러닝 모델을 활용하여 그래프 및 노드를 임베딩 하기 위해 가장 쉽게 생각할 수 있는 방법은 단순하게 그래프의 구조적인 특성을 나타내는 인접 행렬과 노드 feature 행렬을 그냥 이어 붙여서 딥러닝 모델에 던져주는 것입니다. . . 위와 같은 undirected 그래프의 각 노드가 2차원의 feature를 각각 가지고 있다면, 단순히 두 행렬을 이어 붙여서 만든 7차원의 벡터를 뉴럴 네트워크에 전달하면 각 노드를 간단히 임베딩 할 수 있을 것입니다. 하지만 이러한 방법에는 다음과 같은 문제가 존재합니다. . $O(|V|)$ 파라미터가 필요함 노드 feature이 $d$차원이라고 가정하면, 각 노드가 뉴럴 네트워크에 입력되는 차원이 $|V|+d$ 이겠죠? 따라서 그래프의 노드 갯수에 비례하여 모델의 파라미터가 증가합니다. | 다른 사이즈의 그래프에는 적용할 수 없음 위와 같은 그래프에 대해 뉴럴 네트워크를 기껏 학습시켜 놓았는데, 100개의 노드로 구성된 새로운 그래프가 인풋으로 들어온다면, 인풋 차원이 맞지 않아 학습시킨 임베딩 모델을 활용할 수 없을 것입니다. | 노드 순서가 바뀌면 동일 노드의 임베딩이 달라질 수 있음 위 그래프에서 노드 순서를 A→B→C→D→E에서 B→E→A→C→D 등으로 바꾼다면, 이에 따라 인접 행렬도 바뀌게 됩니다. 이렇게 된다면 같은 A 노드를 임베딩 하기 위해 인풋으로 활용되는 7차원의 벡터가 달라지기 때문에 임베딩 결과 값도 달라질 것입니다. | . From Images to Graphs . 그렇다면 기존 CNN 모델에서 아이디어를 차용해보는 건 어떨까요? . . . 이미지를 다루는 CNN은 위와 같이 고정된 사이즈의 convolution 필터를 사용하여 이미지를 주욱 훑습니다. 여기서의 convolution 필터는 붉은 원으로 표시된 타겟 픽셀의 주위 픽셀 정보를 축약하는 역할을 합니다. 사실 이미지가 특수한 형태의 그래프로 해석될 수 있음을 생각해보면, 그래프 데이터에서도 타겟 노드의 임베딩을 만들기 위해 주변 노드의 정보를 사용한다는 아이디어는 나쁘지 않아 보입니다. . 하지만 그래프에서는 CNN에서와 같이 고정된 크기의 필터(?)를 사용할 수 없습니다. 어떤 노드는 한두개의 이웃 노드를 가지지만 또 어떤 노드는 수백수천개의 이웃 노드를 가질 수 있기 때문이죠. . . 그렇다면 그래프를 임베딩 할 때 타겟 노드의 이웃 노드에서 정보를 전달받아 이를 활용하여 타겟 노드의 임베딩을 업데이트 하되, 타겟 노드마다 이웃 노드의 갯수가 다를 수 있는 점을 고려하여 각기 다른 computation graph를 갖도록 하는 것이 좋겠습니다! . 다음과 같은 아이디어를 근간으로 Graph Convolutional Network가 등장하게 되었습니다. 남은 강의에서는 이 GCN을 디테일하게 설명하고 있습니다. . Idea: Aggregate Neighbors . 주요 Idea: 이웃 노드 정보를 가지고 타겟 노드 임베딩을 생성하자! . . 왼쪽과 같은 그래프에서 우리가 임베딩하고 싶은 타겟 노드가 노란색의 A 노드라고 생각해 봅시다. 그렇다면 A 노드의 이웃 노드, 그리고 그 이웃 노드들의 이웃 노드를 가지고 오른쪽과 같은 computation graph가 생깁니다. . 타겟 노드 A는 직속 이웃인 노드 B, C, D로부터 메시지를 전달 받고, 모든 메시지를 합친 후 어떠한 변환을 거쳐 본인의 임베딩으로 활용합니다. 우측 그림에서 회색 박스로 표시된 뉴럴 네트워크가 바로 이러한 1) 메시지 변환, 2) 이웃 노드로부터 온 메시지를 통합하는 두 과정을 수행하게 됩니다. 이 뉴럴 네트워크 내의 모델 파라미터가 최종적인 우리의 학습 대상이 되는 것입니다. . 여기서 또 눈여겨 보아야 할 점이 있습니다. 여지껏 우리는 노란색 A 노드를 타겟 노드로 한 computation graph만 보았는데, 그렇다면 B, C, D 등 다른 타겟 노드에 대해서도 동일한 computation graph를 가질까요? . . 아닙니다. 노드마다 이웃 노드의 갯수와 종류가 다르기 때문에 당연히 노드마다 서로 다른 computation graph를 가지게 됩니다. . Deep Model: Many Layers . . Layer의 수 . Deep Encoder은 여러 layer로 구성할 수 있는데, 한 layer이 직속 이웃 노드에 대한 정보를 aggregate하는 것이기 때문에 layer을 두 개 쌓는다면 직속 이웃 노드에 대한 이웃 노드, 즉 타겟 노드로부터 2-hop 떨어진 노드의 정보까지 활용하겠다는 의미로 해석할 수 있습니다. 위 그림에서도 잘 나타나 있는데, 2개의 layer로 구성된 모델을 사용하기 때문에 타겟 노드 A로부터 2-hop 떨어진 노드 E, F의 정보도 임베딩 생성에 활용되는 것을 볼 수 있습니다. . 노드 임베딩 초기화 . 또한, 보통 Layer-0에서 최초 노드 임베딩으로 노드 feature을 사용합니다. 모든 노드 임베딩은 layer을 거칠수록 이웃 노드의 정보를 변환하고 합친 후 업데이트 됩니다. 결국 모든 layer을 거치고 나면 최종 노드 임베딩이 생성되어 우리가 downstream task를 위해 사용하게 되는 것이죠. . Aggregator 함수 . 여기서 타겟 노드 A가 이웃 노드 B, C, D의 메시지를 aggregation 할 때, 노드 B, C, D의 순서와 관계 없이 aggregate된 메시지는 동일해야 합니다. 즉, 메시지를 aggregate하는 함수는 permutation-invariant한 속성을 가져야 한다는 말입니다. 일반적인 GNN은 주로 합, 평균, 맥스 풀링등의 aggregator를 활용합니다. . The Math: Deep Encoder . . 자, 이제 가장 기본적인 GNN 형태를 정의하고 이를 수식으로 나타내어 알고리즘의 원리를 자세히 들여다보는 시간을 갖겠습니다. 우리의 GNN은 타겟 노드의 이웃 노드 임베딩을 전달받아 이를 평균냄으로써 메시지를 aggregate 합니다. 그 후, 뉴럴 네트워크를 통해 어떠한 변환을 거치고 이를 활용하여 타겟 노드 임베딩을 업데이트 합니다. 이를 수식으로 나타내면 아래와 같습니다. . . 식이 처음엔 되게 복잡해 보이지만, 하나씩 뜯어보면 사실 아주 간단합니다. 식 전반에 나타나는 $h_{v}^{(l)}$ 은 $l$번째 layer에서 노드 $v$의 임베딩을 나타낸다고 보시면 됩니다. . 초록색 수식 블럭 : 첫 노드 임베딩을 노드 feature로 초기화합니다. | 파란색 수식 블럭 : $l+1$번째 layer에서의 노드 임베딩을 만들기 위해, 먼저 타겟 노드 $v$의 이웃 노드에 대해 $l$ 번째 layer에서의 노드 임베딩 평균을 구합니다. 그 후, 이웃 노드의 평균 임베딩에 어떠한 transformation $W_{l}$ 을 가합니다. | 빨간색 수식 블럭 : 타겟 노드의 임베딩을 업데이트할 때 이웃 노드 뿐 아니라, 이전 layer에서 갖고 있던 자기 자신의 임베딩도 활용합니다. 같은 방법으로 $h_{v}^{(l)}$에 어떠한 transformation $B_{l}$ 을 가합니다. | 노란색 수식 블럭 : 최종적으로 비선형 함수를 적용해서 $l+1$번째 layer에서의 타겟 노드 임베딩을 구합니다. | 보라색 수식 블럭 : 노드 임베딩 업데이트 과정을 $L$번 반복합니다. 이 때 최종적으로 형성된 노드 임베딩은 본인으로부터 L-hop 떨어진 노드의 정보까지 활용하여 만든 것입니다. | Matrix Formulation . 이전에 Random Walk를 행렬 형태로 표현했듯이, 벡터식으로 다뤘던 GNN도 행렬식으로 다시 표현해보겠습니다. . 모든 노드 임베딩 벡터를 한데 모아 노드 임베딩 행렬을 만든다면, 이는 아래와 같이 표현할 수 있습니다. . $H^{L} = {[h_{1}^{(l)} … h_{|V|}^{(l)}]}^T$ . 그렇다면 이웃 노드의 임베딩을 합산하는 과정은 그래프의 인접 행렬을 사용하여 아래와 같이 간단하게 나타낼 수 있습니다. . $ sum_{u in N_{v}} h_{u}^{(l)} = A_{v:}H^{(l)}$ . 만약 대각 행렬을 이렇게 정의한다면, 이 대각 행렬의 역행렬은 다음과 같기 때문에, . $D_{v,v} = Deg(v) = |N(v)|$ , $D_{v,v}^{-1} = 1/|N(v)|$ . 이를 활용하면 이웃 노드의 임베딩을 평균내는 연산을 행렬식으로 간단하게 표현할 수 있습니다. . . 따라서 최종적으로 노드 임베딩 업데이트 함수를 행렬식으로 나타내면 아래와 같습니다. . . 벡터식으로 이해하기도 어려웠는데 왜 굳이 사서 행렬식으로 변환하냐고요? 사실 행렬식이 가지는 구현상의 이점이 있기 때문입니다. 행렬식을 사용한다면 각 노드에 대한 임베딩을 따로 따로 업데이트 하지 않고 하나의 행렬로써 한번에 업데이트 할 수 있으며, 이 과정에서 $ tilde{A}$가 희소 행렬이기 때문에 보다 더 효율적인 행렬 연산이 가능하기 때문에 구현할 때는 행렬식이 더 선호됩니다. . How to train a GNN . 오늘 강의의 마지막 부분으로 이렇게 구성한 GNN을 어떻게 학습시켜야 하는지 알아보겠습니다. 시작하기에 앞서 학습의 대상이 되는 파라미터가 무엇인지 짚고 넘어가보죠. . . 다음 식에서 학습되는 파라미터는 $W_{l}$와 $B_{l}$ 입니다. 딸린 subscript를 봐도 알 수 있듯이, 두 파라미터 행렬은 모두 layer마다 따로 존재하며, 한 layer 내에서는 공유됩니다. 이를 간단하게 그림으로 표현하면 아래와 같습니다. . . GNN을 학습하는 방법은 여느 딥러닝 학습과 마찬가지로 크게 지도 학습, 비지도 학습 세팅으로 나뉩니다. 이를 하나씩 살펴보도록 합시다. . 지도 학습 세팅 노드 label $y$가 존재하는 상황 | 정답 노드 label $y$를 활용하여 $min_{ theta} mathcal{L}(y,f(z_v))$를 풂, 이 때 task에 맞게 L2 loss 혹은 Cross entropy loss 등을 loss 함수를 사용함 | 예시) 각 노드가 safe한지 혹은 toxic한지 분류하는 node classification → 분류 문제이므로 cross-entropy loss를 사용하고, 노드의 정답 클래스 label을 활용하여 직접 모델 학습 가능 . 💡 아래 loss 식에서 $ sigma(z_v^T theta)$는 학습된 노드 임베딩 $z_v^T$를 가지고 모델이 예측한 노드 $v$의 클래스 확률 을 나타냅니다. . | . . . | 비지도 학습 세팅 노드 label이 존재하지 않는 상황 | 3강에서 공부했던 그래프 상 노드 similarity를 supervision으로 활용 . . 임의의 supervision 시그널을 만들어 비지도 학습 세팅을 지도 학습 세팅으로 바꾸기 위해서 원본 그래프에서의 노드 similarity를 바탕으로 label을 지정해줍니다. 여기서 노드 similarity는 3강에서 다뤘던 Random Walk 등을 활용할 수 있습니다. . 간단하게 DeepWalk로 노드 similarity를 정의하는 경우를 생각해볼까요? 만약 두 노드 $u$와 $v$가 랜덤 워크 상에서 co-occur한다면 두 노드는 ‘similar’하다고 말할 수 있으며, $y_{u,v} = 1$로 임의의 label을 붙입니다. 또한 여기서 $DEC(z_u,z_v)$는 학습된 두 노드의 임베딩을 내적함으로써 임베딩 공간에서의 노드 similarity를 측정합니다. Loss 함수로 cross-entropy를 사용함으로써 그래프에서 노드 similarity를 최대한 잘 보존하도록 노드 임베딩을 학습할 수 있게 됩니다. . 💡 원본 그래프에서 similar한 노드는 → similar한 임베딩을 갖도록 합니다 . | . | Model Design: Overview . 자 그럼, 오늘 배웠던 내용을 한번 쭉 훑어 정리하고 포스트를 마무리하도록 하겠습니다. 그래프를 위한 Deep Encoder, a.k.a. GNN 모델을 만들기 위해서 아래와 같은 단계를 따라가야 합니다. . 이웃 노드 임베딩을 aggregate하는 함수를 정함 | Task의 특성에 맞추어 loss 함수를 정의함 | . . 여러 computation graph에 대해 GNN 모델을 학습시킴 | 학습된 모델을 갖고 노드에 대한 임베딩을 생성할 수 있음. 이 때, 모든 노드에 대해 뉴럴 네트워크의 파라미터가 공유되기 때문에 학습에 사용되지 않은 노드에 대해서도 임베딩을 생성할 수 있음 (Inductive Capability) | . 💡 Inductive Capability1. 새로운 그래프 : 예를 들어 분자 그래프에서, 화합물 A에 대해 학습된 GNN 모델이 화합물 B 그래프에서 임베딩을 만드는데 활용될 수 있음2. 새로운 노드 : 시간에 따라 evolving 하는 그래프에서 새로운 노드가 추가될 때마다 바로바로 임베딩을 생성할 수 있음 . Summary . Deep Encoder (GNN)의 핵심 아이디어: 이웃 노드의 정보를 aggregate 함으로써 노드 임베딩을 생성하자! | 모델 내 Aggregator과 Transformation 함수를 각각 어떻게 정의하느냐에 따라 모델 구조가 달라질 수 있습니다. | 다음 강의에선 GNN variant의 하나인 GraphSAGE를 다룰 것입니다. | . .",
            "url": "https://cs224w-kor.github.io/blog/graph%20neural%20network/gnn/graph%20convolution%20network/gcn/2022/07/27/lecture-0603.html",
            "relUrl": "/graph%20neural%20network/gnn/graph%20convolution%20network/gcn/2022/07/27/lecture-0603.html",
            "date": " • Jul 27, 2022"
        }
        
    
  
    
        ,"post16": {
            "title": "Lecture 6.2 - Basics of Deep Learning",
            "content": ". Lecture 6. Graph Neural Networks (1) GNN Model . Lecture 6.1 - Introduction to Graph Neural Networks | Lecture 6.2 - Basics of Deep Learning | Lecture 6.3 - Deep Learning for Graphs | . . 이번 파트는 본격적으로 그래프 데이터를 위한 딥러닝을 설명하기에 앞서 딥러닝에 익숙하지 않은 사람들을 위해 딥러닝의 기본 개념을 간단하게 설명하는 부분입니다. 많은 내용을 커버하긴 하지만 딥러닝 초심자들은 CS231n과 같은 딥러닝 강의를 먼저 수강하고 오심을 추천드립니다! . Machine Learning as Optimization . 먼저 간단한 지도 학습 문제를 생각해 봅시다. 지도 학습(Supervised Learning)이란 데이터에 대한 ground truth label이 존재하는 경우를 일컫는데, 다시 말해 input $x$가 주어졌을 때, label $y$를 예측하는 문제라고 할 수 있습니다. 이러한 task는 아래와 같은 식을 통해 최적화 문제로 바꾸어 해결할 수 있습니다. . . 위 식을 이해하기 위해 먼저 구성 요소에 대해 하나씩 짚어보겠습니다. . $ theta$ : 우리가 최적화(학습) 하려는 파라미터들 . 최종적으로 우리가 학습하고자 하는 파라미터 값입니다. 예를 들어 앞의 Shallow Encoder에서는 학습으로 결정되는 $|V| times d$ 사이즈의 임베딩 look-up table이 $ theta$에 해당하겠죠. . $ mathcal{L}$ : Loss 함수 . Loss 함수는 ground truth label $y$와 머신러닝 모델이 예측한 label $f(x)$의 차이를 계산하는 metric 입니다. 회귀(Regression) 문제에서 주로 사용되는 L2 loss와 분류(Classification) 문제에서 주로 사용되는 Cross entropy loss 이외에도 L1 loss, Huber loss, Hinge loss 등 다양한 loss 함수가 존재합니다. 대표적인 loss 함수인 L2 loss와 Cross entropy loss의 수식은 각각 다음과 같습니다. . L2 loss . . Cross entropy loss . . . 결국 우리가 원하는 것은 모델이 예측한 label이 정답 ground truth label에 가까워지는 것이기 때문에, 이 loss 함수 값이 작으면 작을수록 우리의 모델이 더 정확한 예측을 한다는 의미입니다. . . 그럼 이제 위에서 공부한 각 구성 요소를 바탕으로 다시 이 최적화 식을 해석해 봅시다. 결국 우리가 풀고자 하는 머신러닝 문제는, 정답 label $y$와 모델이 예측한 label $f(x)$의 차이를 나타내는 loss 함수를 최소화 하는 방향으로 모델 파라미터 $ theta$를 최적화하는 문제로 해석할 수 있습니다. . Gradient Descent . 지금까진 두루뭉술하게만 보였던 머신러닝 문제를 동일한 의미인 최적화 문제로 재정의했습니다. 그렇다면 이 최적화 문제를 어떻게 해결해야 할까요? . . (출처: https://ieeexplore.ieee.org/abstract/document/9298092) . 우리는 일반적으로 Loss 함수로 convex function(볼록 함수)를 활용합니다. 이 loss 함수의 값이 작아지는 방향으로 모델 파라미터 $ theta$ 를 업데이트 하기 위해, $ theta$에 대한 $ mathcal{L}$의 기울기를 구한 후, 기울기가 작아지는 방향으로 $ theta$를 업데이트 해줍니다. . (위 그림에서 Cost를 $ mathcal{L}$, Weights를 $ theta$로 보시면 됩니다!) . . 이를 다시 Gradient 벡터라는 개념으로 정리해서 이야기 해보겠습니다. Gradient 벡터란 위의 식과 같이 $ theta$에 대한 $ mathcal{L}$의 편미분, 즉 기울기 값으로 구성된 벡터로써, 가장 빠르게 $ mathcal{L}$이 증가하는 방향을 나타내는 방향 도함수 벡터입니다. . 💡 Gradient is the directional derivative in the direction of largest increase . 일단 Gradient를 구했으면 이제 할 일은 모델 파라미터 $ theta$를 gradient의 반대방향으로, 즉 $ mathcal{L}$이 작아지는 방향으로, 반복적으로 업데이트 하는 것입니다. . . 위 식에서 $ eta$는 learning rate로, 한번 파라미터를 업데이트 시 얼마나 변경할 것인지 보폭을 나타내는 값입니다. 이는 학습 과정 동안 동일하게 유지할 수도 있고, 목적에 따라 계속 변하게 할 수도 있습니다. 이론적으로 모델 파라미터의 업데이트는 loss 함수의 local minimum에 도달하여 gradient가 0이 될 때까지 진행하는 것이 맞지만, 실전에서는 검증 데이터셋에서의 성능이 더 이상 증가하지 않는 기점에서 모델 파라미터 업데이트를 중단합니다. . Stochastic Gradient Descent (SGD) . Gradient Descent의 문제점 . Gradient descent 방법으로 최적화 문제를 푸는 것은 이론적으론 무결하지만 현실적으로는 어렵습니다. 앞서 언급했듯이, Gradient 벡터를 계산하기 위해서는 전체 데이터에 대한 loss 값을 구해야 하기 때문에 몇십억개의 데이터를 갖는 오늘날의 데이터셋에 적용하기에는 계산적으로 무리가 있습니다. . . (출처: https://www.slideshare.net/w0ong/ss-82372826) . 따라서 전체 데이터셋을 작은 미니 배치로 나누어 모델 파라미터를 업데이트하는 SGD 방법이 등장하게 되었습니다. 미니 배치 마다 gradient를 구하고 모델 파라미터를 업데이트하는 것이 전체 데이터셋을 활용한 모델 업데이트 정도를 근사할 수 있기 때문이죠. . 💡 SGD is unbiased estimator of full gradient . 오늘날의 최적화 optimizer은 SGD의 여러 발전된 형태로써, Adam, Adagrad, Adadelta, RMSprop 등이 있습니다. . Back-propagation . 지금까지 머신러닝 문제를 최적화 문제로 재정의하고, 최적화 문제를 풀기 위해 gradient를 활용해 모델 파라미터를 업데이트 하는 방법에 대해서 배웠습니다. 잘 따라오고 계신가요? . 이제부터는 실질적으로 머신러닝 모델이 주어졌을 때, gradient를 계산하는 방법에 대해 알아보겠습니다. 오늘날의 머신러닝/딥러닝 모델은 아주 복잡한 형태를 가지지만, 일단 이해의 편의를 돕기 위해 가장 간단한 linear function를 예시로 설명하겠습니다. . Case 1 . . 첫번째로 우리의 머신러닝 모델이 단순한 선형 변환 함수인 경우를 다뤄보겠습니다. . . 선형 변환 함수가 벡터 형태이든 행렬 형태이든 관계 없이 gradient는 이렇게 간단히 구할 수 있습니다. . Case 2 . . . 두번째로 조금 더 복잡한 형태의 모델로 확장해보겠습니다. (물론 아직 엄청 단순한 형태긴 하지만..) 이 경우에는 $W_{1}$과 $W_{2}$에 대해 모두 gradient를 구해야 합니다. 여기서 우리가 고등학교 때 배운 합성함수 미분에서의 연쇄법칙이 사용됩니다. . 💡 Chain Rule (연쇄법칙) . . . . 연쇄법칙에 따라 gradient가 $ mathcal{L}$ → $f(x)$ → $W_{2}$ → $W_{1}$ 을 따라 차례로 거꾸로 흐르면서 계산됩니다. 이렇게 말단에서부터 앞쪽까지 gradient가 흘러오기 때문에 역전파(back-propagation)이라는 이름이 붙었다고 합니다. . Non-linearity . 지금까지 예시로써 다뤄본 두 케이스는 사실 모두 선형 모델로써, 비선형적인 데이터를 잘 모델링할 수 없습니다. 따라서 오늘날의 머신러닝 모델은 비선형적인 활성 함수(Activation function)를 도입함으로써 이러한 문제를 해결합니다. 대표적인 비선형 함수로는 ReLU, Sigmoid 등이 있습니다. . . Multi-layer Perceptron (MLP) . . MLP란 한 layer마다 선형 변환과 비선형 변환이 합쳐진 가장 기본적인 형태의 머신러닝 모델이라고 볼 수 있습니다. 위 식은 MLP 한 layer을 나타내는데, layer $l$ 의 인풋으로 들어온 $x^{(l)}$에 $W_{l}$이 곱해져 선형 변환 된 후 bias 항이 더해집니다. 최종적으로 비선형 함수를 거친 아웃풋이 layer $l+1$의 인풋으로 전달됩니다. 이를 그림으로 나타내면 다음과 같습니다. . . Summary . 지금까지 간단하게 배운 딥러닝의 기본 개념을 정리해보고, 본격적인 오늘 강의의 주제로 넘어가겠습니다. . 머신러닝 문제는 최적화 문제로 풀 수 있습니다. | . . 모델 $f$는 간단한 선형 함수, MLP, 또는 다른 형태의 신경망일 수 있습니다. (e.g., 추후에 다룰 GNN도 가능합니다) | 먼저 전체 데이터셋을 미니배치로 나누어 인풋 $x$으로 사용합니다. | 순전파(Forward Propagation): $x$가 주어졌을 때 loss 함수 값 $ mathcal{L}$ 구하기 | 역전파(Backward Propagation): 연쇄법칙으로 gradient $ nabla_{ theta} mathcal{L}$ 구하기 | SGD를 활용하여 반복적으로 모델 파라미터 $ theta$를 최적화합니다. | . .",
            "url": "https://cs224w-kor.github.io/blog/graph%20neural%20network/gnn/graph%20convolution%20network/gcn/2022/07/27/lecture-0602.html",
            "relUrl": "/graph%20neural%20network/gnn/graph%20convolution%20network/gcn/2022/07/27/lecture-0602.html",
            "date": " • Jul 27, 2022"
        }
        
    
  
    
        ,"post17": {
            "title": "Lecture 6.1 - Introduction to Graph Neural Networks",
            "content": ". Lecture 6. Graph Neural Networks (1) GNN Model . Lecture 6.1 - Introduction to Graph Neural Networks | Lecture 6.2 - Basics of Deep Learning | Lecture 6.3 - Deep Learning for Graphs | . . Recap: Shallow Encoders . 이전 강의에서 배운 내용을 다시 떠올려 봅시다. 다양한 downstream task를 머신러닝으로 푸는 과정에서 비정형 데이터인 그래프 인풋을 활용하기 위해 그래프를 임베딩하는 방법을 공부했었습니다. . Intuition 그래프 상에서 similar한 노드끼리 임베딩 공간에서도 가깝도록 임베딩 하자! . . 임베딩 공간에서의 노드 간 similarity는 간단하게 코사인 유사도를 통해 구할 수 있지만, 1) 원래 그래프 상에서의 노드 간 similarity와 2) 노드를 임베딩 벡터로 mapping하는 encoder은 우리가 새로 정의해야 합니다. . Encoder: Shallow Encoder . . 먼저, 인풋 그래프의 노드를 d차원의 벡터로 임베딩하기 위해 가장 간단한 look-up table 방식인 Shallow Encoder를 다뤘습니다. 예를 들어, 노드의 갯수가 $|V|$인 경우 임베딩 행렬의 크기는 $|V| times d$ 가 됩니다. . | Similarity Function: Random Walk (DeepWalk, Node2vec) 또한, Random Walk상에서 co-occur 되는 두 노드는 그래프 상 similar한 노드라고 정의하였습니다. Random Walk의 전략에 따라 서로 다른 DeepWalk와 Node2vec 등의 방법을 배웠던 것 기억 나시나요? . | Limitations of Shallow Encoder . 이렇듯 Shallow Encoder을 통해서도 성공적으로 노드와 그래프를 임베딩할 수 있지만, 다음과 같은 한계점 때문에 보다 더 고도화된 Encoder를 재정의 할 필요가 있습니다. . $O(|V|)$ 파라미터가 필요함 그래프의 크기가 커지면 커질수록, 즉 노드의 갯수 $|V|$가 증가함에 따라, 임베딩 행렬의 크기도 선형적으로 증가합니다. 또한 각 노드가 모두 서로 다른 d 차원의 임베딩 벡터를 가지기 때문에 파라미터 공유도 일어나지 않습니다. | Transductivity Transductive Learning 그래프 학습 관점에서 Transductive Learning이란 하나의 그래프 상 일부 노드와 엣지의 ground truth를 아는 상태에서 나머지 노드와 엣지의 값을 추정하는 방식입니다. 학습 과정 중, 모델은 ground truth를 알지 못하는 노드를 포함한 모든 노드와 엣지를 사용합니다. . | Inductive Learning 그래프 학습 관점에서 Inductive Learning이란 ground truth를 알고 있는 그래프(들)에 대해 모델을 학습 한 후, 전혀 새로운 그래프의 노드와 엣지의 값을 추정하는 방식입니다. 즉, 학습이 완료된 후에는 모델이 새로운 처음 보는 노드의 값을 추정하는 데에도 적용될 수 있다는 의미이죠. Shallow Encoder은 Transductive Learning으로 학습해야 하는 대표적인 케이스입니다. 학습 도중 보지 못한 노드는 look-up table상 존재할 리 없으니 맵핑되는 임베딩 벡터가 없을 것이고, 임베딩 벡터가 없다면 node classification 등의 downstream task에서 예측이 불가능하겠죠? 이런 특성 때문에 시간에 따라 노드가 추가될 수 있는 evolving 그래프와 같은 경우 그래프가 변할 때마다 전체 임베딩을 다시 scratch부터 학습해야 한다는 불편함이 있습니다. . | . | 노드 feature을 활용하지 않음 대부분의 그래프 데이터셋은 우리가 활용할 수 있는 노드 feature이 존재합니다. 예를 들어, 소셜 그래프의 경우, 단순히 철수가 영희가 친구라는 정보 이외에도, 철수는 성균관대학교에 다니는 23세 남학생이라는 정보도 존재합니다. 단순한 노드 간 연결 상태 이외에도 이러한 노드 feature를 고려하여 노드를 임베딩 한다면 정보량이 더 풍부해져 효과적일 것입니다. | . Deep Graph Encoders . 이제 지금껏 다뤄왔던 간단한 look-up table로 이루어진 encoder에서 벗어나, 좀 더 복잡한 형태의 Deep Encoder을 공부해봅시다. . . Deep Encoder은 인풋 그래프에 수차례의 비선형적인 transformation을 가하여 end-to-end으로 최종 임베딩을 얻는 방식을 말합니다. 수업 슬라이드에 쓰인 말 그대로, . Deep Encoder = multiple layers of non-linear transformations based on graph structure . 로 생각할 수 있겠습니다. 잘 와닿지 않으신다고요? 사실, 인풋 데이터가 우리가 익숙치 않은 형태의 그래프라 그렇지, 오늘날의 머신러닝/딥러닝 모델이 이미지나 텍스트와 같은 정형 데이터를 처리하는 방식과 유사합니다. . . 위의 두 그림이 유사하다는 점이 한눈에 보이실 겁니다. 이해를 돕기 위해 가장 기본적인 CNN 구조를 생각해 볼까요? 원본 이미지가 여러 convolution layer을 거치며 더욱 더 축약된 feature map을 만드는 방식과 유사하게, 인풋으로 들어온 그래프는 여러 graph convolution layer을 거치며 원본 그래프의 의미를 적절히 축약하는 노드 임베딩을 만드는 것입니다. . 또한, 개 고양이 이미지 분류 모델이 지도 학습으로 학습될 때 학습 데이터에 대해 각 이미지가 개인지, 고양이인지 나타내는 클래스 label을 활용하는 것과 같이, 노드 분류 문제의 경우 각 노드에 대한 클래스 label이 있다면 이를 직접 활용하여 encoder를 학습할 수 있습니다. . 💡 이 경우 decoder은 노드 클래스 label 입니다. . 물론, ground truth label이 존재하지 않는 비지도 학습 상황에서는 기존 Shallow Encoder을 학습하던 방법과 동일하게 Random Walk 등으로 정의되는 인풋 그래프상 노드 similarity를 유지하도록 학습할 수도 있겠죠. 이 부분은 본 강의 말에 다시 다루니까 이해되지 않는대도 걱정 마세요! :) . 💡 이 경우 decoder은 임베딩 벡터 간 similarity metric인 벡터 내적 등으로 정의할 수 있습니다. (lecture 4) . 이렇게 Deep Encoder을 통해 얻은 노드/그래프 임베딩은 여러 task에서 agnostic하게 활용할 수 있습니다. . 학습된 임베딩을 활용할 수 있는 여러 task의 예 Node classification | Link prediction | Community detection | Network similarity | . | . Why is it Hard? . 아까 언급했듯이 Deep Encoder을 통해 그래프를 임베딩 한다는 개념은 지금껏 우리가 정형 데이터를 처리했던 방식과 비슷하기 때문에 낯설지 않습니다. . 그렇다면 그냥 널리 사용되고 있는 CNN이나 RNN을 활용해서 그래프를 임베딩 하면 되지 않을까요? . . 그럴 수 없습니다. . 정형 데이터인 이미지, 텍스트에 비해 비정형 데이터인 그래프는 너무나도 복잡하기 때문이죠. 그래프는 이미지와 같이 (0,0) 등의 기준점을 둘 수 없으며, 텍스트와 같이 명백한 순서가 있지도 않습니다. 그래프는 제각기 다른 사이즈 일 수 있으며 각각의 topological structure 또한 모두 다릅니다. 심지어는 노드 마다 multimodal feature을 가질 수도 있습니다. . 따라서, 비정형 그래프 구조에서 각 노드의 구조적 특징 및 노드 feature을 고려하여 적절하게 임베딩 하는 새로운 방법이 필요합니다. . 💡 노드의 Multimodal feature 다시 소셜 그래프를 떠올려 봅시다. 각 노드는 철수, 영희를 포함한 개인을 나타내고, 엣지는 각 개인 사이에 친구 관계가 성립하는지를 나타냅니다. 이 때, 철수라는 노드는 프로필 사진(이미지), 자기 소개 글(텍스트) 등 여러 부가적인 feature을 가질 수 있습니다. . .",
            "url": "https://cs224w-kor.github.io/blog/graph%20neural%20network/gnn/graph%20convolution%20network/gcn/2022/07/27/lecture-0601.html",
            "relUrl": "/graph%20neural%20network/gnn/graph%20convolution%20network/gcn/2022/07/27/lecture-0601.html",
            "date": " • Jul 27, 2022"
        }
        
    
  
    
        ,"post18": {
            "title": "Lecture 5.3 - Collective Classification - Belief Propagation",
            "content": ". Lecture 5 . Lecture 5.1 - Message passing and Node Classification | Lecture 5.2 - Relational and Iterative Classification | Lecture 5.3 - Collective Classification : Belief Propagation | . . 5.3 - Collective Classification : Belief Propagation . Collective Classification : Belief Propagation . Collective Classification Models . Relational classifiers | Iterative classification | Loopy belief propagation | . . 관계 분류자 | 반복구분 | 루피 신앙 전파 | . . 💬 먼저 지금까지 배운 내용으로는 각 노드는 이웃 노드의 확률의 가중평균을 자신의 새로운 확률로 삼고 있다. 혹은 각 노드는 이웃 노드의 레이블을 활용해 자신의 새로운 확률을 계산하게 된다. 즉, 이웃노드의 정보가 각각의 노드에서 사용되고 있는 것이다. 이것을 각 노드는 이웃노드에게 Belief를 전달받는다고 할 수 있다. 즉 이웃 노드의 belief를 받아 자신의 belief를 생성한다. 다르게 말하면, 모델은 각 노드에 대해 데이터마다 belief를 가지고 있고, 이웃 노드의 belief를 이용해 각 노드의 belief를 업데이트하고 있다. 그렇다면 왜 굳이 바로 이웃노드에서만 belief를 받아야 할까. 좀 더 먼 노드의 belief도 중요하게 작동하지 않을까? 왜냐하면 결국 이터레이션을 반복하여 이웃노드의 belief를 받게 된다면, 해당 belief는 이웃노드의 이웃노드의 belief가 섞여있는 상태기 때문에, 이터레이션을 반복한다는 것은 자신의 이웃노드의 이웃노드의 이웃노드의 .... belief를 받고 있는 것이기 때문이다. 이를 역으로 생각하여 belief가 그래프에 직접 흐르도록 알고리즘을 구성한 것이 loopy belief propagation이 된다. Loopy Belief Propagation . Belief Propagation is a dynamic programming approach to answering probability queries in a graph (e.g. probability of node $v$ belonging to class 1 ) | Iterative process in which neighbor nodes “talk” to each other, passing messages . 📌 &quot;I (node v) believe you (node u) belong to class 1 with likelihood ...&quot; | When consensus is reached, calculate final belief | . . 믿음 전파는 그래프에서 확률 쿼리에 응답하는 동적 프로그래밍 접근법이다(예: 클래스 1에 속하는 노드 $v$의 확률). | 인접 노드가 메시지를 전달하면서 서로 “대화”하는 반복 프로세스 . 📌 &quot;나(노드 v)는 (노드 u)가 클래스 1에 속한다고 믿고 있습니다.” | 합의가 이루어지면 최종 믿음을 계산합니다. | . . 💬 Belief Propagation 확률 쿼리에 응답하기 위한 동적 프로그래밍 접근법이다. 또한 반복적으로 이웃노드와 &#39;talk&#39;하면서 메시지를 전달하는 방법이다. 따라서 주된 아이디어는 각 노드는 메시지를 이웃으로부터 얻는다. 그리고 업데이트하고 앞으로 전달한다. Message Passing: Basics . Task: Count the number of nodes in a graph* | Condition: Each node can only interact (pass message) with its neighbors | Example: path graph | . . 과제: 그래프의 노드 수 계산* | 조건: 각 노드는 인접 노드와만 상호 작용(메시지 전달)할 수 있습니다. | 예: 경로 그래프 | . . Potential issues when the graph contains cycles. . We’ll get back to it later! . . 그래프에 주기가 포함되어 있을 때 발생할 수 있는 문제. 나중에 다시 얘기하자! . . 💬 위와 같이 가장 단순한 그래프의 형태를 생각해보자. 우리가 원하는 것은 그래프의 노드 수를 계산하고자 한다. 이때 belief는 각 이터레이션마다 이웃 노드로만 전달될 수 있다. Message Passing: Algorithm . Task: Count the number of nodes in a graph | Algorithm: Define an ordering of nodes (that results in a path) | Edge directions are according to order of nodes Edge direction defines the order of message passing | . | For node $i$ from 1 to 6 Compute the message from node $i$ to $i+1$ (number of nodes counted so far) | Pass the message from node $i$ to $i+1$ | . | . | . . 과제: 그래프의 노드 수 계산 | 알고리즘: 노드 순서 정의(경로 생성) | 에지 방향은 노드 순서에 따릅니다. 에지 방향은 메시지 전달 순서를 정의합니다. | . | 노드 $i$의 경우 1 ~ 6까지 노드 $i$에서 $i+1$로 메시지 계산 (지금까지 카운트된 노드 수) | 노드 $i$에서 $i+1$로 메시지 전달 | . | . | . . . 💬 알고리즘은 다음과 같을 것이다. 1. 노드의 순서를 정한다. 2. 1에서 정한 순서에 따라 엣지의 방향을 정한다. 3. $i$번째 노드에 대해 다음을 시행한다. $i-1$ 노드에서 belief(이전까지 지나온 노드의 수)를 받는다. belief에 $1$(자신에 대한 count)을 더한다. $i+1$ 노드로 belief를 전달한다. . Message Passing: Basics . Task: Count the number of nodes in a graph Condition: Each node can only interact (pass message) with its neighbors Solution: Each node listens to the message from its neighbor, updates it, and passes it forward $m$ : the message . . 작업: 그래프의 노드 수 계산 조건: 각 노드는 인접 노드와만 상호 작용(메시지 전달)할 수 있습니다. 솔루션: 각 노드는 인접 노드로부터 메시지를 수신하고 업데이트한 후 전달 $m$ : 메시지 . . . 💬 그래프의 노드의 수를 세는 방법을 구하기 위해서 위에서 본 알고리즘을 따라 먼저 노드의 순서와 그에 따른 방향을 정해준다. 그리고 난뒤로 이전 노드로 부터 message를 전달받고(listen) 현재노드에서 메시지를 업데이트하고 앞의 노드로 전달해준다. Generalizing to a Tree . We can perform message passing not only on a path graph, but also on a tree-structured graph | Define order of message passing from leaves to root | . . 경로 그래프뿐만 아니라 트리 구조 그래프에서도 메시지 전달을 수행할 수 있습니다. | 리프에서 루트로 전달되는 메시지 순서 정의 | . . . 💬 같은 알고리즘을 위와 같은 트리 구조에 적용한다. 트리는 parent와 child로 구성되어 있기 때문에, 전체 노드의 수를 세기 위해서 child에서 parent 방향으로 belief가 흐르면 될 것이다. Message passing in a tree . Update beliefs in tree structure . 트리 구조의 신뢰 업데이트 . . . 💬 이때 왼쪽 그림과 같이 parent 노드는 자신의 child 노드의 belief를 받아 종합하는 일종의 계산을 수행한 수 자신의 parent 노드로 belief를 넘겨주게 된다. 이를 통해 최종적으로 root 노드에서 전체 노드의 수를 구할 수 있을 것이다. 하지만 실제로 count를 belief로 간주하고 이웃 노드에 전달하지 않을 것이다. 실제로 알고리즘이 어떻게 되어 있는지 살펴보자. Loopy BP Algorithm . What message will $i$ send to $j$ ? . It depends on what $i$ hears from its neighbors | Each neighbor passes a message to $i$ its beliefs of the state of $i$ | . . $i$가 $j$에 보낼 메시지는 무엇입니까? . $i$가 이웃으로부터 무엇을 듣느냐에 따라 달라진다. | 각 이웃은 $i$ 상태에 대한 믿음을 $i$에 전달한다. | . . 📌 I (node $i$ ) believe that you (node $j$ ) belong to class $Y_{j}$ with probability $ cdots$ 📌 I(노드 $i$)는 당신(노드 $j$)이 확률로 클래스 $Y_{j}$에 속한다고 믿는다$ cdots$ . 💬 일반적으로는 여러 노드의 정보를 $i$노드로 전달하고(hear) $i$노드에서 $j$노드로 전달한다. Notation . Label-label potential matrix $ psi$ : Dependency between a node and its neighbor. $ boldsymbol{ psi} left(Y_{i}, Y_{j} right)$ is proportional to the probability of a node $j$ being in class $Y_{j}$ given that it has neighbor $i$ in class $Y_{i}$. | Prior belief $ phi: phi left(Y_{i} right)$ is proportional to the probability of node $i$ being in class $Y_{i}$. | $m_{i rightarrow j} left(Y_{j} right)$ is $i^{ prime}$ s message / estimate of $j$ being in class $Y_{j}$. | $ mathcal{L}$ is the set of all classes/labels | . . 레이블 레이블 잠재적 매트릭스 $ psi$ : 노드와 인접 노드 간의 종속성. $ boldsymbol{ psi} left(Y_{i}, Y_{j} right)$는 노드 $j$가 클래스 $Y_{j}$에 있을 확률에 비례한다. | $ phi: phi left(Y_{i} right)$는 노드 $i$가 클래스 $Y_{i}$에 포함될 확률에 비례한다. | $m_{i rightarrow j} left(Y_{j} right)$는 클래스 $Y_{j}$에 속하는 $j$의 $i^{ prime}$ 메시지/추정이다. | $ mathcal{L}$ 는 모든 클래스/라벨의 집합입니다. | . . 💬 **Notation** - $ psi$ (Label-Label Potential Matrix) : $ psi$ 는 각 노드가 이웃노드의 클래스에 대한 영향력(비례)을 행렬로 표현한 것이다. - 예를 들어 $ psi left(Y_{i}, Y_{j} right)$ 는 이웃 노드 i의 레이블이 $Y_{i}$ 일 때, 노드 $ mathrm{j}$ 가 $Y_{j}$ 레이블에 속할 확률의 비중이다. - 만약 $i$와 $j$가 Homophily가 존재한다면(같은 class를 가진다면) 대각원소들의 크기는 높을것이다. - 또한 이 행렬을 얻기위해서는 학습이 필요하다. - $ phi$ (Prior Belief) : 노드 $i$가 $Y_{i}$ 에 속할 확률에 비례한다. - $m_{i rightarrow j} left(Y_{j} right)$ : $i$의 메세지가 $j$로 전달되는 것을 의미하는데, $i$가 이웃 노드로 부터 받은 belief와 자신의 정보를 종합해 $j$의 레이블을 believe하는 것을 의미한다. - $j$의 노드를 예측할 수 있도록 $i$에서 $j$로 전달하는 메시지이다. - $L:$ 모든 레이블(클래스)을 포함하는 집합 - $b_{i} left(Y_{i} right)$ : 노드 i의 클래스가 $Y_{i}$ 일 belief Loopy BP Algorithm . Initialize all messages to 1 | Repeat for each node: | . 모든 메시지를 1로 초기화 | 각 노드에 대해 반복합니다. | . . After convergence: $b_{i} left(Y_{i} right)=$ node $i$ ‘s belief of being in class $Y_{i}$ . 수렴 후: $b_{i} left(Y_{i} right)=$ 노드 $i$의 클래스 $Y_{i}$에 대한 믿음 . . . 1. 가장 처음에는 모든 노드의 메세지를 1로 초기화한다. 2. 이후 가운데 이미지와 같이 모든 노드에 대해 다음 노드로 메세지를 전달하는 과정을 반복한다. 이때 가운데 이미지의 수식을 설명해보자면, 가장 앞의 분홍색 부분은 현재 노드 $i$의 모든 레이블의 가능성에 대해 반복하여 더한다는 의미이다. 녹색 부분은 label-label potential로서, $i$노드의 각 레이블마다 $j$노드가 $Y_{j}$ 레이블을 가질 확률을 계산하게 된다. 적색 부분은 Prior로서 $i$노드가 $Y_{i}$ 레이블을 가질 확률을 계산하게 된다. 청색 부분은 $i$ 노드가 메세지를 넘겨받는 이웃 노드에서 $i$ 노드가 $Y_{i}$ 레이블일 belief를 넘겨 받는 부분이다. 만약 위의 과정이 충분히 반복되어 수렴한다면 세번째 이미지에 해당하는 실제 확률 $[b_{i} left(Y_{i} right)]$이 계산되게 된다. 1. 즉, Prior 확률에 belief를 모두 곱하여 최종적인 belief ($[b_{i} left(Y_{i} right)]$) 를 결정한다. 💡 Q: 수렴 하는 것이 무엇인지? 무엇이 수렴하는 것인지 상황에 대한 질문 Example: Loopy Belief Propagation . Now we consider a graph with cycles | There is no longer an ordering of nodes | We apply the same algorithm as in previous slides: Start from arbitrary nodes | Follow the edges to update the neighboring nodes | . | . What if our graph has cycles? Messages from different subgraphs are no longer independent! But we can still run BP, but it will pass messages in loops. . . 이제 주기가 있는 그래프를 살펴봅시다. | 더 이상 노드 순서가 없습니다. | 이전 슬라이드와 동일한 알고리즘을 적용합니다. 임의 노드에서 시작 | 가장자리를 따라 인접 노드를 업데이트합니다. | . | . 만약 우리 그래프에 주기가 있다면? 다른 하위 그래프의 메시지는 더 이상 독립적이지 않습니다! 하지만 BP는 여전히 실행할 수 있지만 메시지를 루프 형태로 전달합니다. . . . 💬 지금까지 이야기한 그래프들은 순환하는 구조를 가지고 있지 않아 메세지를 전달할 순서를 정하는데 문제가 없었다. 하지만 순환하는 구조를 가지는 그래프의 경우에는 단순하게 노드의 순서를 정해서 메세지를 전달하도록 만들 수 없다 그에 대해 자세히 살펴보자. 만약 위와 같은 그래프가 있고, 위와 같은 순서로 메세지를 주고 받는다고 생각해보자. $u$ 노드는 $k$에게 메세지를 받는 것처럼 보이지만, 실제로는 자기 자신의 메세지마저 받고 있는 상황이다. 즉, 더 이상 모든 노드가 독립적이지 않고, 의존성이 생긴다. 순서가 반대로 트리와 같이 $j$가 $i, k$로 메세지를 전달하고, $i, k$가 $u$로 메세지를 전달한다면, $j$의 메세지는 $u$에게 중복되어 두 번 전달되는 문제가 생긴다. 이렇게 되면 알고리즘이 크게 문제가 생기는 것 같지만, 실제 적용해보니 그렇지 않다고 한다. 실제 그래프들은 무척 크고, 거기에 순환하는 cycle 구조는 그렇게 큰 부분을 차지하지 않는데 반면, 전체 구조는 매우 복잡하기 때문에 Loopy BP 알고리즘이 잘 작동한다고 한다. What Can Go Wrong? . Beliefs may not converge | Message $m_{u rightarrow i} left(Y_{i} right)$ is based on initial belief of $i$, not a separate evidence for $i$ | The initial belief of $i$ (which could be incorrect) is reinforced by the cycle | . i→j→k→u→ii rightarrow j rightarrow k rightarrow u rightarrow ii→j→k→u→i . However, in practice, Loopy BP is still a good heuristic for complex graphs which contain many branches. | . . 신념이 수렴되지 않을 수 있다. | 메시지 $m_{u rightarrow i} left(Y_{i} right)$는 $i$에 대한 별도의 증거가 아니라 $i$의 초기 믿음에 기초한다. | (잘못될 수 있음) $i$의 초기 신념은 주기에 의해 강화된다. | . i→j→k→u→ii rightarrow j rightarrow k rightarrow u rightarrow ii→j→k→u→i . 그러나 실제로 Loopy BP는 많은 분기를 포함하는 복잡한 그래프에 여전히 좋은 휴리스틱이다. | . . . Messages loop around and around: $2,4,8,16,32, ldots$ More and more convinced that these variables are $T$ ! | BP incorrectly treats this message as separate evidence that the variable is $ mathrm{T}$!. | Multiplies these two messages as if they were independent. But they don’t actually come from independent parts of the graph. | One influenced the other (via a cycle). | . | . . 메시지는 돌고 돈다: $2,4,8,16,32, ldots$ 이러한 변수가 $T$임을 점점 더 확신하게 된다! | BP는 이 메시지를 변수가 $ mathrm{T}$라는 별도의 증거로 잘못 취급한다. | 이 두 메시지를 독립한 것처럼 곱합니다. 하지만 그것들은 사실 그래프의 독립적인 부분에서 나온 것이 아닙니다. | 한 사람이 다른 사람에게 영향을 주었다. | . | . . This is an extreme example. Often in practice, the cyclic influences are weak. (As cycles are long or include at least one weak correlation.) . . 이것은 극단적인 예입니다. 실제로, 주기적인 영향은 약하다. (주기가 길거나 하나 이상의 약한 상관 관계를 포함하기 때문에) . Advantages of Belief Propagation . Advantages: Easy to program &amp; parallelize | General: can apply to any graph model with any form of potentials Potential can be higher order: e.g. $ boldsymbol{ psi} left(Y_{i}, Y_{j}, Y_{k}, Y_{v} ldots right)$ | . | . | Challenges: Convergence is not guaranteed (when to stop), especially if many closed loops | . | Potential functions (parameters) Require training to estimate | . | . . 장점: 프로그래밍 및 병렬화가 용이함 | 일반: 모든 형태의 잠재력이 있는 그래프 모델에 적용할 수 있습니다. 잠재력은 고차일 수 있다. 예를 들어 $ boldsymbol{ psi} left(Y_{i}, Y_{j}, Y_{k}, Y_{v} ldots right)$ | . | . | 과제: 특히 닫힌 루프가 많은 경우 수렴이 보장되지 않습니다(정지 시기). | . | 잠재적 함수(모수) 평가하려면 교육 필요 | . | . . 💬 **Advantages:** 1. 코딩이 쉽고, 병렬화가 가능하다. 2. 어떠한 그래프 모델이더라도, potential matrix를 구성할 수 있으므로 범용적이다. **Challenges:** 1. 수렴이 보장되지 않아 언제 멈춰야 할지 알 수 없으며, 특히 순환구조일 경우 수렴이 보장되지 않아 반복횟수를 지정해주어야 한다. 2. cycle 구조로 인해 종종 적은 이터만 돌리기도 한다. Summary . We learned how to leverage correlation in graphs to make prediction on nodes | Key techniques: Relational classification | Iterative classification | Loopy belief propagation | . | . . 우리는 그래프의 상관 관계를 활용하여 노드에 대한 예측을 하는 방법을 배웠다. | 주요 기술: 관계구분 | 반복구분 | 루피 신앙 전파 | . | . . CS224W: Machine Learning with Graphs 2021 Lecture 5.3 - Collective Classification .",
            "url": "https://cs224w-kor.github.io/blog/graph%20neural%20network/gnn/graph%20convolution%20network/gcn/2022/07/20/lecture-0503.html",
            "relUrl": "/graph%20neural%20network/gnn/graph%20convolution%20network/gcn/2022/07/20/lecture-0503.html",
            "date": " • Jul 20, 2022"
        }
        
    
  
    
        ,"post19": {
            "title": "Lecture 5.2 - Relational and Iterative Classification",
            "content": ". Lecture 5 . Lecture 5.1 - Message passing and Node Classification | Lecture 5.2 - Relational and Iterative Classification | Lecture 5.3 - Collective Classification : Belief Propagation | . . 5.2 - Relational and Iterative Classification . Relational Classification . Probabilistic Relational Classifier . Idea: Propagate node labels across the network Class probability $Y_v$ of node $v$ is a weighted average of class probabilities of its neighbors. | . | For labeled nodes $v$, initialize label $Y_v$ with ground-truth label $Y^*_v$. | For unlabeled nodes, initialize $Y_v$ = 0.5. | Update all nodes in a random order until convergence or until maximum number of iterations is reached. | Update for each node $v$ and label $c$ (e.g. 0 or 1 ) . P(Yv=c)=1∑(v,u)∈EAv,u∑(v,u)∈EAv,uP(Yu=c)P left(Y_{v}=c right)= frac{1}{ sum_{(v, u) in E} A_{v, u}} sum_{(v, u) in E} A_{v, u} P left(Y_{u}=c right)P(Yv​=c)=∑(v,u)∈E​Av,u​1​(v,u)∈E∑​Av,u​P(Yu​=c) If edges have strength/weight information, $A_{v, u}$ can be the edge weight between $v$ and $u$ | $P left(Y_{v}=c right)$ is the probability of node $v$ having label $c$ | . | Challenges: Convergence is not guaranteed | Model cannot use node feature information | . | . . 아이디어: 노드 레이블을 네트워크에 전파 노드 $v$의 클래스 확률 $Y_v$는 이웃의 클래스 확률에 대한 가중 평균이다. | . | 레이블링된 노드 $v$의 경우 지상 실측 레이블 $Y^*_v$로 레이블 $Y_v$를 초기화한다. | 레이블이 없는 노드의 경우 $Y_v$ = 0.5를 초기화합니다. | 수렴할 때까지 또는 최대 반복 횟수에 도달할 때까지 모든 노드를 임의의 순서로 업데이트합니다. | 각 노드 $v$ 및 레이블 $c$에 대한 업데이트(예: 0 또는 1) . P(Yv=c)=1∑(v,u)∈EAv,u∑(v,u)∈EAv,uP(Yu=c)P left(Y_{v}=c right)= frac{1}{ sum_{(v, u) in E} A_{v, u}} sum_{(v, u) in E} A_{v, u} P left(Y_{u}=c right)P(Yv​=c)=∑(v,u)∈E​Av,u​1​(v,u)∈E∑​Av,u​P(Yu​=c) 가장자리의 강도/무게 정보가 있는 경우 $A_{v,u}$는 $v$와 $u$ 사이의 에지 가중치일 수 있습니다. | $P left(Y_{v}=c right)$는 노드 $v$가 $c$ 레이블을 가질 확률이다. | . | 과제: 수렴이 보장되지 않음 | 모델은 노드 피쳐 정보를 사용할 수 없습니다. | . | . . 💬 기본 아이디어 : 노드 $v$의 레이블 확률 $Y_v$는 노드 $v$의 주변노드의 레이블 확률의 가중평균과 같다. 즉, 레이블이 없는 노드에 대해 이웃 노드들의 레이블 확률을 가중평균하여 예측하게 된다. 이진 분류문제라 가정하고, 모든 노드에 레이블 확률이 존재해야 하기 때문에, 레이블이 없는 노드는 0.5의 확률로 초기화하여 시작하게 된다. 업데이트는 반복적으로 진행되며, 모든 노드에 대해 수렴하거나 반복횟수에 도달할 경우 멈추게 된다. 노드 $*v*$에 대한 확률을 계산하는 법은 위 수식과 같다. 이때 행렬 A는 인접행렬에 해당한다. 즉, 주변의 이웃노드의 확률 $*P left(Y_{v}=c right)*$를 평균하여 사용하되, 해당 이웃노드와 연결된 degree만큼 가중치, $*A_{v,u}*$를 주게 된다. 이때 두가지 문제점이 있다. 1. 위 수식은 수렴이 보장되지 않는다. 2. 모델이 노드의 변수를 활용하지 않는다. 이에 대해선 이후 모델을 통해 개선될 것이라 기대해보자. Example: Initialization . Initialization: . All labeled nodes with their labels | All unlabeled nodes 0.5 (belonging to class 1 with probability 0.5) | . . 초기화: . 라벨이 표시된 모든 노드 | 표시되지 않은 모든 노드 0.5 (확률 0.5의 클래스 1에 속함) | . . 💬 위 그래프에서 본래 레이블이 있는 노드는 녹색과 적색으로 표시가 되어 있다. 이에 대해 이진분류 문제이기 때문에, 녹색을 기준으로 확률을 계산하여, 녹색 노드는 1, 적색 노드는 0, 레이블이 없는 회색노드는 0.5로 초기화한다. Example: $1^{st}$ Iteration, Update Node 3 . Update for the $1^{st}$ Iteration: For node 3, $N_3=[1,2,4]$ | . | . . $1^{st}$ 반복에 대한 업데이트: 노드 3의 경우 $N_3=[1,2,4]$ | . | . . 💬 각 노드별로 이웃노드의 확률을 이용해 순차적으로 확률을 업데이트한다. 위 그래프의 경우 undirected이고 엣지가 각 노드 간 최대 하나만 존재하기 때문에 단순 평균을 통해 새로운 확률을 계산하게 된다. Example: $1^{st}$ Iteration, Update Node 4 . Update for the $1^{st}$ Iteration: For node 4, $N_4=[1,3,5,6]$ | . | . . $1^{st}$ 반복에 대한 업데이트: 노드 4의 경우 $N_4=[1,3,5,6]$ | . | . . 💬 위에서 업데이트된 3번 노드의 확률을 이용해 4번 노드 역시 업데이트하게 된다. 즉, 노드를 업데이트하는 순서에 따라 계산이 조금씩 달라지게 된다. Example: $1^{st}$ Iteration, Update Node 5 . Update for the $1^{st}$ Iteration: For node 5, $N_5=[4,6,7,8]$ | . | . . $1^{st}$ 반복에 대한 업데이트: 노드 5의 경우 $N_5=[4,6,7,8]$ | . | . . 💬 3, 4번 노드를 업데이트하고 나서 5번 노드를 업데이트한다. 이때 역시 업데이트된 4번 노드의 확률을 이용하게 된다. Example: After $1^{st}$ Iteration . After Iteration 1 (a round of updates for all unlabeled nodes) 반복 후 1 (라벨이 지정되지 않은 모든 노드에 대한 업데이트 라운딩) . . 💬 첫번째 이터가 종료된 후의 모습니다. 9번 노드의 경우 녹색 노드만 연결되어 있기 때문에 확률이 1로 고정된 모습을 보이고 있다. 이외에 8번 노드 역시 주변에 녹색 노드 2개, 확률이 높은 노드(5번) 한개가 이웃노드이기 때문에 확률이 높은 것을 볼 수 있다. 하지만 4번 노드의 경우 녹색 노드와 적색 노드 각각 하나씩 연결되어 있고, 녹색과 가까운 노드와 적색과 가까운 노드 하나씩 연결되어 있어 0.5에 가까운 확률을 보이고 있다. Example: After $2^{nd}$ Iteration . After Iteration 2 (반복 후 2) . . Example: After $3^{rd}$ Iteration . After Iteration 3 (반복 후 3) . . Example: After $4^{th}$ Iteration . After Iteration 4 (반복 후 4) . . 💬 이후 0.5보다 확률이 큰 노드들은 class 1이라 분류하고, 0.5보다 작은 노드들은 class 0으로 분류한다. class 0 : 1, 2, 3 class 1 : 4, 5, 6, 7, 8, 9 Example: Convergence . All scores stabilize after 4 iterations. . We therefore predict: . Nodes 4,5,8,9 belong to class 1 ($𝑃_{Y_v}$ &gt; 0.5) | Nodes 3 belong to class 0 ($𝑃_{Y_v}$ &lt; 0.5) | . | . . 4회 반복 후 모든 점수가 안정됩니다. . 따라서 다음과 같이 예측한다. . 노드 4,5,8,9가 클래스 1($𝑃_{Y_v}$ &gt; 0.5)에 속함 | 노드 3은 클래스 0($𝑃_{Y_v}$ &lt; 0.5)에 속합니다. | . | . . 💬 몇 번의 iterations을 지나자 모든 확률값이 수렴하고 있는 모습을 보여 종료되었으나 이 모델의 수렴이 보장되지 않는 단점이 존재한다. 이를 이전에 배웠던 개념과 연결지어 생각하자면, Influence가 녹아있는 모델이라고 할 수 있겠다. 가까운 노드의 영향을 받아 이웃 노드와 비슷한 레이블 분포를 가지도록 업데이트하고 있기 때문이다. 그 결과 8번 노드는 이웃노드가 녹색일 확률이 높으니 해당 분포와 비슷해지고, 4번 노드는 이웃노드로 녹색과 적색에 가까운 노드들이 모두 있어 0.5에 가까운 확률을 가지게 되었다. Relational Classification은 그래프의 구조적 정보를 일부 활용하고, 노드 레이블은 활용하지만, 노드의 변수 = node featured information(attributes)를 활용하지 못한다는 단점이 크게 작용한다. 단지 노드의 라벨과 이웃들의 엣지를 이용하는 것인 네트워크 정보만 사용하게 된다. 결국 주어진 정보를 최대한 활용하지 못하는 머신러닝 모델은 부족한 점이 많은 모델일 뿐이다. Iterative Classification . Iterative Classification . Relational classifier does not use node attributes. | How can one leverage them? | Main idea of iterative classification: . Classify node $v$ based on its attributes $f_v$ as well as labels $z_v$ of neighbor set $N_v$. . | Input: Graph $f_{v}$ : feature vector for node $v$ | Some nodes $v$ are labeled with $Y_{v}$ | . | Task: Predict label of unlabeled nodes | Approach: Train two classifiers: $ phi_{1} left(f_{v} right)=$ Predict node label based on node feature vector $f_{v}$. This is called base classifier. | $ phi_{2} left(f_{v}, z_{v} right)=$ Predict label based on node feature vector $f_{v}$ and summary $z_{v}$ of labels of $v’{ text {s }}$ neighbors. . This is called relational classifier. . | . . 관계 분류자가 노드 특성을 사용하지 않습니다. | 어떻게 그들을 활용할 수 있을까? | 반복 분류의 주요 개념: . 노드 $v$는 속성 $f_v$와 인접 세트 $N_v$의 레이블 $z_v$를 기준으로 분류한다. . | 입력: 그래프 $f_{v}$ : 노드 $v$의 피쳐 벡터 | 일부 노드 $v$는 $Y_{v}$로 레이블 지정됨 | . | 작업: 레이블이 없는 노드의 레이블 예측 | 접근법: 두 가지 분류기 훈련: $ phi_{1} left(f_{v} right)=$ 노드 특징 벡터 $f_{v}$를 기반으로 노드 레이블을 예측한다. 이를 기본 분류기라고 합니다. | $ phi_{2} left(f_{v}, z_{v} right)=$ 노드 특징 벡터 $f_{v}$와 $v’{ text {s}}$ 이웃 레이블의 요약 $z_{v}$를 기반으로 레이블을 예측한다. . 이를 관계 분류기라고 합니다. . | . . 💬 Relational Classifier는 노드의 변수를 활용하지 않는 것이 단점이라고 했다. Iterative Classifier는 노드의 변수를 활용하여 이를 개선했다. 핵심 아이디어는 다음과 같다. &gt; 노드 $`v`$를 분류할 때, 노드의 변수 $`f_v`$를 이웃노드 집합 $`N_v`$의 레이블 $`z_u`$와 함께 사용하자. 입력은 그래프를 사용하며, $f_v$는 노드 $v$의 변수 벡터를 의미하며, 여기서 일부 노드는 레이블 $Y_v$를 갖는다. 목표는 레이블이 없는 노드에 대해 레이블을 예측하는 것이다. 이를 위해 활용하는 것은 두 개의 분류기를 활용한다. 1. $ phi_{1} left(f_{v} right)$ : 노드 $v$의 변수 $f_v$ 만을 이용해 레이블을 예측하는 모델 2. $ phi_{2} left(f_{v}, z_{v} right)$: 노드 $v$의 변수 $*f_v*$와 이웃 노드의 레이블에 대한 기술 통계벡터 $*z_v*$를 이용하여 레이블을 예측하는 모델 $z_u$: 이웃 노드의 라벨을 표현하는 벡터 (라벨의 비율, 개수 등으로 표현된다.) Computing the Summary $z_v$ . How do we compute the summary $z_v$ of labels of $v’s$ neighbors $N_v$? . $z_v$ = vector that captures labels around node $v$ Histogram of the number (or fraction) of each label in $N_v$ | Most common label in $N_v$ | Number of different labels in $N_v$ | . | . . $v$의 인접 $N_v$ 레이블의 요약 $z_v$는 어떻게 계산합니까? . $z_v$ = 노드 $v$ 주변의 레이블을 캡처하는 벡터 $N_v$ 단위의 각 레이블의 숫자(또는 부분) 히스토그램 | $N_v$의 가장 일반적인 레이블 | $N_v$의 서로 다른 레이블 수 | . | . . 💬 위와 같은 그래프에서 청색 노드에 대한 $z_{v}$ 는 이웃 노드의 색의 count 분포나 존재 유무 분포, 비율 분포 등을 사용해 만들 수 있다. - count 분포 : [녹색 노드의 수, 적색 노드의 수] = $[2,1]$ - 존재 유무 분포 : [녹색 노드 존재 여부, 적색 노드 존재 여부]= $[1,1]$ - 비율 분포 : [녹색 노드 비율, 적색 노드 비율] =$ left[ frac{2}{3}, frac{1}{3} right]$ 두 분류기를 이용하여 학습과 예측 과정이 조금 복잡한데 크게 두 단계로 나눌 수 있다. Architecture of Iterative Classifiers . Phase 1: Classify based on node attributes alone On the labeled training set, train two classifiers: Base classifier: $ phi_{1} left(f_{v} right)$ to predict $Y_{v}$ based on $f_{v}$ | Relational classifier: $ phi_{2} left(f_{v}, z_{v} right)$ to predict $Y_{v}$ based on $f_{v}$ and summary $z_{v}$ of labels of $v^{ prime}$s neighbors | . | . | Phase 2: Iterate till convergence On test set, set labels $Y_{v}$ based on the classifier $ phi_{1}$, compute $z_{v}$ and predict the labels with $ phi_{2}$ | Repeat for each node $v$ : Update $z_{v}$ based on $Y_{u}$ for all $u in N_{v}$ | Update $Y_{v}$ based on the new $z_{v} left( phi_{2} right)$ | . | Iterate until class labels stabilize or max number of iterations is reached | Note: Convergence is not guaranteed | . | . . 1단계: 노드 속성만을 기준으로 분류 라벨이 부착된 교육 세트에서 두 가지 분류기를 교육합니다. 기본 분류자: $f_{v}$를 기반으로 $Y_{v}$를 예측하기 위한 $ phi_{1} left(f_{v} right)$ | 관계 분류자: $f_{v}$와 $v^{ prime}$s 인접 레이블의 요약 $z_{v}$를 기반으로 $Y_{v}$를 예측하기 위한 $ phi_{2} left(f_{v}, z_{v} right)$ | . | . | 2단계: 수렴될 때까지 반복 테스트 세트에서 $ phi_{1}$ 분류기를 기반으로 레이블 $Y_{v}$을 설정하고 $z_{v}$를 계산하고 $ phi_{2}$로 레이블을 예측한다. | 각 노드 $v$에 대해 반복: 모든 $u in N_{v}$에 대해 $Y_{u}$를 기준으로 $z_{v}$ 업데이트 | 새 $z_{v} left( phi_{2} right)$를 기준으로 $Y_{v}$ 업데이트 | . | 클래스 레이블이 안정되거나 최대 반복 횟수에 도달할 때까지 반복 | 참고: 수렴이 보장되지 않음 | . | . . 💬 Phase 1 : **Train** 학습 데이터의 경우에 모든 노드에 레이블이 달려있다고 간주하고 두 분류기를 학습한다. 1. $ phi_{1} left(f_{v} right): f_{v}$ 를 이용해 $Y_{v}$ 를 예측한다. : 노드 피쳐 정보들만을 이용하여 노드라벨을 예측하는 모델 2. $ phi_{2} left(f_{v}, z_{v} right): f_{v}$ 와 $z_{v}$ 를 이용해 $Y_{v}$ 를 예측한다. 이때, $z_{v}$ 는 실제 레이블을 이용해 구성한다. : 노드 피쳐 정보 + 이웃들의 라벨정보를 이용하여 노드 라벨을 예측하는 모델 Phase 2 : **Inference** 테스트 데이터의 경우엔 일부 노드에만 레이블이 달려있다고 간주한다. 혹은 레이블이 아예 없을 수도 있다고 간주한다. 이때 $ phi_{1} left(f_{v} right)$ 는 $f_{v}$ 가 변하지 않기 때문에 초기에 한번만 계산하여 라벨 $Y_{v}$ 를 예측한다. 이를 통해 모든 노드에 $Y_{v}$ 가 할당된다. 모든 노드에 $Y_{v}$ 가 할당된 후 다음과 같은 과정을 수렴하거나 최대반복횟수에 도달할 때까지 반복한다. 1. 새로운 $Y_{v}$ 에 맞추어 $z_{u}$ 를 업데이트한다. $ left(u in N_{v} right)$ 2. 새로운 $z_{u}$ 에 맞추어 $Y_{z}= phi_{2} left(f_{v}, z_{v} right)$ 를 업데이트한다. 다만, 해당 모델 또한 수렴을 보장하지 않기에, 최대 반복 횟수를 지정한다. Example: Web Page Classification . Input: Graph of web pages | Node: Web page | Edge: Hyper-link between web pages Directed edge: a page points to another page | . | Node features: Webpage description For simplicity, we only consider two binary features | . | Task: Predict the topic of the webpage | Baseline: Train a classifier (e.g., linear classifier) to classify pages based on node attributes. | . . 입력: 웹페이지 그래프 | 노드: 웹 페이지 | 가장자리: 웹 페이지 간의 하이퍼링크 (Directed Edge) 방향 가장자리: 한 페이지가 다른 페이지를 가리키다 | . | 노드 기능: 웹 페이지 설명 (TF-IDF 등의 토큰 정보, 여기선 2차원 벡터로 표현) 단순성을 위해 두 개의 이진 기능만 고려한다. | . | 작업: 웹 페이지의 주제 예측 | 기준: 노드 속성을 기준으로 페이지를 분류하도록 분류기(예: 선형 분류기)를 훈련합니다. | . . . 💬 Web Page의 주제를 예측하여 분류하기 위해서 아래의 데이터가 사용된다. - Input : 웹페이지 그래프 - Node : 웹페이지 - Edge : 웹페이지 간 하이퍼링크(Directed Edge) - Node Features : 웹페이지 정보(TF-IDF 등의 토큰 정보, 여기선 2차원 벡터로 표현) - Task : 각 웹페이지의 주제 예측 . Each node maintains vectors $z_v$ of neighborhood labels: $I$ = Incoming neighbor label information vector. | $O$ = Outgoing neighbor label information vector. $I_0$ = 1 if at least one of the incoming pages is labelled 0. . Similar definitions for $I_0$, $O_0$, and $O_1$ . | . | . | . . 각 노드는 이웃 레이블의 벡터 $z_v$를 유지한다. $I$ = 들어오는 인접 레이블 정보 벡터. | $O$ = 나가는 인접 레이블 정보 벡터. $I_0$ = 1 (수신 페이지 중 하나 이상이 0으로 표시된 경우) . $I_0$, $O_0$ 및 $O_1$에 대한 유사한 정의 . | . | . | . . . 💬 $f_{v}$ : 변수 벡터 (TF_IDF등) $I$ : incoming neighbor 레이블에 대한 기술 통계치 벡터([0인 이웃노드 유무, 1인 이웃노드 유무 $O$ : outgoing neighbor 레이블에 대한 기술 통계치 벡터([0인 이웃노드 유무, 1인 이웃노드 유무 $*z_v*$를 여러 방법들중 들어오고 나오는 엣지들의 라벨개수로 설정하기로 한다. 따라서 회색 노드에서는 1번인 초록 노드에서만 엣지가 들어오고 0번 클래스인 노드에서 들어오는것이 없으므로 $I=[0,1]$이 된다. 또한 0번, 1번 클래스 노드로 모두 나가므로 $O=[1,1]$으로 구성한다. Iterative Classifier - Step 1 . On training labels, train two classifiers: Node attribute vector only: $ phi_1(f_v)$ | Node attribute and link vectors $z_v$: $ phi_2(f_v,z_v)$ | . | . Train classifiers | Apply classifier to unlab. set | Iterate | Update relational features $z_v$ | Update label $Y_v$ | . 교육 라벨에서 두 가지 분류기를 교육합니다. 노드 속성 벡터만: $ phi_1(f_v)$ | 노드 속성 및 링크 벡터 $z_v$: $ phi_2(f_v,z_v)$ | . | . 분류기 학습 | 레이블 해제 세트에 분류자 적용 | 반복 | 관계 기능 업데이트 $z_v$ | 레이블 업데이트 $Y_v$ | . . 💬 $ phi_1(f_v)$에서는 피쳐 정보($f_v$)만을 사용하며, $ phi_2(f_v,z_v)$는 피쳐 정보($f_v$)와 이웃라벨 정보($I,O)$를 이용하여 학습한다. Iterative Classifier - Step 2 . On the unlabeled set: Use trained node feature **vector classifier $ phi_1$ to set $Y_v**$ | . | . Train classifiers | Apply classifier to unlab. set | Iterate | Update relational features $z_v$ | Update label $Y_v$ | . 라벨이 없는 세트에서: 훈련된 노드 특징 벡터 분류기 $ phi_1$를 사용하여 $Y_v$ 설정 | . | . 분류기 학습 | 레이블 해제 세트에 분류자 적용 | 반복 | 관계 기능 업데이트 $z_v$ | 레이블 업데이트 $Y_v$ | . 💬 $ phi_1$이 부여한 라벨을 이용해 $z_v$를 업데이트한다. Iterative Classifier - Step 3.1 . Update $z_v$ for all nodes: | . Train classifiers | Apply classifier to unlab. set | Iterate | Update relational features $z_v$ | Update label $Y_v$ | . 모든 노드에 대해 $z_v$ 업데이트: | . 분류기 학습 | 레이블 해제 세트에 분류자 적용 | 반복 | 관계 기능 업데이트 $z_v$ | 레이블 업데이트 $Y_v$ | . . 💬 $ phi_2$가 업데이트 된 $z_v$와 $I$, $O$ 정보를 사용하여 라벨을 예측한다. Iterative Classifier - Step 3.2 . **Re-classify all nodes with $ phi_2$: | . Train classifiers | Apply classifier to unlab. set | Iterate | Update relational features $z_v$ | Update label $Y_v$ | . $ phi_2$로 모든 노드를 다시 분류합니다. | . 분류기 학습 | 레이블 해제 세트에 분류자 적용 | 반복 | 관계 기능 업데이트 $z_v$ | 레이블 업데이트 $Y_v$ | . 💬 $ phi_2$에 의해 라벨이 변화했으므로, 다시 $z_v$를 업데이트한다. Iterative Classifier - Iterate . Continue until convergence Update $z_v$ based on $Y_v$ | Update $Y_v$ = $ phi_2(f_v,z_v)$ | . | . Train classifiers | Apply classifier to unlab. set | Iterate | Update relational features $z_v$ | Update label $Y_v$ | . 정합될 때까지 계속합니다 $Y_v$를 기준으로 $z_v$ 업데이트 | 업데이트 $Y_v$= $ phi_2(f_v,z_v)$ | . | . 분류기 학습 | 레이블 해제 세트에 분류자 적용 | 반복 | 관계 기능 업데이트 $z_v$ | 레이블 업데이트 $Y_v$ | . 💬 $z_v$가 업데이트 되었으므로 $ phi_2$가 레이블을 예측한다. Iterative Classifier - Final Prediction . Stop iteration After convergence or when maximum iterations are reached | . | . . 반복 중지 수렴 후 또는 최대 반복 횟수에 도달한 경우 | . | . . . 💬 $ phi_2$를 통한 예측과 $z_v$에 대한 업데이트를 종료 고전에 도달할 때까지 반복한다. Summary . We talked about 2 approaches to collective classification | Relational classification Iteratively update probabilities of node belonging to a label class based on its neighbors | . | Iterative classification Improve over collective classification to handle attribute/feature information | Classify node 𝑣 based on its features as well as labels of neighbors | . | . . 집단 분류에 대한 2가지 접근법에 대해 이야기했습니다 | 관계구분 인접 관계에 따라 레이블 클래스에 속하는 노드의 확률을 반복적으로 업데이트합니다. | . | 반복구분 특성/특징 정보를 처리하기 위해 집합 분류보다 개선 | 특징과 이웃의 label을 기준으로 노드 based 분류 | . | . . CS224W: Machine Learning with Graphs 2021 Lecture 5.2 - Relational and Iterative Classification .",
            "url": "https://cs224w-kor.github.io/blog/graph%20neural%20network/gnn/graph%20convolution%20network/gcn/2022/07/20/lecture-0502.html",
            "relUrl": "/graph%20neural%20network/gnn/graph%20convolution%20network/gcn/2022/07/20/lecture-0502.html",
            "date": " • Jul 20, 2022"
        }
        
    
  
    
        ,"post20": {
            "title": "Lecture 5.1 - Message passing and Node Classification",
            "content": ". Lecture 5 . Lecture 5.1 - Message passing and Node Classification | Lecture 5.2 - Relational and Iterative Classification | Lecture 5.3 - Collective Classification : Belief Propagation | . . 5.1 - Message passing and Node Classification . Message Passing and Node Classification . Today’s Lecture: outline . Main question today: Given a network with labels on some nodes, how do we assign labels to all other nodes in the network? Example: In a network, some nodes are fraudsters, and some other nodes are fully trusted. How do you find the other fraudsters and trustworthy nodes? We already discussed node embeddings as a method to solve this in Lecture 3 . . 오늘 주요 질문: 일부 노드에 레이블이 있는 네트워크에서 네트워크의 다른 모든 노드에 레이블을 할당하려면 어떻게 해야 합니까? 예: 네트워크에서 일부 노드는 사기꾼이고 다른 일부 노드는 완전히 신뢰됩니다. 다른 사기꾼과 신뢰할 수 있는 노드를 어떻게 생각하십니까? 우리는 이것을 해결하기 위한 방법으로 노드 임베딩을 이미 강의 3에서 논의하였습니다 . . 💬 이번 수업에서 다룬 내용은 한 그래프에서 특정 노드들에 레이블에 매겨져 있을 때, 다른 노드들에 레이블을 매기는 방법에 대해 다루게 된다. Example: Node Classification . . Given labels of some nodes Let’s predict labels of unlabeled nodes This is called semi-supervised node classification . . 일부 노드의 지정된 레이블 레이블이 없는 노드의 레이블을 예측해 봅시다. 이를 준지도 노드 분류라고 합니다. . . 💬 위 그림과 같이 그래프 전체에서 일부 노드에 레이블이 있고, 나머지 노드에는 레이블이 없을 때, 이를 예측하는 방법론을 다룬다. Today’s Lecture: outline . Main question today: Given a network with labels on some nodes, how do we assign labels to all other nodes in the network? | Today we will discuss an alternative framework: Message passing | Intuition: Correlations (dependencies) exist in networks. In other words: Similar nodes are connected. | Key concept is collective classification: Idea of assigning labels to all nodes in a network together. | . | We will look at three techniques today: Relational classification | Iterative classification | Correct &amp; Smooth | . | . . 오늘 주요 질문: 일부 노드에 레이블이 있는 네트워크에서 네트워크의 다른 모든 노드에 레이블을 할당하려면 어떻게 해야 합니까? | 오늘은 대체 프레임워크에 대해 논의하겠습니다: 메시지 전달 | 직관: 상관 관계(의존성) 네트워크에 존재하다 즉, 유사한 노드가 연결되어 있습니다. | 핵심 개념은 집단 분류: 네트워크의 모든 노드에 label을 함께 할당하는 아이디어입니다. | . | 오늘은 세 가지 기법을 살펴보겠습니다. 관계구분 | 반복구분 | 정확하고 매끄러운 | . | . . 💬 이를 Semi-supervised node classification이라 한다고 한다. 이미 레이블이 있는 정보와, 없는 구조를 학습하여 새로이 레이블을 예측하기 때문으로 보인다. 이를 Message Passing이라는 프레임 워크를 살펴보게 되는데, message passing 의 주요 개념은 상관관계(correlation)이다. 즉, 비슷한 노드 간에는 상관관계가 있을 것이기 때문에, 노드 간에 상관관계를 파악하여 이를 이용해 레이블을 예측하고자 한다. 여기서 반복적으로 labeling 작업이 발생하기 때문에, 이전 수업인 Pagerank와 비슷한 점이 있다고 넘어가자. Correlations Exist in Networks . Behaviors of nodes are correlated across the links of the network | Correlation: Nearby nodes have the same color (belonging to the same class) | . . 노드의 동작은 네트워크 링크 전체에서 상관됨 | 상관: 근처 노드의 색상은 동일합니다(동일한 클래스에 속함). | . . . Two explanations for why behaviors of nodes in networks are correlated: . | 네트워크에서 노드의 동작이 상관되는 이유에 대한 두 가지 설명은 다음과 같다. . | . . . 💬 위의 그래프에서, 비슷한 노드(같은 레이블을 가지고 있는 노드, 초록색, 빨간색과 같이)는 근처에 위치하는 것을 알 수 있다. 이와 같이, 노드 간의 관계를 파악하여, 서로 근접한 노드를 찾을 수 있다면, 거기에 비슷한 레이블을 매길 수 있을 것이다. 여기서 이제 어떻게 노드 간의 상관관계를 이끌어 낼 수 있을지 살펴보자. 노드 간의 상관관계는 Homophily, Influence 두 개념에 의해 정의되어진다. 두 개념 모두 사회과학 분야에서 social network를 분석하면서 쓰인 개념으로 보이는데, 하나씩 자세히 살펴보도록 하자. Social Homophily . Homophily: The tendency of individuals to associate and bond with similar others | “Birds of a feather flock together” | It has been observed in a vast array of network studies, based on a variety of attributes (e.g., age, gender, organizational role, etc.) | Example: Researchers who focus on the same research area are more likely to establish a connection (meeting at conferences, interacting in academic talks, etc.) | . . 동성애: 개인이 유사한 타인과 연관되고 유대감을 갖는 경향 | “깃털 같은 새들” | 다양한 속성(예: 연령, 성별, 조직 역할 등)을 기반으로 한 광범위한 네트워크 연구에서 관찰되었다. | 예시: 동일한 연구 분야에 집중하는 연구자는 (학회에서의 회의, 학술 강연에서의 상호작용 등) 연결을 확립할 가능성이 높다. | . . Homophily: Example . Example of homophily . Online social network Nodes = people | Edges = friendship | Node color = interests (sports, arts, etc.) | . | People with the same interest are more closely connected due to homophily | . . 동음이의 예 . 온라인 소셜 네트워크 노드 = 사람 | 가장자리 = 우정 | 노드 색상 = 관심 사항(스포츠, 예술 등) | . | 동종애로 인해 같은 관심사를 가진 사람들이 더 밀접하게 연결되어 있다. | . . . 💬 개인의 특징은 나이, 성별, 직업, 취미, 거주지 등 다양한 요소가 있을 것이다. Homophily는 개인들이 비슷한 특징을 가지는 타인들과 서로 연결되고, 함께 행동하려고 한다는 개념이다. 예를 들어, 머신러닝 연구자들은 비슷한 학회를 동시에 참여하고, 비슷한 커뮤니티를 읽고 토론하게 되면서 자연스레 친분을 쌓게 된다. 또한, 가수들은 서로 같이 공연하고, 서로의 앨범을 들으면서 서로 사회적으로 연결되게 된다. 위의 그래프는 한 학교의 학생들을 나타낸 그래프인데, 학생 개인이 노드, 친분이 엣지로 표현되어 있다. 이때 노드의 레이블인 색은 각 학생의 관심사로 운동, 예술 등이 있다. 직관적으로 살펴보아도 알 수 있지만 총 4개의 작은 그룹으로 나누어질 수 있으며, 각 그룹은 비슷한 관심사를 가지는 학생들이 모여있는 것을 알 수 있다. 이를 Homophily라고 할 수 있을 것이다. Social Influence: Example . Influence: Social connections can influence the individual characteristics of a person. Example: I recommend my musical preferences to my friends, until one of them grows to like my same favorite genres! | . | 영향: 사회적 관계는 개인의 특성에 영향을 미칠 수 있다. 예시: 친구 중 한 명이 제가 좋아하는 장르를 좋아하게 될 때까지 친구들에게 제 음악적 취향을 추천합니다! | . | . . 💬 Influence는 사회적으로 연결된 개인 간에는 서로 영향을 주고 받으면서 비슷한 특징을 가지게 된다는 것을 의미한다. 예를 들어, 내가 1, 2학년 때는 경상계열 친구들과 친하게 지내면서, 경제, 사회, 제도 등에 관심을 가지고 해당 직군을 희망했다면, 3, 4학년이 되면서 통계나 수학, 컴퓨터 공학 친구들과 친하게 지내면서 ML, DL, 코딩 등에 관심을 가지고 해당 직군을 희망하게 된 것이 Influence 때문이라고 할 수 있을 것이다. How do we leverage node correlations in networks? . Classification with Network Data . How do we leverage this correlation observed in networks to help predict node labels? . | 네트워크에서 관찰된 이 상관 관계를 활용하여 노드 레이블을 예측하는 방법은 무엇입니까? . | . . How do we predict the labels for the nodes in grey? . 노드의 레이블을 회색으로 어떻게 예측합니까? . Motivation . Similar nodes are typically close together or directly connected in the network: Guilt-by-association: If I am connected to a node with label 𝑋, then I am likely to have label 𝑋 as well. | Example: Malicious/benign web page: Malicious web pages link to one another to increase visibility, look credible, and rank higher in search engines | . | Classification label of a node 𝑣 in network may depend on: Features of 𝑣 | Labels of the nodes in 𝑣’s neighborhood | Features of the nodes in 𝑣’s neighborhood | . | . . 유사한 노드는 일반적으로 네트워크에서 서로 가까이 있거나 직접 연결됩니다. 연관별 죄책감: 레이블 𝑋이 있는 노드에 연결되어 있다면 레이블 𝑋도 있을 수 있습니다. | 예: 악성/악성 웹 페이지: 악성 웹 페이지는 가시성을 높이고 신뢰도를 높이며 검색 엔진에서 더 높은 순위를 차지하기 위해 서로 연결됩니다. | . | 네트워크에 있는 노드 $v$의 분류 라벨은 다음 조건에 따라 달라질 수 있습니다. $v$의 기능 | $v$의 이웃에 있는 노드의 레이블 | $v$의 이웃에 있는 노드의 기능 | . | . 💬 한 그래프 내에서 비슷한 노드는 가까이 위치하거나 직접 연결되어 있을 것이다. 이를 Guilt-by-association이라고 하는데, 노드 b가 아직 레이블이 없는 상태에서, 이웃노드 x가 1로 레이블 되어 있다면, 이웃노드 x와 가깝기 때문에 노드 b 역시 1로 레이블 될 가능성이 높다는 개념이다. 구체적인 예시로는 스팸 사이트들이 안전한 사이트와의 연결고리는 생성할 수 없기 때문에, 노출도와 신뢰도를 높이기 위해 서로 링크를 연결하는 경향이 있는데, 이를 이용해서 스팸 사이트 하나를 잡을 수 있다면, 서로 연결된 다른 스팸 사이트도 색출 할 수 있다고 한다. 이때, 노드 v의 분류에 이용하는 정보들은 다음과 같다. 1. 노드 v의 변수들 2. 노드 v의 이웃 노드들의 레이블 3. 노드 v의 이웃 노드들의 변수들 Semi-supervised Learning . Formal setting: . Given: . Graph | Few labeled nodes | . Find: Class (red/green) of remaining nodes . Main assumption: There is homophily in the network . . 공식 설정: . 제공됨: . 그래프 | 레이블이 지정된 노드 몇 개 | . 찾기: 나머지 노드의 클래스(빨간색/녹색) . 주요 가정: 네트워크에 동질성이 있습니다. . . Example task: . Let 𝑨 be a 𝑛×𝑛 adjacency matrix over 𝑛 nodes | Let Y = $[0,1]^n$ be a vector of labels: $Y_v$ = 1 belongs to Class1 | $Y_v$ = 0 belongs to Class0 | There are unlabeled node needs to be classified | . | Goal: Predict which unlabeled nodes are likely Class 1, and which are likely Class 0 | . . 작업 예: . A를 n개 노드의 nxn 인접 행렬로 설정 | Y = $[0,1]^n$ 을 레이블 벡터라고 하자: $Y_v$ = 1이 클래스 1에 속함 | $Y_v$ = 0이 클래스 0에 속함 | 라벨이 지정되지 않은 노드가 분류되어야 합니다 | . | 목표: 레이블이 없는 노드가 클래스 1일 가능성이 높고 클래스 0일 가능성이 높은 노드를 예측합니다. | . . 💬 실제로 이용하게 되는 입력값은 인접행렬: $A_{n*n}$, 레이블 벡터 $Y=[0,1]^n$, $Y_v$ **= 1이 클래스 1**에 속함, $Y_v$ **= 0이 클래스 0**에 속함이 있으며, 아직 레이블이 없는 노드에 대해 레이블이 0 혹은 1일 확률을 계산하는 것이 목표이다. Problem Setting . How to predict the labels $Y_v$ for the unlabeled nodes $v$ (in grey color)? Each node $v$ has a feature vector $f_v$ Labels for some nodes are given (1 for green, 0 for red) Task: Find $P(Y_v)$ given all features and the network 라벨이 없는 노드 $v$(회색)에 대한 레이블 $Y_v$를 예측하는 방법 각 노드 $v$에는 특징 벡터 $f_v$가 있습니다. 일부 노드의 라벨이 제공됩니다(녹색은 1, 빨간색은 0). 작업: 모든 기능 및 네트워크가 주어진 $P(Y_v)$ 찾기 . P(Yv)=?P(Y_v)=?P(Yv​)=? . . Example applications: . Many applications under this setting: Document classification | Part of speech tagging | Link prediction | Optical character recognition | Image/3D data segmentation | Entity resolution in sensor networks | Spam and fraud detection | . | . . 이 설정 아래의 많은 응용 프로그램: 문서구분 | 음성 태그의 일부 | 링크 예측 | 광학식 문자 인식 | 영상/3D 데이터 분할 | 센서 네트워크의 엔티티 해상도 | 스팸 및 부정 행위 탐지 | . | . . 💬 이러한 방법론은 위와 같은 다양한 분야에 적용이 가능하다. Collective Classification Overview (1) . collective classification을 전반적으로 살펴보면 다음과 같다. . 📌 Intuition(직관) : 노드 간 상관관계를 이용해 서로 연결된 노드들을 동시에 분류하기 이때 1차 마르코프 연쇄를 사용한다. 즉, 노드 $ mathrm{v}$ 의 레이블 $Y_{v}$ 를 예측하기 위해서는 이웃노드 $N_{v}$ 만 필요하다는 것이다. 2차 마르코프 연쇄를 사용할 경우 $N_{v}$ 의 이웃노드 역시 사용할 것이다. 1차 마르포크 연쇄를 사용할 경우 식은 다음과 같아질 것이다. . P(Yv)=P(Yv∣Nv)P left(Y_{v} right)=P left(Y_{v} mid N_{v} right)P(Yv​)=P(Yv​∣Nv​) . collective classification은 하나의 모델을 이용하거나 기존의 분류모델처럼 한번의 과정으로 구성되지 않고 총 세가지 과정으로 구성된다. . Collective Classification Overview (2) . Local Classifier . 최초로 레이블을 할당하기 위해 사용되는 분류기이다. 즉, 그래프에서 레이블이 없는 노드들에 대해 우선 노드를 생성해야 하기 때문에, 기존의 분류문제와 동일하게 구성된다. 이때 예측 과정은 각 노드의 변수만 사용하여 이미 레이블이 있는 노드로 학습하고, 레이블이 없는 노드로 예측하게 된다. 그래프의 구조적 정보가 사용되지 않는다는 점에 유의하자. . Relational Classifier . 노드 간 상관관계를 파악하기 위해 이웃 노드의 레이블과 변수를 사용하는 분류기이다. 이를 통해 이웃 노드의 레이블과 변수와 현재 노드의 변수를 이용해 현재 노드의 레이블을 예측할 수 있다. 이때, 이웃노드의 정보가 사용되기 때문에, 그래프의 구조적 정보가 사용된다. . Collective Inference . Collective Classification은 한번의 예측으로 종료되지 않는 것이 핵심이다. 특정 조건을 만족할 때까지 각 노드에 대해 분류하고 레이블을 업데이트한다. 이때의 조건이란 더이상 레이블이 변하지 않거나, 정해진 횟수를 의미한다. 이때 동일한 변수를 가진 노드라 하더라도 그래프의 구조에 따라 최종 예측이 달라질 수 있다는 점을 유념하자. . Overview of What is Coming . We focus on semi-supervised binary node classification | We will introduce three approaches: Relational classification | Iterative classification | Correct &amp; Smooth | . | . . 우리는 준지도 이진 노드 분류에 초점을 맞춘다. | 다음 세 가지 접근 방식을 소개합니다. 관계구분 | 반복구분 | 정확하고 매끄러운 | . | . . CS224W: Machine Learning with Graphs 2021 Lecture 5.1 - Message passing and Node Classification .",
            "url": "https://cs224w-kor.github.io/blog/graph%20neural%20network/gnn/graph%20convolution%20network/gcn/2022/07/20/lecture-0501.html",
            "relUrl": "/graph%20neural%20network/gnn/graph%20convolution%20network/gcn/2022/07/20/lecture-0501.html",
            "date": " • Jul 20, 2022"
        }
        
    
  
    
        ,"post21": {
            "title": "Lecture 4.4 - Matrix Factorization and Node Embeddings",
            "content": ". Lecture 4. Graph as Matrix . Lecture 4.1 - PageRank | Lecture 4.2 - PageRank, How to Solve? | Lecture 4.3 - Random Walk with Restarts | Lecture 4.4 - Matrix Factorization and Node Embeddings | . . Recall: Node Embeddings &amp; Embedding matrix . 이전 강의에서 배웠던 embedding matrix $ mathbf{Z}$에 대해 다시 떠올려봅시다. 이 매트릭스는 그래프의 각 노드들을 잠재변수 공간(embedding space)으로 encoding하는 행렬로 열의 차원은 embedding하는 크기, 행의 차원은 그래프에 있는 노드의 수가 됩니다. 이 매트릭스의 한 열은 특정 노드 $u$의 embedding vector $ mathbf{z}_u$를 나타내게 됩니다. . . . 이러한 Node embedding에서 objective는 그래프상에서 실제로 유사한 노드들의 simliarity가 embedding vector들의 내적(inner product)값도 높도록 만드는 것입니다. . 📌 Objective: Maximize $ mathbf{z}_{v}^{ mathrm{T}} mathbf{z}_{u}$ for node pairs $(u, v)$ that are similar Matrix Factorization . Embedding matrix를 Matriz Factorization 관점에서 다시 생각해봅시다. 그래프를 노드들간의 연결이 되어 있으면 1, 아니면 0으로 나타낸 인접행렬 $ mathbf{A}$을 embedding matrix $ mathbf{Z}$로 factorization 한다고 생각해볼 수 있습니다. 즉 $ mathbf{Z}^{ mathrm{T}}$와 $ mathbf{Z}$의 내적으로 인접행렬 $ mathbf{A}$를 만드는 것입니다. . ZTZ=A mathbf{Z}^{ mathrm{T}} mathbf{Z} = mathbf{A}ZTZ=A . . 하지만 embedding matrix $ mathbf{Z}$의 행의 수, 즉 embedding dimension $d$는 노드의 수 $n$보다 작으므로 완벽한 factorization을 할 수 없고 대신에 이를 최적화 기법을 사용하여 근접(approzimate)시킬 수 있습니다. 이 최적화를 목적함수는 다음과 같습니다. . min⁡Z∥A−ZTZ∥2 min _{ mathbf{Z}} left |A- boldsymbol{Z}^{T} boldsymbol{Z} right |_{2}Zmin​∥ . ∥​A−ZTZ∥ . ∥​2​ . 결론은 edge connectivity로 정의된 node similarity을 나타내는 decoder($ mathbf{Z}$)의 내적은 $ mathbf{A}$의 matrix factorization과 동일하다는 것입니다. . RandomWalk-based Similarity . DeepWalk와 node2vec 알고리즘에서는 random walks를 기반으로한 좀 더 복잡한 node similarity를 사용합니다. 2개의 알고리즘 모두에서 matrix factorization을 사용하고 있습니다. DeepWalk에서 사용하는 node simliarity는 다음과 같이 정의됩니다. (node2vec은 이보다 조금 더 복잡합니다. 자세한 내용을 확인하고 싶으면 Network Embedding as Matrix Factorization paper를 참고) . . Limitations . Matrix factorization과 random walk로 node embedding을 할 경우 몇가지 제약(단점)이 있습니다. . 그래프에 새로운 노드가 생겼을 때 대응하지 못합니다. training과정에서 보지 못한 노드가 생겼을 때 scratch부터 다시 계산해야 합니다. | . 구조적인 유사성을 파악하지 못합니다. 아래의 그림에서 1-2-3과 11-12-13은 그래프에서 비슷한 구조를 가지고 있지만 각 노드마다 unique한 embedding 값으로 인해 구조적인 유사성을 파악하지 못합니다. | . 노드, 엣지, 그래프의 feature 정보를 활용할 수 없습니다. DeepWalk나 node2vec에 쓰인 embedding은 노드에 있는 feature 정보를 활용할 수 없습니다. 이는 추후에 배울 Deep Representation Learning으로 해결할 수 있습니다. | . Algorithms 정리 . PageRank: 그래프에서 노드의 importance를 측정하는 알고리즘이며 인접행렬의 power iteration으로 계산할 수 있다. 총 3가지 관점에서 해석할 수 있다. | (1) Flow formulation | (2) Random walk &amp; Stationary distribution | (3) Linear Algebra - eigenvector | . | Personalized PageRank(PPR): PageRank에서 좀 더 발전시킨 알고리즘으로 random walk로 구한 특정 노드의 중요성을 더 고려하여 teleport를 하는 알고리즘 | Random walks 기반 Node Embeddings은 Matrix factorization으로 표현될 수 있다. | . 그래프를 행렬로 이해하는 것은 위의 알고리즘들을 이해하는데 매우 중요하다는 것을 알 수 있습니다. . . Original Lecture Video : CS224W: Machine Learning with Graphs 2021 Lecture 4.4 - Matrix Factorization and Node Embeddings .",
            "url": "https://cs224w-kor.github.io/blog/matrix/node%20embedding/factorization/random%20walks/2022/07/13/lecture-0404.html",
            "relUrl": "/matrix/node%20embedding/factorization/random%20walks/2022/07/13/lecture-0404.html",
            "date": " • Jul 13, 2022"
        }
        
    
  
    
        ,"post22": {
            "title": "Lecture 4.3 - Random Walk with Restarts",
            "content": ". Lecture 4. Graph as Matrix . Lecture 4.1 - PageRank | Lecture 4.2 - PageRank, How to Solve? | Lecture 4.3 - Random Walk with Restarts | Lecture 4.4 - Matrix Factorization and Node Embeddings | . . Recommendation . 추천 시스템에서 이분그래프(Bipartite graph)로 user와 item의 (구매)관계를 나타낸 Bipartite User-Item Graph는 다음 그림과 같습니다. 여기에서 특정 item Q를 구매한 user에게 어떤 item을 추천해주는 것이 좋을지를 고민한다면, 직관적으로 item Q가 item P와 비슷하게 user들과 관계를 가지고 있을 때 item P를 이 유저에게 추천하는 것이 좋을 것이라고 생각할 수 있습니다. 즉, item Q와 item P가 얼마나 가까운 관계인지 판단하는 것이 중요합니다. . . Node proximity Measurements . 노드 근접성(proximity) 측정에 대해 생각해보기 위해 아래의 3가지 케이스를 보겠습니다. A-A’은 B-B’보다 더 가까운 관계를 가지고 있다고 할 수 있습니다. 왜냐하면 A-A’ path에서 user을 한번만 거치는데 반해, B-B’path에서는 B-user-item-user-B’ 로 path의 길이가 더 길기 때문입니다. A-A’와 C-C’를 비교해보면 C-C’이 더 가까운 관계를 가지고 있다고 판단할 수 있는데 그 이유는 C-C’이 A-A’보다 더 많은 공통의 이웃(Common Neighbors)를 가지고 있기 때문입니다. C-C’은 A-A’의 shortest path가 2개있는 것으로도 볼 수 있습니다. . . Proximity on Graphs . 이전에 PageRank를 다시 떠올려보면, (1) rank는 node의 “importance”를 정의하며 (2) 그래프의 모든 node들에 균일 분포로 teleport 이동을 할 수 있는 알고리즘이었습니다. . 여기에 좀 더 아이디어를 덧붙여서 Personalized PageRank 알고리즘을 생각해 볼 수 있습니다. 그래프의 모든 노드들에 대해 균일 분포로 teleport 이동을 하는 것이 아닌, 그래프 노드들의 부분집합(subset) $ mathbf{S}$의 노드들로만 teleport 이동을 하도록 할 수 있습니다. 모든 노드들로 랜덤하게 teleport하지 않고 좀 더 연관성이 높은 노드들로 teleport할 수 있도록 하는 것입니다. item Q와 item P가 더 연관성이 높다는 것(Node proxmity ↑)을 어떻게 알 수 있을까요? 이는 Random Walks로 확인해볼 수 있습니다. . Random Walks . item Q가 우리가 알고싶은 item 노드들의 집합인 QUERY_NODES집합에 속해있다고 해봅시다. Bipartite User-Item Graph 상에서 QUERY_NODES 집합에 속해 있는 어떤 노드(item Q)에서 시작하여 랜덤하게 움직이면서 과정을 기록합니다. 이 과정을 기록한다는 것은 item↔user 사이를 계속 랜덤하게 움직이면서 방문(visit)하게 된 item 노드에는 +1 count를 하는 것을 의미합니다. 이렇게 랜덤하게 움직이면서 이동을 결정할 때마다 일정 확률 ALPHA 만큼 재시작을 하게되는데, 재시작시에는 QUERY_NODES집합에 속해 있는 하나의 노드로 이동해서 다시 랜덤하게 움직이기 시작합니다. (아래 pseudo code 참고) . . 이렇게 계속 Random Walks를 하다보면 item 노드의 visit 수가 높을수록 query item Q와 높은 관계성을 가진것으로 판단할 수 있습니다. . Benefits . 이와 같은 Random Walks를 통한 시뮬레이션과 visit 수로 노드들간의 근접성(proximity)을 판단하는데 좋은 이유는 다음과 같은 사항들을 고려하여 similarity를 나타낼 수 있는 방법이기 때문입니다. . Multiple connnections | Multiple paths | Direct and Indirect connections | Degree of the node | . PageRank Varients 정리 . PageRank와 이를 변형한 총 3가지 알고리즘들을 정리하면 다음과 같습니다. . PageRank Personalized PR Random Walk w/ Restarts . 모든 노드들에 같은 확률로 teleport 이동 | 특정 노드들로 특정 확률을 가지고 teleport 이동 | 항상 똑같은 1개의 노드로 이동 | . . . Original Lecture Video : CS224W: Machine Learning with Graphs 2021 Lecture 4.3 - Random Walk with Restarts .",
            "url": "https://cs224w-kor.github.io/blog/matrix/pagerank/recommendation/proximity/random%20walks/ppr/restarts/2022/07/13/lecture-0403.html",
            "relUrl": "/matrix/pagerank/recommendation/proximity/random%20walks/ppr/restarts/2022/07/13/lecture-0403.html",
            "date": " • Jul 13, 2022"
        }
        
    
  
    
        ,"post23": {
            "title": "Lecture 4.2 - PageRank, How to Solve?",
            "content": ". Lecture 4. Graph as Matrix . Lecture 4.1 - PageRank | Lecture 4.2 - PageRank, How to Solve? | Lecture 4.3 - Random Walk with Restarts | Lecture 4.4 - Matrix Factorization and Node Embeddings | . . 이전의 강의에서 Powe iteration 방법으로 반복적인 매트릭스 곱 연산으로 $ mathbf{r}$을 구할 수 있음을 확인했습니다. 이 방법에 대해 조금 더 구체적으로 살펴보겠습니다. . Power Iteration Method . power iteration은 2가지 표현식이 있는데 하나는 벡터의 요소 관점에서의 업데이트 식(왼쪽)과 다른 하나는 매트릭스 관점의 업데이트 식(오른쪽)으로 나타낼 수 있습니다. . . 과정을 살펴보면 다음과 같습니다. . 처음 초기화로 모든 노드의 importance score를 똑같은 값으로 만들어 줍니다.(반복적인 연산으로 수렴을 보장하므로 사실 어떤 값으로 초기화하든 상관없습니다.) $ boldsymbol{r}^{(0)}=[1 / N, ldots ., 1 / N]^{T}$ | 반복적인 연산을 하면서 $ mathbf{r}$ 값을 업데이트합니다. $ boldsymbol{r}^{(t+1)}= boldsymbol{M} cdot boldsymbol{r}^{(t)}$ | 수렴조건 $ left| boldsymbol{r}^{( boldsymbol{t}+ mathbf{1})}- boldsymbol{r}^{(t)} right|_{1}&lt; varepsilon$ 을 만족할 때까지 2번 과정의 연산을 진행합니다. | 예시 그래프에서의 power iteration 과정은 다음과 같습니다. . . . Three Questions . Does this converge? 반복적인 연산과정을 통해 값이 수렴하는가? | Does it converge to what we want? 수렴한 값이 우리가 원하는 값인가? | Are results reasonable? 연산 결과가 합당한가?(말이 되는가?) | (어색한 한국어 번역보다 영어로된 질문에서 얻어가는 insight가 좋을 것 같습니다.) . Problems . PageRank에는 2가지의 문제가 있습니다. . Dead Ends | out-link를 가지지 않는 일부 페이지(노드)들에서 생기는 문제로 이런 페이지들에서 importance가 leak out 됩니다. leak out의 세어나가다 라는 뜻 그대로 importance flow의 흐름에서 값이 세어나가는 문제를 말합니다. . 아래의 예시에서 페이지 b에서 나가는 out-link가 없다보니 importance update를 한 결과가 $r_a = 0, r_b=0$이 됨을 확인할 수 있습니다. 이는 앞서 page rank $ mathbf{r}$ vector의 정의에서 약속한 모든 노드의 importance의 합이 1이 된다는 column stochastic 수학적 전제에서 벗어난 결과 입니다. . . Spider traps | 특정 페이지의 모든 out-link들이 다른페이지로 나가지 않아 결국 spider trap 페이지가 모든 importance 값을 독차지하게 됩니다. . 아래의 예시에서 a에서 walk를 시작하더라고 b로 이동한 후 b에서 빠져나올 수 없습니다. 이런 경우 importance update 결과 모든 importance를 페이지 b가 가지게 되어 $r_a = 0, r_b=1$이 됩니다. 이런 경우 페이지 a에 아무리 큰 웹 그래프가 연결되어 있다고 하더라도 이동할 수 없습니다. 사실 spider trap은 column stochastic을 만족하기 때문에 수학적으로 문제되진 않습니다. 하지만 우리가 원하지 않는 값에 수렴하는 문제로 볼 수 있습니다. . . Solutions . 위의 2가지 문제들 모두 Teleports로 해결할 수 있습니다. . . Dead Ends를 Teleports로 해결하기 | Dead Ends인 m 페이지에서 column stochastic을 만족하지 않고 모든 값이 0이 되지 않도록 자신을 포함한 그래프의 모든 노드들로 uniform random 하게 teleport 이동을 하도록 합니다. 이때 그래프의 노드가 총 3개이므로 m열의 행렬값을 $1/3$으로 채워 $ mathbf{M}$을 완성합니다. . . Spider Traps를 Teleports로 해결하기 | Spier Trap인 m 페이지에서 다른 노드로 빠져나갈 수 있도록 일정 확률 $1- beta$만큼 random 페이지로 점핑(teleport)할 수 있도록 합니다. 즉, 확률 $ beta$만큼은 원래 그래프의 out-links 중에 골라서(random) 이동하고 나머지 확률($1- beta$)로는 out-link와 상관없이 그래프의 모든 페이지들 중에 골라서 이동하여 거미줄, Spider trap에서 벗어나게 되는 것 입니다. 보통 $ beta$값으로는 0.8~0.9값을 사용하는 것이 일반적입니다. . . The Google Matrix . PageRank에서 생길 수 있는 2가지 문제를 Teleport로 해결한다면 PageRank Equation은 다음과 같이 바꿀 수 있습니다. 첫번째 항은 기존의 수식에 있던 부분으로 페이지 $i$의 out-link를 random하게 골라서 이동하는 것에다 확률 $ beta$값을 곱해 보통 0.8~0.9의 확률로 out-link를 통해 이동하게 합니다. . 두번째 항은 Teleport를로 out-link와 상관없이 그래프의 모든 페이지들중 하나로 랜덤하게 순간이동하는 것을 수식적으로 표현한 부분입니다. 그래프에 존재하는 모든 페이지의 수를 $N$이라고 할 때, 추가적으로 $1/N$의 확률로 페이지 $j$로 갈 수 있고 이는 앞서 확률 $ beta$를 제외한 나머지 확률, 약 0.2~0.1의 확률로 이동하는 것이므로 $1- beta$를 곱해줍니다. . rj=∑i→jβridi+(1−β)1Nr_{j}= sum_{i rightarrow j} beta frac{r_{i}}{d_{i}}+(1- beta) frac{1}{N}rj​=i→j∑​βdi​ri​​+(1−β)N1​ . (단, 위의 수식은 $ mathbf{M}$에 dead ends가 없다고 가정하며, 실제로 모든 dead ends를 없애거나 dead ends인 부분들에는 random teleport를 확률1로 따르게 하여 계속 다른 노드로 이동하게 할 수 있습니다.) . 구글 매트릭스는 이와 크게 다르지 않습니다. 단지 위의 PageRank equation을 행렬식으로 바꿔쓰면 구글 매트릭스가 됩니다. 각각의 항들이 의미하는 바는 위에서 설명된 것과 동일하며, 두번째 항의 $ left[ frac{1}{N} right]_{N times N}$는 행렬의 모든 원소가 $ frac{1}{N}$으로 채워진 $N times N$차원의 행렬을 말합니다. . G=βM+(1−β)[1N]N×NG= beta M+(1- beta) left[ frac{1}{N} right]_{N times N}G=βM+(1−β)[N1​]N×N​ . Random Teleports ($ beta=0.8$) . 아래의 $ beta=0.8$일 때 Random Teleports 예시에서 검은색 선들은 teleports를 적용하지 않았을 때의 그래프의 directed links를 표현하며 초록색 선들은 0.2확률의 teleports가 추가된 부분을 나타냅니다. Power iteration을 통해 계산되면 페이지 $y, a, m$이 각각 $7/33, 5/33, 21/33$으로 수렴하는 것을 알 수 있고 spider trap인 페이지 m이 모든 importance를 흡수하지 않는 것을 확인할 수 있습니다. . . Solving PageRank 정리 . PageRank $ mathbf{r} = mathbf{G} mathbf{r}$을 power iteration method로 풀 수 있다. | PageRank에서 생길 수 있는 문제들인 Dead Ends와 Spider Traps를 Random Uniform Teleportation으로 해결할 수 있다. | . . Original Lecture Video : CS224W: Machine Learning with Graphs 2021 Lecture 4.2 - PageRank: How to Solve? .",
            "url": "https://cs224w-kor.github.io/blog/matrix/pagerank/spider%20trap/dead%20ends/teleports/2022/07/13/lecture-0402.html",
            "relUrl": "/matrix/pagerank/spider%20trap/dead%20ends/teleports/2022/07/13/lecture-0402.html",
            "date": " • Jul 13, 2022"
        }
        
    
  
    
        ,"post24": {
            "title": "Lecture 4.1 - PageRank",
            "content": ". Lecture 4. Graph as Matrix . Lecture 4.1 - PageRank | Lecture 4.2 - PageRank, How to Solve? | Lecture 4.3 - Random Walk with Restarts | Lecture 4.4 - Matrix Factorization and Node Embeddings | . . 4강에서는 Graph를 매트릭스(선형대수) 관점으로 바라보는 것에 대해 이야기 합니다. . . 다음 3가지 키워드, Random walk(Node Importance), Matrix Factorization, Node embedding를 중심으로 공부합니다. 강의는 총 4파트로 나누어져 진행됩니다. . The Web as a Directed Graph . . 웹을 거시적인 관점으로 보게되면, 하나의 웹 페이지 → Node로 하이퍼링크 → Edge로 생각하여 하나의 거대한 Graph로 볼 수 있습니다. . Side issue . 다이나믹하게 새로 페이지들이 생길 수 있습니다. | 다크웹과 같은 접근할 수 없는 페이지들도 있을 수 있습니다. | . 잠시 Side issue는 내려놓고, 새로 페이지들이 생기지도 않고 기존의 페이지들이 사라지지도 않는 Static pages 상황을 가정해봅시다. 아래의 그림에서처럼 페이지들은 하이퍼링크들로 서로 연결되어 있고, 유저는 페이지들에 달려있는 하이퍼 링크들로 이루어진 연결망을 기반으로 항해하듯이 Navigational 하게 page to page 이동을 하게 됩니다. (오늘날에는 post, comment, like 등의 기반의 transactional한 웹에서의 상호작용이 일어나지만 이는 우선 논외로 하겠습니다.) . . 위의 그림처럼 웹 그래프는 방향성이 있는 유향 그래프(Directed graph)임을 알 수 있습니다. 위키피디아와 같은 웹 사전 페이지들 간의 관계성이나 논문의 인용 관계 그래프 등에서 예시를 쉽게 찾아볼 수 있습니다. . . Ranking Nodes on the Graph . 웹을 하나의 거대한 유향 그래프로 생각할 때 한가지 중요한 insight가 있습니다. . 💡 모든 웹 페이지들이 똑같이 중요하지는 않다 . 바로 각 페이지의 중요성이 똑같지 않다는 이야기는 그래프에서 각 노드의 중요성(importance)가 다르다는 말로 바꿔 생각할 수 있습니다. 아래 사진을 보면 직관적으로 파란색 노드가 빨간색 노드보다 더 중요할 것 같다라고 생각할 수 있습니다. 왜 그렇게 보일까요? 아직 노드의 중요성에 대해 정의하지 않았지만 그래프에서 각 노드를 중심으로 뻗어있는 edge(link)의 수가 한눈에 비교되기 때문에 직관적으로 파악할 수 있는 것입니다. 이처럼 웹 그래프의 link structure를 가지고 우리는 각 페이지들(node)의 ranking을 매길 수 있습니다. . . Link Analysis Algorithms . 각 페이지들의 중요성(importance)를 파악하기 위해 Link Analysis가 필요합니다. . 본 수업에서 다룰 Link Analysis 알고리즘들은 아래 총 3개에 대해서 다룰 예정입니다. . PageRank | Personalized PageRank (PPR) | Random Walk with Restarts | . Links as Votes . 링크가 투표용지라고 생각해봅시다. 여기서 유향 그래프인 웹 그래프에서 링크는 2가지 종류가 있다는 것을 다시한번 생각해봐야 합니다. . in-comming links(in-links): 기준 페이지로 들어오는 방향의 링크 | out-going links(out-links): 기준 페이지에서 나가는 방향의 링크 | . 이렇게 방향까지 고려하여 링크를 투표라고 생각할 때, 엄밀히 말하자면 in-link를 투표라고 생각해야 할 것 입니다. 한 가지 더 생각해볼 문제는 모든 in-link들을 동등하게 생각할 수 있는가?라는 문제입니다. 어떤 링크들은 다른 링크들에 비해 좀 더 중요한 페이지로부터(from) 기준페이지로(to) 온 링크일 수도 있기 때문에 count에 차등을 둬야 하지 않을까라고 생각할 수도 있습니다. 이런 고민들은 결국 페이지들이 서로 연결되어 있어서 recursive한 문제로 볼 수 있습니다. . ➕ recursive한 문제란, 물리고 물리는 문제로 생각할 수 있습니다. A→B 링크에서 A가 중요한 페이지라는 사실을 기반으로 B가 중요해지고, 이어지는 B→C 링크에서 이 영향을 이어받아 C까지 중요한 페이지라고 판단하게 되기 때문입니다. PageRank . The “Flow” Model . 위에서 설명한 recursive한 특성을 기반으로 중요성이 흘러가는(flow) 모델을 생각해볼 수 있습니다. 중요성을 $r$이라는 변수로 두고 기준 노드 j의 importance가 어떻게 flow되는지 살펴보겠습니다. . j로 in-link되어있는 i, k 의 importance $r_i$, $r_k$를 각 노드의 out-link의 수만큼 나누어서 j로 전달됩니다. i 노드의 out-link는 총 3개 이므로 $ frac{r_i}{3}$, k노드의 out-link는 총 4개 이므로 $ frac{r_k}{4}$로 계산되어 두 값의 합이 $r_j$가 됩니다. | $r_j$는 j노드의 out-link를 통해 flow하게 되는데 out-link의 수, 즉 3으로 나누어져 $ frac{r_j}{3}$ 값이 각각의 다음 노드들로 $r_j$값이 전달되게 됩니다. | . 이처럼 importance가 높은 페이지로부터 in-link된 페이지는 영향을 받아 importance가 높아짐을 알 수 있습니다. 노드 $j$의 rank, $r_j$를 정의하면 다음과 같이 수식으로 나타낼 수 있습니다. (이때 $d_i$는 노드 i의 out-degree를 말합니다.) . rj=∑i→jridir_{j}= sum_{i rightarrow j} frac{r_{i}}{d_{i}}rj​=i→j∑​di​ri​​ . 다음과 같은 예시에서 각 기준 노드를 가지고 in-link들을 고려하여 “Flow equation”을 계산해보면 다음과 같다. . . 노드 y 노드 a 노드 m . y에서 오는 링크 + a에서 오는 링크 | y에서 오는 링크 + m에서 오는 링크 | a에서 오는 링크 | . $r_y = frac{r_y}{2} + frac{r_a}{2}$ | $r_a = frac{r_y}{2} + r_m$ | $r_m = frac{r_a}{2}$ | . ➕ 3 Unknowns, 3 Equations 이기 때문에 4번째 constraint로 $r_y + r_a + r_m =1$로 scale관련 constraint를 추가하여 Gaussian elimination을 사용하여 선형방정식으로 풀려고 하는 생각은 좋지 않다. 왜냐하면 importance는 이런식으로 scalable하지 않기 때문이다. (It’s not scalable) 좀 더 정교한 설계가 필요하다. Matrix Formulation . Stochastic Adjacency Matrix $ mathbf{M}$ . $ mathbf{M}$은 $(node의 수) times (node의 수)$차원의 매트릭스 입니다. | $i$→$j$ 링크에서 매트릭스 요소 $M_{ji}$는 $ frac{1}{d_i}$가 됩니다. ($d_i$를 노드 $i$의 out-degree라고 정의합니다.) . Mji=1diM_{ji} = frac{1}{d_i}Mji​=di​1​ 오른쪽 예시에서처럼 노드 $i$를 기준으로 총 3개의 out-link들이 있다면 각각의 값은 $1/3$이 됩니다. . | column 기준 stochastic : 열 방향의 모든 값들을 더하면 1이 되는 확률값이 됩니다. | . . Rank Vector $r$ . $ mathbf{r}$은 각 페이지의 entry 값을 가지는 $(node의 수) times 1$ 차원의 벡터입니다. | 각 페이지의 importance score를 $r_i$로 정의합니다. | 모든 노드의 importance score의 합은 1입니다. 따라서 이 또한 확률값으로 생각할 수 있습니다. | . ∑iri=1 sum_ir_i = 1i∑​ri​=1 . Flow Equations . 이전에 정의했던 노드의 rank 수식을 새롭게 정의한 매트릭스 $M$과 벡터 $r$로 다시 써보면 Flow Equation을 완성할 수 있습니다. . r=M⋅r mathbf{r}= mathbf{M} cdot mathbf{r}r=M⋅r . 앞서 살펴본 간단한 그래프 예시를 가져와서 flow equation을 매트릭스 연산으로 표현해보면 아래와 같습니다. (flow equation은 앞내용을 참고) . . . Connection to Random Walk . 다음과 같은 조건을 만족하며 랜덤하게 웹페이지들을 돌아다니고 있는 유저를 생각해보겠습니다. . 시점 $t$에 페이지 $i$에 있습니다. | 다음 시점 $t+1$에 페이지 $i$로부터 나가는 방향의 out-link들 중에 uniform하게 선택하여 서핑을 합니다. | 앞서 선택된 out-link를 통해 $i$와 연결된 $j$ 페이지에 도달합니다. | 이 과정(1~3)을 무한으로 반복합니다. | . 여기에서 우리는 시점 의 개념을 고려하여 새로운 개념 정의를 하나 할 수 있습니다. . p(t) mathbf{p(t)}p(t) . $ mathbf{p(t)}$는 확률 벡터(probability distribution)로, 이 벡터의 $i$번째 요소는 앞서 가정한 유저가 시점 $t$에 페이지 $i$에 있을 확률을 나타냅니다. . The Stationary Distribution . 앞서 정의한 $ mathbf{p(t)}$를 가지고 이 유저가 시점 $t+1$에 있을 확률분포는 다음과 같이 계산합니다. . p(t+1)=M⋅p(t) mathbf{p(t+1)}= mathbf{M} cdot mathbf{p(t)}p(t+1)=M⋅p(t) . 💡 만약에 유저가 웹 서핑을 계속하다가 $ mathbf{p(t+1)} = mathbf{p(t)}$ 같은 상황이 되면 어떨까요? . p(t+1)=M⋅p(t)=p(t) mathbf{p(t+1)}= mathbf{M} cdot mathbf{p(t)} = mathbf{p(t)}p(t+1)=M⋅p(t)=p(t) . 이러한 상황에서는 더 이상 유저가 특정 페이지에 있을 확률이 변하지 않고 유지되는 경우가 되며, 이를 stationary distribution of a random walk 라고 합니다. . 이러한 형태는 낮설지가 않은데, 앞서 rank vector $ mathbf{r}$가 매트릭스 $ mathbf{M}$과 flow equation을 구성할 때 이러한 꼴이었으며, 따라서 $ mathbf{r}$은 stationary distribution of a random walk 입니다. . Eigenvector Formulation . 이전 Lecture 2에서 잠시 배웠던 eigenvector와 eignvalue를 생각해보면 다음 수식을 떠올려볼 수 있습니다. . λc=Ac lambda mathbf{c} = mathbf{A} mathbf{c}λc=Ac . 여기에서 flow equation을 다시 위와 같은 꼴로 작성해보면, 아래와 같이 eigenvalue가 1이고 eigenvector가 $ mathbf{r}$인 수식으로 해석될 수 있습니다. . 1⋅r=M⋅r1 cdot mathbf{r}= mathbf{M} cdot mathbf{r}1⋅r=M⋅r . 따라서 $ mathbf{r}$은 매트릭스 $ mathbf{M}$의 principle eigenvector(eigenvalue 1)이며, 임의의 벡터 $ mathbf{u}$에서 시작해서 계속 매트릭스 $ mathbf{M}$을 곱하여 극한 $ mathbf{M}( mathbf{M}(…( mathbf{M}( mathbf{M} mathbf{u}))))$으로 도달하게되는 long-term distribution이 됩니다. 이러한 방식으로 $ mathbf{r}$을 구하는 방법을 Power iteration 이라고 합니다. . PageRank 정리 . 웹 구조에서 볼 수 있는 link들을 기반으로 node들의 importance를 측정할 수 있다. | 랜덤하게 웹 서핑하는 유저 모델은 stochastic advacency matrix $ mathbf{M}$으로 나타낼 수 있다. | PageRank 수식은 $ mathbf{r} = mathbf{M} mathbf{r}$ 이며, $ mathbf{r}$은 (1) 매트릭스 $ mathbf{M}$의 principle eigenvector, (2) stationary distribution of a random walk 2가지로 해석될 수 있다. | . . Original Lecture Video : CS224W: Machine Learning with Graphs 2021 Lecture 4.1 - PageRank .",
            "url": "https://cs224w-kor.github.io/blog/web/matrix/pagerank/rank/flow%20model/flow%20equations/eigenvector/stationary%20distribution/power%20iteration/2022/07/13/lecture-0401.html",
            "relUrl": "/web/matrix/pagerank/rank/flow%20model/flow%20equations/eigenvector/stationary%20distribution/power%20iteration/2022/07/13/lecture-0401.html",
            "date": " • Jul 13, 2022"
        }
        
    
  
    
        ,"post25": {
            "title": "Lecture 01",
            "content": "CS224W: Machine Learning with Graphs |2021 | Lecture 1 정리 . . ** Lecutre 1. Introduction to Graph . Lecture 1.1 Why Graphs | Lecture 1.2 Applications of GraphML | Lecture 1.3 Choice of Graph Representation | . . 1.1 - Why Graphs . 그래프란? . ⇒ a general language for describing and analyzing entities with relations/interactions . ⇒ 서로 관계/상호작용하는 entity들을 설명하고 분석하기 위한 언어라고 할 수 있음 (여기서 entity란 정보의 세계에서 의미있는 하나의 정보 단위) . 그래프 예시 event graphs | computer networks | disease pathways | food webs | particle networks | underground networks | social networks | economic networks | communication networks | citation networks | internet | networks of neurons | knowledge graphs | regulatory networks | scene graphs | code graphs | molecules | 3D shapes | . | . ⇒ 그래프로 세상에 일어나는 모든 현상과 구조들을 설명할 수 있다(broadly applicable). . (관심있는 분야의 현상을 그래프로 표현하여 딥러닝 모델 구조로 변환할 수 있는 능력이 있다면…) . e.g) 분자 구조, 3D 이미지 모형(voxel), 먹이사슬, 소셜 네트워크 등 . (graph와 network의 차이?) . 그래프에서 얻을 수 있는 정보 유형 . 데이터 포인트 간의 구성(organization)과 연결 | 유사한 데이터 포인트 간의 밀접성(similarity) | 데이터 포인트 간의 연결들이 이루는 그래프 구조 | . 그래프가 갖는 구조를 어떻게 활용해 나은 예측을 할 수 있을까? . ⇒ 현상을 명시적(explicitly)으로 잘 반영한 그래프 모델링이 중요하다! . 그래프 ML이 더 어려운 이유? . ⇒ arbitrary size and complex topology . ⇒ spatial locality(공간 지역성)가 없다 . ⇒ 이미지나 텍스트 인풋의 경우 어느 한 데이터포인트로부터 다른 데이터 포인트 간 상대적 위치가 정해져있다(e.g 상하좌우). 하지만, 그래프의 경우 축이 되는 데이터 포인트가 존재하지 않는다. . 그래프를 사용한 딥러닝 . ⇒ 인풋으로는 그래프를 받고, 아웃풋으로는 아래와 같은 형식(ground truth도 동일한 형식)이 가능하다. . node-level | edge-level | graph/subgraph generation | graph/subgraph classification | . 그래프 딥러닝 모델에서 우리가 바라는 플로우 . . 위 플로우를 거친 좋은 성능의 모델을 만들기 위해서는, 인풋이 현상을 잘 반영한 embedding vector로 변환될 수 있도록 학습하는 것이 중요하다.(Representation Learning) . 코스 과정동안 배울 그래프 방법들 traditional methods : graphlets, graph kernels | node embeddings : DeepWalk, Node2Vec | Graph Neural Networks : GCN, GraphSAGE, GAT, Theory of GNNs | Knowledge graphs and reasoning : TransE, BetaE | Deep Generative Models for graphs | Applications | . | . 1.2 - Applications of Graph ML . 그래프 ML은 다양한 태스크 커버가 가능하다 . Node Level(node classification) | . example1) protein folding (구글의 알파폴드) . 배경 : 단백질은 아미노산으로 이루어져있는데, 복잡한 3D 입체 구조의 아미노산 연결 때문에 단백질 구조를 파악하는 태스크는 많게는 1-2년까지 걸린다고 함. | key idea : spatial graph graph : 단백질 | nodes : 아미노산 | edges : 사슬구조 | . | . Edge Level | . example1) Recommender system (PinSage) . nodes : users and items | edges : user-item interactions . . | . example2) drugs and side effects . nodes : drugs &amp; proteins | edges : interaction | . using 2 heterogeneous graphs(drugs &amp; proteins) . . ⇒ drug A와 B를 함께 썼을 때, 생길 수 있는 interaction(edge)는 무엇인가? . Community(subgraph) level | . example1) Traffic Prediction . nodes : Road Segments(도로 구간) | edges : 도로 구간 교차점 | . . ⇒ 아웃풋 : 도착 예정 시간 . Graph-level prediction | . 4-1) graph classification . example1) drug discovery . nodes : atoms | edges : chemical bonds | graph : molecules | . ⇒ 노드와 에지 정보를 통해 그래프(분자) 예측 . 4-2 ) Graph-level generation . example1) drug generation . nodes : atoms | edges : chemical bonds | graph : moelcules | . ⇒ 새로운 그래프(분자) 생성 . example2) physics simulation (graph evolution) . nodes : particles | edges : interaction between particles | . ⇒ t시점 전의 정보로 t시점 이후의 그래프 생성 . . 1.3 - Choice of Graph . 그래프 구성요소 . . objects : nodes, vertices . Interactions : links, edges . system : network, graph . **object와 interactions로 이루어진 데이터 구조 ⇒ graph . 그래프로 세상에 일어나는 모든 현상과 구조들을 설명할 수 있다(broadly applicable). . . 표현법에 따라 그래프의 활용방법은 무궁무진하다. 중요한 것은 적절한 표현을 선택하는 것. | 설명하고자하는 현상을 그래프로 정의하고 싶다면, 먼저 아래 두가지 질문을 하자. what are nodes? | what are edges? | | . Directed vs Undirected Graphs . . ‘페이스북의 친구와 인스타그램의 팔로우’는 directed와 undirected graph를 이해하기에 좋은 예이다. . Node Degrees (차수) . undirected graph undirected graph의 경우, node degree는 해당 노드에 연결된 에지의 총 갯수 | average degree는 연결된 에지의 총 갯수 곱하기 2(쌍방향 연결이기 때문)를 그래프를 이루는 전체 노드 수로 나눈 값 | . | directed graph directed graph의 경우, 해당 노드로 향하는 in-degree와 해당 노드로부터 뻗어나가는 out-degree로 나눌 수 있다. node degree는 이 in-degree와 out-degree의 합. | . | . Bipartite Graph . . 자주 등장하는 또다른 그래프의 종류는 bipartite graph(이분 그래프)이다. | 이분 그래프는 2개의 집합 U와 V의 interaction을 나타낸 그래프이다. U와 V는 서로 독립적인 집합이며, 같은 집합의 원소끼리는 연관되지 않는다. e.g) A와 B 간 연결X | 이분 그래프의 예로는 구매자-구매 아이템 관계 등이 있다. | . Folded/Projected Bipartite Graph . Bipartite 그래프에서 집합 간의 요소들 간의 상관관계가 명시되어있다면 Folded 또는 Projected Bipartite Graph라고 부른다. | . (정보에 depth가 생긴다는 의미에서 folded라고 붙인듯하다.) . . Adjacency Matrix . . 그래프의 노드 간 연결관계(edge)를 나타낸 매트릭스이다. | 각 행과 열은 노드의 번수를 의미하고, 0은 연결되지 않음, 1은 연결됨을 의미한다. 만약 3번째 행에 4번째 열이 1이라면, 3번 노드와 4번 노드는 연결되어있음을 의미한다. | undirect graph라면, 주대각선을 기준으로 adj matrix는 대칭이고, directed graph라면, 대칭이 아닐 수도 있다. | adjacency matrix는 컴퓨터가 그래프를 이해할 수 있는 형태이지만, 문제는 노드의 수가 수백개에서 수십만개로 늘어나고, 많은 노드들의 연결이 몇 개 없을 때, 메모리 사이즈에 비해 0인 값이 너무 많게되는 문제(sparse)가 발생한다. | . Edge List . . Edge List는 그래프를 엣지들의 리스트로 나타낸 값이다. | 서로 연결된 노드를 짝지어 리스트에 배열한다. | 그래프가 크고 sparse할 때 유용하다. | . Node and Edge Attributes . ⇒ 노드와 엣지로 나타낼 수 있는 값들은 어떤 것들이 있을까? . ⇒ 그래프에서 어떤 정보들을 얻을 수 있을까? . weight (e.g., frequency of communication) 엣지가 0과 1 이외의 값을 갖는다면? | . . | ranking (best friend, second best friend, …) | type (friend, relative, co-worker) | sign (+ / - ) | properties depending on the structure of the rest of the graph : Number of common friends | . More Types of Graphs . Connected(undirected) graph | . . **연결이 부분적으로만 되어 있어도, 그래프는 adjacency matrix에 표현 가능하다. . . Connectivity of Driected Graphs . Strongly connected directed graph 모든 노드들이 다른 모든 노드들로 방향 상관없이 다다르는 path가 항상 있다면, strongly connected directed graph이다. | . | Weakly connected directed graph 에지 방향을 무시했을 때, 노드 간 전부 연결되어있다면, weakly connected directed graph이다. | . | Strongly connected components(SCC) . . 그래프에 속한 다른 노드들 전부는 아니지만, 해당 그룹 간의 연결이 한 노드에서 다른 노드로 항상 도달할 수 있다면(strong connection)한다면, 그 그룹을 strongly connected components라고 지칭한다. | . | .",
            "url": "https://cs224w-kor.github.io/blog/why%20graphs/applications%20of%20graphml/choice%20of%20graph%20representation/2022/07/06/lecture-01.html",
            "relUrl": "/why%20graphs/applications%20of%20graphml/choice%20of%20graph%20representation/2022/07/06/lecture-01.html",
            "date": " • Jul 6, 2022"
        }
        
    
  
    
        ,"post26": {
            "title": "Lecture N.1 - 강의 소제목(분할되어 있는 동영상 제목)",
            "content": "위에 title에 특수문자 :나 []를 사용하지 마세요! . .md 파일의 제목은 날짜-lecture-0N0n.md로 작성해주세요.(2022-07-13-lecture-0401) . Lecture N. 강의 제목 . N 강의의 첫번째 포스트에 강의에 대한 전반적인 소개를 작성합니다. &gt; 표시를 앞에 작성합니다. . Imgur로 이미지도 넣을 수 있습니다.(아래에 Imgur에 대해 설명있습니다.) 설명설명 . 강의가 세분화되어 있어서 나누어진 포스팅들을 모아서 한번에 보여주기 위해 링크를 답니다. . Lecture N.1 - 소제목1 | Lecture N.2 - 소제목2 | Lecture N.3 - 소제목3 | Lecture N.4 - 소제목4 | . . 제목 . 제목같은 경우는 자유롭게 작성하셔도 되나 저는 강의 자료 ppt를 기반으로 해서 섹션 제목(약 80% 정도)을 가져왔습니다. . 소제목 . #, ## 등 제목을 세분화하면 좋겠지만 너무 많이 제목을 쓰면 위에 TOC가 길어져서 보기 좋지 않을 것 같아 제목은 #만 쓰고 하위 제목으로는 **소제목**을 사용해서 작성했습니다. . 이미지 . 이미지는 깃헙에 직접업로드한 후 markdown으로 작성할 수 있으나 그러면 추후에 용량문제가 생긴다고 해서 Imgur를 이용해서 포스팅합니다. . 계정을 만들어야 합니다. | 계정 아이콘의 아래에 있는 Images를 누룹니다. . . | 업로드 하고자 하는 이미지를 드래그 앤 드랍을 합니다. | 업로드 된 이미지 위로 커서를 가져가면 편집 아이콘이 뜨고 이걸 눌러서 이미지 크기를 조절합니다. (이미지 크기를 조절할때마다 이미지 주소링크가 달라지기 때문에 글에 markdown 링크를 가져오기 전에 조정하는 것이 좋습니다. 여러개 이미지들을 눌러서 일일이 누르지 않고 연속적으로 편집할 수 있습니다.) . . 저는 개인적으로 가로 800픽셀이 넘어가지 않는 선에서 width만 조절하면 나머지 height는 비율에 맞게 저장 됩니다. 편집후 위에 있는 save를 눌러 저장해주세요. . | 크기 편집을 다 한 후 이미지를 클릭하면 markdown link를 복사할 수 있고 이를 md 파일에 붙여넣으면 됩니다. | 이때 `!`없이 링크가 복사되는데 md 파일에 가져올때 앞에 `!`를 붙여주세요. (이 부분은 노션에서 `callout`으로 작성하면 생기는 부분입니다. **raw text**만 인식되며 굵은 글씨, 코드 포맷 등 다 지원되지 않습니다. 수식은 지원됩니다. $a$) 표 . title 1 2 3 . 일반적인 | 작성 양식을 | 따릅니다. | :) | . 토글 . html을 이용하여 토글을 할 수 있습니다. 하지만 이는 노션의 callout과 마찬가지로 안에 내용은 raw text만 지원됩니다. . 토글된 제목 숨겨진 내용은 raw text만 됩니다. 더 다양한 작성 형식 . 은 2020-01-14-test-markdown-post.md 공식 예시글을 참고해주세요. 여기에 없는 다른 기능들은 보통 html 형식을 맞춰주면 적용이 되는 것 같습니다. . 작성 흐름 . 작성하는 흐름은 팁공유하는 차원으로 제가 하고 있는 방식을 말씀드리겠습니다. . 노션에 강의 내용 정리 | 노션에서 export를 markdown으로 해서 zip 파일로 받은 후 압축해제하면 안에 md 파일과 이미지들이 모아져 있는 폴더가 생깁니다. | blog repository에 다운받은 md 파일을 복붙해서 조금의 편집을 합니다.(위에 작성해야 하는 항목(title, description, tag 등), 수식, 인터넷 링크확인) | Imgur에 접속해서 2번에서 받은 이미지들을 한번에 업로드하고 편집한 후 markdown link들을 md 파일에 가져옵니다. | 포스팅 맨 아래 원래 동영상 링크를 첨부합니다. | 깃헙에 commit 합니다. fastpages에서 만들어놓은 action이 실행된 후 조금 시간이 흐른 후 포스팅이 올라옵니다. | 원하는 대로 포스팅이 제대로 올라왔는지 확인합니다. 이과정을 여러번 commit하면서 확인해야 할 수도 있습니다. | 꼭 이 순서대로 작성하실 필요는 없으며 처음에 좀 익숙해지는데 시간이 걸립니다. . 이모지 작성가능합니다. https://getemoji.com/ 제가 주로 이용하는 사이트입니다. | . 💡 이외에 혹시 잘 모르겠는 부분은 말씀해주세요! 😀 . . Original Lecture Video : 원래 유튜브 동영상 제목을 그대로 복붙하되 |은 제외하고 작성해주세요. 저 문자가 있을 경우 표로 인식됩니다. .",
            "url": "https://cs224w-kor.github.io/blog/tag1/tag2/keyword1/2022/07/01/template-0000.html",
            "relUrl": "/tag1/tag2/keyword1/2022/07/01/template-0000.html",
            "date": " • Jul 1, 2022"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "Questions",
          "content": "스터디에서 같이 공부하며 나왔던 질문들에 대해 정리해보았습니다. 토의하며 정리한 것이니 절대적인 답은 아님을 주의해주세요. . Q: QUESTION | . A: blabla | . Contributers . Alphabetical order . JungYeon Lee | person2 | person3 | person4 | person5 | person6 | person7 | .",
          "url": "https://cs224w-kor.github.io/blog/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  

  
  

  
      ,"page11": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://cs224w-kor.github.io/blog/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}