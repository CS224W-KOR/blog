{
  
    
        "post0": {
            "title": "Lecture 0603",
            "content": "Lecture 6.3 . **Lecture 6.3 - Deep Learning for Graphs** . 이제 본격적으로 이번 강의의 메인 주제였던 딥러닝을 활용한 그래프 데이터 임베딩 방법에 대해 공부해 봅시다. 참고로 Deep Encoder은 그래프 뉴럴 네트워크(GNN)으로도 부르기 때문에 혼재해서 사용하더라도 헷갈리지 않으시길 바랍니다. . Setup . 들어가기에 앞서 반복적으로 활용할 notation에 대해 간략히 설명하고 시작하겠습니다. 앞으로 아래 기호를 쭉 사용하여 설명을 할 예정이니 잘 알아두시기 바랍니다. . $V$: 노드 집합 | $A$: 인접 행렬 (Adjacency matrix) | $X in mathbb{R}^{m times | V | }$: 노드 features | . | $v$: 노드 집합에 포함된 한 노드, $N(v)$: $v$의 이웃 노드 | . 그래프 구조는 엣지의 방향성 및 가중 여부에 따라 여러 종류로 분류할 수 있지만, 이해를 위해 여기서는 가장 간단한 undirected &amp; unweighted 그래프로 설명하겠습니다. 따라서 인접 행렬은 0과 1로 이루어진, 대각 방향으로 symmetric한 행렬이라고 생각하실 수 있습니다. . 또한, 이전의 Shallow Encoder과는 달리 이제 노드 feature도 함께 고려하여 노드 임베딩을 학습한다는 점에 유의하시기 바랍니다. . 사실 대부분의 그래프 데이터셋은 노드 feature을 포함하고 있지만, 만에 하나 없는 경우라면 다음과 같은 벡터/값을 노드 feature로 사용하기도 합니다. 노드의 one-hot 인코딩 벡터 | 상수 벡터 [1, 1, …, 1] | 노드 차수(degree) | . | . Naive Approach . 딥러닝 모델을 활용하여 그래프 및 노드를 임베딩 하기 위해 가장 쉽게 생각할 수 있는 방법은 단순하게 그래프의 구조적인 특성을 나타내는 인접 행렬과 노드 feature 행렬을 그냥 이어 붙여서 딥러닝 모델에 던져주는 것입니다. . . 위와 같은 undirected 그래프의 각 노드가 2차원의 feature를 각각 가지고 있다면, 단순히 두 행렬을 이어 붙여서 만든 7차원의 벡터를 뉴럴 네트워크에 전달하면 각 노드를 간단히 임베딩 할 수 있을 것입니다. 하지만 이러한 방법에는 다음과 같은 문제가 존재합니다. . $**O( | V | )$ 파라미터가 필요함** | . 노드 feature이 $d$차원이라고 가정하면, 각 노드가 뉴럴 네트워크에 입력되는 차원이 $ | V | +d$ 이겠죠? 따라서 그래프의 노드 갯수에 비례하여 모델의 파라미터가 증가합니다. | . | 다른 사이즈의 그래프에는 적용할 수 없음 위와 같은 그래프에 대해 뉴럴 네트워크를 기껏 학습시켜 놓았는데, 100개의 노드로 구성된 새로운 그래프가 인풋으로 들어온다면, 인풋 차원이 맞지 않아 학습시킨 임베딩 모델을 활용할 수 없을 것입니다. | 노드 순서가 바뀌면 동일 노드의 임베딩이 달라질 수 있음 위 그래프에서 노드 순서를 A→B→C→D→E에서 B→E→A→C→D 등으로 바꾼다면, 이에 따라 인접 행렬도 바뀌게 됩니다. 이렇게 된다면 같은 A 노드를 임베딩 하기 위해 인풋으로 활용되는 7차원의 벡터가 달라지기 때문에 임베딩 결과 값도 달라질 것입니다. | . From Images to Graphs . 그렇다면 기존 CNN 모델에서 아이디어를 차용해보는 건 어떨까요? . . . 이미지를 다루는 CNN은 위와 같이 고정된 사이즈의 convolution 필터를 사용하여 이미지를 주욱 훑습니다. 여기서의 convolution 필터는 붉은 원으로 표시된 타겟 픽셀의 주위 픽셀 정보를 축약하는 역할을 합니다. 사실 이미지가 특수한 형태의 그래프로 해석될 수 있음을 생각해보면, 그래프 데이터에서도 타겟 노드의 임베딩을 만들기 위해 주변 노드의 정보를 사용한다는 아이디어는 나쁘지 않아 보입니다. . 하지만 그래프에서는 CNN에서와 같이 고정된 크기의 필터(?)를 사용할 수 없습니다. 어떤 노드는 한두개의 이웃 노드를 가지지만 또 어떤 노드는 수백수천개의 이웃 노드를 가질 수 있기 때문이죠. . . 그렇다면 그래프를 임베딩 할 때 타겟 노드의 이웃 노드에서 정보를 전달받아 이를 활용하여 타겟 노드의 임베딩을 업데이트 하되, 타겟 노드마다 이웃 노드의 갯수가 다를 수 있는 점을 고려하여 각기 다른 computation graph를 갖도록 하는 것이 좋겠습니다! . 다음과 같은 아이디어를 근간으로 Graph Convolutional Network가 등장하게 되었습니다. 남은 강의에서는 이 GCN을 디테일하게 설명하고 있습니다. . Idea: Aggregate Neighbors . 주요 Idea: 이웃 노드 정보를 가지고 타겟 노드 임베딩을 생성하자! . . 왼쪽과 같은 그래프에서 우리가 임베딩하고 싶은 타겟 노드가 노란색의 A 노드라고 생각해 봅시다. 그렇다면 A 노드의 이웃 노드, 그리고 그 이웃 노드들의 이웃 노드를 가지고 오른쪽과 같은 computation graph가 생깁니다. . 타겟 노드 A는 직속 이웃인 노드 B, C, D로부터 메시지를 전달 받고, 모든 메시지를 합친 후 어떠한 변환을 거쳐 본인의 임베딩으로 활용합니다. 우측 그림에서 회색 박스로 표시된 뉴럴 네트워크가 바로 이러한 1) 메시지 변환, 2) 이웃 노드로부터 온 메시지를 통합하는 두 과정을 수행하게 됩니다. 이 뉴럴 네트워크 내의 모델 파라미터가 최종적인 우리의 학습 대상이 되는 것입니다. . 여기서 또 눈여겨 보아야 할 점이 있습니다. 여지껏 우리는 노란색 A 노드를 타겟 노드로 한 computation graph만 보았는데, 그렇다면 B, C, D 등 다른 타겟 노드에 대해서도 동일한 computation graph를 가질까요? . . 아닙니다. 노드마다 이웃 노드의 갯수와 종류가 다르기 때문에 당연히 노드마다 서로 다른 computation graph를 가지게 됩니다. . Deep Model: Many Layers . . Layer의 수 . Deep Encoder은 여러 layer로 구성할 수 있는데, 한 layer이 직속 이웃 노드에 대한 정보를 aggregate하는 것이기 때문에 layer을 두 개 쌓는다면 직속 이웃 노드에 대한 이웃 노드, 즉 타겟 노드로부터 2-hop 떨어진 노드의 정보까지 활용하겠다는 의미로 해석할 수 있습니다. 위 그림에서도 잘 나타나 있는데, 2개의 layer로 구성된 모델을 사용하기 때문에 타겟 노드 A로부터 2-hop 떨어진 노드 E, F의 정보도 임베딩 생성에 활용되는 것을 볼 수 있습니다. . 노드 임베딩 초기화 . 또한, 보통 Layer-0에서 최초 노드 임베딩으로 노드 feature을 사용합니다. 모든 노드 임베딩은 layer을 거칠수록 이웃 노드의 정보를 변환하고 합친 후 업데이트 됩니다. 결국 모든 layer을 거치고 나면 최종 노드 임베딩이 생성되어 우리가 downstream task를 위해 사용하게 되는 것이죠. . Aggregator 함수 . 여기서 타겟 노드 A가 이웃 노드 B, C, D의 메시지를 aggregation 할 때, 노드 B, C, D의 순서와 관계 없이 aggregate된 메시지는 동일해야 합니다. 즉, 메시지를 aggregate하는 함수는 permutation-invariant한 속성을 가져야 한다는 말입니다. 일반적인 GNN은 주로 합, 평균, 맥스 풀링등의 aggregator를 활용합니다. . The Math: Deep Encoder . . 자, 이제 가장 기본적인 GNN 형태를 정의하고 이를 수식으로 나타내어 알고리즘의 원리를 자세히 들여다보는 시간을 갖겠습니다. 우리의 GNN은 타겟 노드의 이웃 노드 임베딩을 전달받아 이를 평균냄으로써 메시지를 aggregate 합니다. 그 후, 뉴럴 네트워크를 통해 어떠한 변환을 거치고 이를 활용하여 타겟 노드 임베딩을 업데이트 합니다. 이를 수식으로 나타내면 아래와 같습니다. . . 식이 처음엔 되게 복잡해 보이지만, 하나씩 뜯어보면 사실 아주 간단합니다. 식 전반에 나타나는 $h_{v}^{(l)}$ 은 $l$번째 layer에서 노드 $v$의 임베딩을 나타낸다고 보시면 됩니다. . 초록색 수식 블럭 : 첫 노드 임베딩을 노드 feature로 초기화합니다. | 파란색 수식 블럭 : $l+1$번째 layer에서의 노드 임베딩을 만들기 위해, 먼저 타겟 노드 $v$의 이웃 노드에 대해 $l$ 번째 layer에서의 노드 임베딩 평균을 구합니다. 그 후, 이웃 노드의 평균 임베딩에 어떠한 transformation $W_{l}$ 을 가합니다. | 빨간색 수식 블럭 : 타겟 노드의 임베딩을 업데이트할 때 이웃 노드 뿐 아니라, 이전 layer에서 갖고 있던 자기 자신의 임베딩도 활용합니다. 같은 방법으로 $h_{v}^{(l)}$에 어떠한 transformation $B_{l}$ 을 가합니다. | 노란색 수식 블럭 : 최종적으로 비선형 함수를 적용해서 $l+1$번째 layer에서의 타겟 노드 임베딩을 구합니다. | 보라색 수식 블럭 : 노드 임베딩 업데이트 과정을 $L$번 반복합니다. 이 때 최종적으로 형성된 노드 임베딩은 본인으로부터 L-hop 떨어진 노드의 정보까지 활용하여 만든 것입니다. | Matrix Formulation . 이전에 Random Walk를 행렬 형태로 표현했듯이, 벡터식으로 다뤘던 GNN도 행렬식으로 다시 표현해보겠습니다. . 모든 노드 임베딩 벡터를 한데 모아 노드 임베딩 행렬을 만든다면, 이는 아래와 같이 표현할 수 있습니다. . $H^{L} = {[h_{1}^{(l)} ... h_{|V|}^{(l)}]}^T$ . 그렇다면 이웃 노드의 임베딩을 합산하는 과정은 그래프의 인접 행렬을 사용하여 아래와 같이 간단하게 나타낼 수 있습니다. . $ sum_{u in N_{v}} h_{u}^{(l)} = A_{v:}H^{(l)}$ . 만약 대각 행렬을 이렇게 정의한다면, 이 대각 행렬의 역행렬은 다음과 같기 때문에, . $D_{v,v} = Deg(v) = |N(v)|$ , $D_{v,v}^{-1} = 1/|N(v)|$ . 이를 활용하면 이웃 노드의 임베딩을 평균내는 연산을 행렬식으로 간단하게 표현할 수 있습니다. . . 따라서 최종적으로 노드 임베딩 업데이트 함수를 행렬식으로 나타내면 아래와 같습니다. . . 벡터식으로 이해하기도 어려웠는데 왜 굳이 사서 행렬식으로 변환하냐고요? 사실 행렬식이 가지는 구현상의 이점이 있기 때문입니다. 행렬식을 사용한다면 각 노드에 대한 임베딩을 따로 따로 업데이트 하지 않고 하나의 행렬로써 한번에 업데이트 할 수 있으며, 이 과정에서 $ tilde{A}$가 희소 행렬이기 때문에 보다 더 효율적인 행렬 연산이 가능하기 때문에 구현할 때는 행렬식이 더 선호됩니다. . How to train a GNN . 오늘 강의의 마지막 부분으로 이렇게 구성한 GNN을 어떻게 학습시켜야 하는지 알아보겠습니다. 시작하기에 앞서 학습의 대상이 되는 파라미터가 무엇인지 짚고 넘어가보죠. . . 다음 식에서 학습되는 파라미터는 $W_{l}$와 $B_{l}$입니다. 딸린 subscript를 봐도 알 수 있듯이, 두 파라미터 행렬은 모두 layer마다 따로 존재하며, 한 layer 내에서는 공유됩니다. 이를 간단하게 그림으로 표현하면 아래와 같습니다. . . GNN을 학습하는 방법은 여느 딥러닝 학습과 마찬가지로 크게 지도 학습, 비지도 학습 세팅으로 나뉩니다. 이를 하나씩 살펴보도록 합시다. . 지도 학습 세팅 노드 label $y$가 존재하는 상황 | 정답 노드 label $y$를 활용하여 $min_{ theta} mathcal{L}(y,f(z_v))$를 풂, 이 때 task에 맞게 L2 loss 혹은 Cross entropy loss 등을 loss 함수를 사용함 | 예시) 각 노드가 safe한지 혹은 toxic한지 분류하는 node classification → 분류 문제이므로 cross-entropy loss를 사용하고, 노드의 정답 클래스 label을 활용하여 직접 모델 학습 가능 . 💡 아래 loss 식에서 $ sigma(z_v^T theta)$는 학습된 노드 임베딩 $z_v^T$를 가지고 **모델이 예측한 노드 $v$의 클래스 확률**을 나타냅니다. | . | . . 비지도 학습 세팅 노드 label이 존재하지 않는 상황 | 3강에서 공부했던 그래프 상 노드 similarity를 supervision으로 활용 . . 임의의 supervision 시그널을 만들어 비지도 학습 세팅을 지도 학습 세팅으로 바꾸기 위해서 원본 그래프에서의 노드 similarity를 바탕으로 label을 지정해줍니다. 여기서 노드 similarity는 3강에서 다뤘던 Random Walk 등을 활용할 수 있습니다. . 간단하게 DeepWalk로 노드 similarity를 정의하는 경우를 생각해볼까요? 만약 두 노드 $u$와 $v$가 랜덤 워크 상에서 co-occur한다면 두 노드는 ‘similar’하다고 말할 수 있으며, $y_{u,v} = 1$로 임의의 label을 붙입니다. 또한 여기서 $DEC(z_u,z_v)$는 학습된 두 노드의 임베딩을 내적함으로써 임베딩 공간에서의 노드 similarity를 측정합니다. Loss 함수로 cross-entropy를 사용함으로써 그래프에서 노드 similarity를 최대한 잘 보존하도록 노드 임베딩을 학습할 수 있게 됩니다. . 💡 **원본 그래프에서 similar한 노드는 → similar한 임베딩을 갖도록 합니다** ![Untitled](Lecture%206%203%2097ef7a55859549649d9652414b97de2c/Untitled%2016.png) | . | Model Design: Overview . 자 그럼, 오늘 배웠던 내용을 한번 쭉 훑어 정리하고 포스트를 마무리하도록 하겠습니다. 그래프를 위한 Deep Encoder, a.k.a. GNN 모델을 만들기 위해서 아래와 같은 단계를 따라가야 합니다. . 이웃 노드 임베딩을 aggregate하는 함수를 정함 | Task의 특성에 맞추어 loss 함수를 정의함 | . . 여러 computation graph에 대해 GNN 모델을 학습시킴 | 학습된 모델을 갖고 노드에 대한 임베딩을 생성할 수 있음. 이 때, 모든 노드에 대해 뉴럴 네트워크의 파라미터가 공유되기 때문에 학습에 사용되지 않은 노드에 대해서도 임베딩을 생성할 수 있음 (Inductive Capability) | . 💡 **Inductive Capability** 1. 새로운 그래프: 예를 들어 분자 그래프에서, 화합물 A에 대해 학습된 GNN 모델이 화합물 B 그래프에서 임베딩을 만드는데 활용될 수 있음 2. 새로운 노드: 시간에 따라 evolving 하는 그래프에서 새로운 노드가 추가될 때마다 바로바로 임베딩을 생성할 수 있음 Summary . Deep Encoder (GNN)의 핵심 아이디어: 이웃 노드의 정보를 aggregate 함으로써 노드 임베딩을 생성하자! | 모델 내 Aggregator과 Transformation 함수를 각각 어떻게 정의하느냐에 따라 모델 구조가 달라질 수 있습니다. | 다음 강의에선 GNN variant의 하나인 GraphSAGE를 다룰 것입니다. | . .",
            "url": "https://cs224w-kor.github.io/blog/2022/07/27/lecture-0603.html",
            "relUrl": "/2022/07/27/lecture-0603.html",
            "date": " • Jul 27, 2022"
        }
        
    
  
    
        ,"post1": {
            "title": "Lecture 0602",
            "content": "**Lecture 6.2 - Basics of Deep Learning** . 이번 파트는 본격적으로 그래프 데이터를 위한 딥러닝을 설명하기에 앞서 딥러닝에 익숙하지 않은 사람들을 위해 딥러닝의 기본 개념을 간단하게 설명하는 부분입니다. 많은 내용을 커버하긴 하지만 딥러닝 초심자들은 CS231n과 같은 딥러닝 강의를 먼저 수강하고 오심을 추천드립니다! . Machine Learning as Optimization . 먼저 간단한 지도 학습 문제를 생각해 봅시다. 지도 학습(Supervised Learning)이란 데이터에 대한 ground truth label이 존재하는 경우를 일컫는데, 다시 말해 input $x$가 주어졌을 때, label $y$를 예측하는 문제라고 할 수 있습니다. 이러한 task는 아래와 같은 식을 통해 최적화 문제로 바꾸어 해결할 수 있습니다. . . 위 식을 이해하기 위해 먼저 구성 요소에 대해 하나씩 짚어보겠습니다. . $ theta$ : 우리가 최적화(학습) 하려는 파라미터들 . 최종적으로 우리가 학습하고자 하는 파라미터 값입니다. 예를 들어 앞의 Shallow Encoder에서는 학습으로 결정되는 $ | V | times d$ 사이즈의 임베딩 look-up table이 $ theta$에 해당하겠죠. | . $ mathcal{L}$ : Loss 함수 . Loss 함수는 ground truth label $y$와 머신러닝 모델이 예측한 label $f(x)$의 차이를 계산하는 metric 입니다. 회귀(Regression) 문제에서 주로 사용되는 L2 loss와 분류(Classification) 문제에서 주로 사용되는 Cross entropy loss 이외에도 L1 loss, Huber loss, Hinge loss 등 다양한 loss 함수가 존재합니다. 대표적인 loss 함수인 L2 loss와 Cross entropy loss의 수식은 각각 다음과 같습니다. . L2 loss . . Cross entropy loss . . . 결국 우리가 원하는 것은 모델이 예측한 label이 정답 ground truth label에 가까워지는 것이기 때문에, 이 loss 함수 값이 작으면 작을수록 우리의 모델이 더 정확한 예측을 한다는 의미입니다. . . 그럼 이제 위에서 공부한 각 구성 요소를 바탕으로 다시 이 최적화 식을 해석해 봅시다. 결국 우리가 풀고자 하는 머신러닝 문제는, 정답 label $y$와 모델이 예측한 label $f(x)$의 차이를 나타내는 loss 함수를 최소화 하는 방향으로 모델 파라미터 $ theta$를 최적화하는 문제로 해석할 수 있습니다. # . Gradient Descent . 지금까진 두루뭉술하게만 보였던 머신러닝 문제를 동일한 의미인 최적화 문제로 재정의했습니다. 그렇다면 이 최적화 문제를 어떻게 해결해야 할까요? . . (출처: https://ieeexplore.ieee.org/abstract/document/9298092) . 우리는 일반적으로 Loss 함수로 convex function(볼록 함수)를 활용합니다. 이 loss 함수의 값이 작아지는 방향으로 모델 파라미터 $ theta$를 업데이트 하기 위해, $ theta$에 대한 $ mathcal{L}$의 기울기를 구한 후, 기울기가 작아지는 방향으로 $ theta$를 업데이트 해줍니다. . (위 그림에서 Cost를 $ mathcal{L}$, Weights를 $ theta$로 보시면 됩니다!) . . 이를 다시 Gradient 벡터라는 개념으로 정리해서 이야기 해보겠습니다. Gradient 벡터란 위의 식과 같이 $ theta$에 대한 $ mathcal{L}$의 편미분, 즉 기울기 값으로 구성된 벡터로써, 가장 빠르게 $ mathcal{L}$이 증가하는 방향을 나타내는 방향 도함수 벡터입니다. . 💡 Gradient is the directional derivative in the direction of largest increase 일단 Gradient를 구했으면 이제 할 일은 모델 파라미터 $ theta$를 gradient의 반대방향으로($ mathcal{L}$이 작아지는 방향으로) 반복적으로 업데이트 하는 것입니다. . . 위 식에서 $ eta$는 learning rate로, 한번 파라미터를 업데이트 시 얼마나 변경할 것인지 보폭을 나타내는 값입니다. 이는 학습 과정 동안 동일하게 유지할 수도 있고, 목적에 따라 계속 변하게 할 수도 있습니다. 이론적으로 모델 파라미터의 업데이트는 loss 함수의 local minimum에 도달하여 gradient가 0이 될 때까지 진행하는 것이 맞지만, 실전에서는 검증 데이터셋에서의 성능이 더 이상 증가하지 않는 기점에서 모델 파라미터 업데이트를 중단합니다. # . Stochastic Gradient Descent (SGD) . Gradient Descent의 문제점 . Gradient descent 방법으로 최적화 문제를 푸는 것은 이론적으론 무결하지만 현실적으로는 어렵습니다. 앞서 언급했듯이, Gradient 벡터를 계산하기 위해서는 전체 데이터에 대한 loss 값을 구해야 하기 때문에 몇십억개의 데이터를 갖는 오늘날의 데이터셋에 적용하기에는 계산적으로 무리가 있습니다. . . (출처: https://www.slideshare.net/w0ong/ss-82372826) . 따라서 전체 데이터셋을 작은 미니 배치로 나누어 모델 파라미터를 업데이트하는 SGD 방법이 등장하게 되었습니다. 미니 배치 마다 gradient를 구하고 모델 파라미터를 업데이트하는 것이 전체 데이터셋을 활용한 모델 업데이트 정도를 근사할 수 있기 때문이죠. . 💡 SGD is unbiased estimator of full gradient 오늘날의 최적화 optimizer은 SGD의 여러 발전된 형태로써, Adam, Adagrad, Adadelta, RMSprop 등이 있습니다. # . Back-propagation . 지금까지 머신러닝 문제를 최적화 문제로 재정의하고, 최적화 문제를 풀기 위해 gradient를 활용해 모델 파라미터를 업데이트 하는 방법에 대해서 배웠습니다. 잘 따라오고 계신가요? . 이제부터는 실질적으로 머신러닝 모델이 주어졌을 때, gradient를 계산하는 방법에 대해 알아보겠습니다. 오늘날의 머신러닝/딥러닝 모델은 아주 복잡한 형태를 가지지만, 일단 이해의 편의를 돕기 위해 가장 간단한 linear function를 예시로 설명하겠습니다. . Case 1 . . 첫번째로 우리의 머신러닝 모델이 단순한 선형 변환 함수인 경우를 다뤄보겠습니다. . . 선형 변환 함수가 벡터 형태이든 행렬 형태이든 관계 없이 gradient는 이렇게 간단히 구할 수 있습니다. . Case 2 . . . 두번째로 조금 더 복잡한 형태의 모델로 확장해보겠습니다. (물론 아직 엄청 단순한 형태긴 하지만..) 이 경우에는 $W_{1}$과 $W_{2}$에 대해 모두 gradient를 구해야 합니다. 여기서 우리가 고등학교 때 배운 합성함수 미분에서의 연쇄법칙이 사용됩니다. . 💡 Chain Rule (연쇄법칙) ![Imgur](https://i.imgur.com/IqvyTZ1.png?1) . . . 연쇄법칙에 따라 gradient가 $ mathcal{L}$ → $f(x)$ → $W_{2}$ → $W_{1}$ 을 따라 차례로 거꾸로 흐르면서 계산됩니다. 이렇게 말단에서부터 앞쪽까지 gradient가 흘러오기 때문에 역전파(back-propagation)이라는 이름이 붙었다고 합니다. # . Non-linearity . 지금까지 예시로써 다뤄본 두 케이스는 사실 모두 선형 모델로써, 비선형적인 데이터를 잘 모델링할 수 없습니다. 따라서 오늘날의 머신러닝 모델은 비선형적인 활성 함수(Activation function)를 도입함으로써 이러한 문제를 해결합니다. 대표적인 비선형 함수로는 ReLU, Sigmoid 등이 있습니다. . # . Multi-layer Perceptron (MLP) . . MLP란 한 layer마다 선형 변환과 비선형 변환이 합쳐진 가장 기본적인 형태의 머신러닝 모델이라고 볼 수 있습니다. 위 식은 MLP 한 layer을 나타내는데, layer $l$ 의 인풋으로 들어온 $x^{(l)}$에 $W_{l}$이 곱해져 선형 변환 된 후 bias 항이 더해집니다. 최종적으로 비선형 함수를 거친 아웃풋이 layer $l+1$의 인풋으로 전달됩니다. 이를 그림으로 나타내면 다음과 같습니다. . # . Summary . 지금까지 간단하게 배운 딥러닝의 기본 개념을 정리해보고, 본격적인 오늘 강의의 주제로 넘어가겠습니다. . 머신러닝 문제는 최적화 문제로 풀 수 있습니다. | . . 모델 $f$는 간단한 선형 함수, MLP, 또는 다른 형태의 신경망일 수 있습니다. (e.g., 추후에 다룰 GNN도 가능합니다) | 먼저 전체 데이터셋을 미니배치로 나누어 인풋 $x$으로 사용합니다. | 순전파(Forward Propagation): $x$가 주어졌을 때 loss 함수 값 $ mathcal{L}$ 구하기 | 역전파(Backward Propagation): 연쇄법칙으로 gradient $ nabla_{ theta} mathcal{L}$ 구하기 | SGD를 활용하여 반복적으로 모델 파라미터 $ theta$를 최적화합니다. | . .",
            "url": "https://cs224w-kor.github.io/blog/2022/07/27/lecture-0602.html",
            "relUrl": "/2022/07/27/lecture-0602.html",
            "date": " • Jul 27, 2022"
        }
        
    
  
    
        ,"post2": {
            "title": "Lecture 0601",
            "content": "**Lecture 6.1 - Introduction to Graph Neural Networks** . Recap: Shallow Encoders . 이전 강의에서 배운 내용을 다시 떠올려 봅시다. 다양한 downstream task를 머신러닝으로 푸는 과정에서 비정형 데이터인 그래프 인풋을 활용하기 위해 그래프를 임베딩하는 방법을 공부했었습니다. . Intuition 그래프 상에서 similar한 노드끼리 임베딩 공간에서도 가깝도록 임베딩 하자! . . 임베딩 공간에서의 노드 간 similarity는 간단하게 코사인 유사도를 통해 구할 수 있지만, 1) 원래 그래프 상에서의 노드 간 similarity와 2) 노드를 임베딩 벡터로 mapping하는 encoder은 우리가 새로 정의해야 합니다. . Encoder: Shallow Encoder . . 먼저, 인풋 그래프의 노드를 d차원의 벡터로 임베딩하기 위해 가장 간단한 look-up table 방식인 Shallow Encoder를 다뤘습니다. 예를 들어, 노드의 갯수가 $ | V | $인 경우 임베딩 행렬의 크기는 $ | V | times d$ 가 됩니다. | . | Similarity Function: Random Walk (DeepWalk, Node2vec) 또한, Random Walk상에서 co-occur 되는 두 노드는 그래프 상 similar한 노드라고 정의하였습니다. Random Walk의 전략에 따라 서로 다른 DeepWalk와 Node2vec 등의 방법을 배웠던 것 기억 나시나요? # . Limitations of Shallow Encoder . | 이렇듯 Shallow Encoder을 통해서도 성공적으로 노드와 그래프를 임베딩할 수 있지만, 다음과 같은 한계점 때문에 보다 더 고도화된 Encoder를 재정의 할 필요가 있습니다. . $O( | V | )$ 파라미터가 필요함 | . 그래프의 크기가 커지면 커질수록, 즉 노드의 갯수 $ | V | $가 증가함에 따라, 임베딩 행렬의 크기도 선형적으로 증가합니다. 또한 각 노드가 모두 서로 다른 d 차원의 임베딩 벡터를 가지기 때문에 파라미터 공유도 일어나지 않습니다. | . | Transductivity . 💡 Transductive Learning 그래프 학습 관점에서 Transductive Learning이란 하나의 그래프 상 일부 노드와 엣지의 ground truth를 아는 상태에서 나머지 노드와 엣지의 값을 추정하는 방식입니다. 학습 과정 중, 모델은 ground truth를 알지 못하는 노드를 포함한 모든 노드와 엣지를 사용합니다. ![Imgur](https://i.imgur.com/Jpte1Hu.png?1) 💡 Inductive Learning 그래프 학습 관점에서 Inductive Learning이란 ground truth를 알고 있는 그래프(들)에 대해 모델을 학습 한 후, 전혀 새로운 그래프의 노드와 엣지의 값을 추정하는 방식입니다. 즉, 학습이 완료된 후에는 모델이 새로운 처음 보는 노드의 값을 추정하는 데에도 적용될 수 있다는 의미이죠. ![Imgur](https://i.imgur.com/Fi3vDRg.png?1) Shallow Encoder은 Transductive Learning으로 학습해야 하는 대표적인 케이스입니다. 학습 도중 보지 못한 노드는 look-up table상 존재할 리 없으니 맵핑되는 임베딩 벡터가 없을 것이고, 임베딩 벡터가 없다면 node classification 등의 downstream task에서 예측이 불가능하겠죠? 이런 특성 때문에 시간에 따라 노드가 추가될 수 있는 evolving 그래프와 같은 경우 그래프가 변할 때마다 전체 임베딩을 다시 scratch부터 학습해야 한다는 불편함이 있습니다. . | 노드 feature을 활용하지 않음 대부분의 그래프 데이터셋은 우리가 활용할 수 있는 노드 feature이 존재합니다. 예를 들어, 소셜 그래프의 경우, 단순히 철수가 영희가 친구라는 정보 이외에도, 철수는 성균관대학교에 다니는 23세 남학생이라는 정보도 존재합니다. 단순한 노드 간 연결 상태 이외에도 이러한 노드 feature를 고려하여 노드를 임베딩 한다면 정보량이 더 풍부해져 효과적일 것입니다. # Deep Graph Encoders . | . 이제 지금껏 다뤄왔던 간단한 look-up table로 이루어진 encoder에서 벗어나, 좀 더 복잡한 형태의 Deep Encoder을 공부해봅시다. . . Deep Encoder은 인풋 그래프에 수차례의 비선형적인 transformation을 가하여 end-to-end으로 최종 임베딩을 얻는 방식을 말합니다. 수업 슬라이드에 쓰인 말 그대로, . Deep Encoder = multiple layers of non-linear transformations based on graph structure . 로 생각할 수 있겠습니다. 잘 와닿지 않으신다고요? 사실, 인풋 데이터가 우리가 익숙치 않은 형태의 그래프라 그렇지, 오늘날의 머신러닝/딥러닝 모델이 이미지나 텍스트와 같은 정형 데이터를 처리하는 방식과 유사합니다. . . 위의 두 그림이 유사하다는 점이 한눈에 보이실 겁니다. 이해를 돕기 위해 가장 기본적인 CNN 구조를 생각해 볼까요? 원본 이미지가 여러 convolution layer을 거치며 더욱 더 축약된 feature map을 만드는 방식과 유사하게, 인풋으로 들어온 그래프는 여러 graph convolution layer을 거치며 원본 그래프의 의미를 적절히 축약하는 노드 임베딩을 만드는 것입니다. . 또한, 개 고양이 이미지 분류 모델이 지도 학습으로 학습될 때 학습 데이터에 대해 각 이미지가 개인지, 고양이인지 나타내는 클래스 label을 활용하는 것과 같이, 노드 분류 문제의 경우 각 노드에 대한 클래스 label이 있다면 이를 직접 활용하여 encoder를 학습할 수 있습니다. . 💡 이 경우 decoder은 노드 클래스 label 입니다. 물론, ground truth label이 존재하지 않는 비지도 학습 상황에서는 기존 Shallow Encoder을 학습하던 방법과 동일하게 Random Walk 등으로 정의되는 인풋 그래프상 노드 similarity를 유지하도록 학습할 수도 있겠죠. 이 부분은 본 강의 말에 다시 다루니까 이해되지 않는대도 걱정 마세요! :) . 💡 이 경우 decoder은 임베딩 벡터 간 similarity metric인 벡터 내적 등으로 정의할 수 있습니다. (lecture 4) 이렇게 Deep Encoder을 통해 얻은 노드/그래프 임베딩은 여러 task에서 agnostic하게 활용할 수 있습니다. . 학습된 임베딩을 활용할 수 있는 여러 task의 예 Node classification | Link prediction | Community detection | Network similarity # Why is it Hard? . | . | . 아까 언급했듯이 Deep Encoder을 통해 그래프를 임베딩 한다는 개념은 지금껏 우리가 정형 데이터를 처리했던 방식과 비슷하기 때문에 낯설지 않습니다. . 그렇다면 그냥 널리 사용되고 있는 CNN이나 RNN을 활용해서 그래프를 임베딩 하면 되지 않을까요? . . 그럴 수 없습니다. . 정형 데이터인 이미지, 텍스트에 비해 비정형 데이터인 그래프는 너무나도 복잡하기 때문이죠. 그래프는 이미지와 같이 (0,0) 등의 기준점을 둘 수 없으며, 텍스트와 같이 명백한 순서가 있지도 않습니다. 그래프는 제각기 다른 사이즈 일 수 있으며 각각의 topological structure 또한 모두 다릅니다. 심지어는 노드 마다 multimodal feature을 가질 수도 있습니다. . 따라서, 비정형 그래프 구조에서 각 노드의 구조적 특징 및 노드 feature을 고려하여 적절하게 임베딩 하는 새로운 방법이 필요합니다. . 💡 노드의 Multimodal feature 다시 소셜 그래프를 떠올려 봅시다. 각 노드는 철수, 영희를 포함한 개인을 나타내고, 엣지는 각 개인 사이에 친구 관계가 성립하는지를 나타냅니다. 이 때, 철수라는 노드는 프로필 사진(이미지), 자기 소개 글(텍스트) 등 여러 부가적인 feature을 가질 수 있습니다. .",
            "url": "https://cs224w-kor.github.io/blog/2022/07/27/lecture-0601.html",
            "relUrl": "/2022/07/27/lecture-0601.html",
            "date": " • Jul 27, 2022"
        }
        
    
  
    
        ,"post3": {
            "title": "Lecture 4.4 - Matrix Factorization and Node Embeddings",
            "content": "Recall: Node Embeddings &amp; Embedding matrix . 이전 강의에서 배웠던 embedding matrix $ mathbf{Z}$에 대해 다시 떠올려봅시다. 이 매트릭스는 그래프의 각 노드들을 잠재변수 공간(embedding space)으로 encoding하는 행렬로 열의 차원은 embedding하는 크기, 행의 차원은 그래프에 있는 노드의 수가 됩니다. 이 매트릭스의 한 열은 특정 노드 $u$의 embedding vector $ mathbf{z}_u$를 나타내게 됩니다. . . . 이러한 Node embedding에서 objective는 그래프상에서 실제로 유사한 노드들의 simliarity가 embedding vector들의 내적(inner product)값도 높도록 만드는 것입니다. . 📌 Objective: Maximize $ mathbf{z}_{v}^{ mathrm{T}} mathbf{z}_{u}$ for node pairs $(u, v)$ that are similar Matrix Factorization . Embedding matrix를 Matriz Factorization 관점에서 다시 생각해봅시다. 그래프를 노드들간의 연결이 되어 있으면 1, 아니면 0으로 나타낸 인접행렬 $ mathbf{A}$을 embedding matrix $ mathbf{Z}$로 factorization 한다고 생각해볼 수 있습니다. 즉 $ mathbf{Z}^{ mathrm{T}}$와 $ mathbf{Z}$의 내적으로 인접행렬 $ mathbf{A}$를 만드는 것입니다. . ZTZ=A mathbf{Z}^{ mathrm{T}} mathbf{Z} = mathbf{A}ZTZ=A . . 하지만 embedding matrix $ mathbf{Z}$의 행의 수, 즉 embedding dimension $d$는 노드의 수 $n$보다 작으므로 완벽한 factorization을 할 수 없고 대신에 이를 최적화 기법을 사용하여 근접(approzimate)시킬 수 있습니다. 이 최적화를 목적함수는 다음과 같습니다. . min⁡Z∥A−ZTZ∥2 min _{ mathbf{Z}} left |A- boldsymbol{Z}^{T} boldsymbol{Z} right |_{2}Zmin​∥ . ∥​A−ZTZ∥ . ∥​2​ . 결론은 edge connectivity로 정의된 node similarity을 나타내는 decoder($ mathbf{Z}$)의 내적은 $ mathbf{A}$의 matrix factorization과 동일하다는 것입니다. . RandomWalk-based Similarity . DeepWalk와 node2vec 알고리즘에서는 random walks를 기반으로한 좀 더 복잡한 node similarity를 사용합니다. 2개의 알고리즘 모두에서 matrix factorization을 사용하고 있습니다. DeepWalk에서 사용하는 node simliarity는 다음과 같이 정의됩니다. (node2vec은 이보다 조금 더 복잡합니다. 자세한 내용을 확인하고 싶으면 Network Embedding as Matrix Factorization paper를 참고) . . Limitations . Matrix factorization과 random walk로 node embedding을 할 경우 몇가지 제약(단점)이 있습니다. . 그래프에 새로운 노드가 생겼을 때 대응하지 못합니다. training과정에서 보지 못한 노드가 생겼을 때 scratch부터 다시 계산해야 합니다. | . 구조적인 유사성을 파악하지 못합니다. 아래의 그림에서 1-2-3과 11-12-13은 그래프에서 비슷한 구조를 가지고 있지만 각 노드마다 unique한 embedding 값으로 인해 구조적인 유사성을 파악하지 못합니다. | . 노드, 엣지, 그래프의 feature 정보를 활용할 수 없습니다. DeepWalk나 node2vec에 쓰인 embedding은 노드에 있는 feature 정보를 활용할 수 없습니다. 이는 추후에 배울 Deep Representation Learning으로 해결할 수 있습니다. | . Algorithms 정리 . PageRank: 그래프에서 노드의 importance를 측정하는 알고리즘이며 인접행렬의 power iteration으로 계산할 수 있다. 총 3가지 관점에서 해석할 수 있다. | (1) Flow formulation | (2) Random walk &amp; Stationary distribution | (3) Linear Algebra - eigenvector | . | Personalized PageRank(PPR): PageRank에서 좀 더 발전시킨 알고리즘으로 random walk로 구한 특정 노드의 중요성을 더 고려하여 teleport를 하는 알고리즘 | Random walks 기반 Node Embeddings은 Matrix factorization으로 표현될 수 있다. | . 그래프를 행렬로 이해하는 것은 위의 알고리즘들을 이해하는데 매우 중요하다는 것을 알 수 있습니다. . . Original Lecture Video : CS224W: Machine Learning with Graphs 2021 Lecture 4.4 - Matrix Factorization and Node Embeddings .",
            "url": "https://cs224w-kor.github.io/blog/matrix/node%20embedding/factorization/random%20walks/2022/07/13/lecture-0404.html",
            "relUrl": "/matrix/node%20embedding/factorization/random%20walks/2022/07/13/lecture-0404.html",
            "date": " • Jul 13, 2022"
        }
        
    
  
    
        ,"post4": {
            "title": "Lecture 4.3 - Random Walk with Restarts",
            "content": "Recommendation . 추천 시스템에서 이분그래프(Bipartite graph)로 user와 item의 (구매)관계를 나타낸 Bipartite User-Item Graph는 다음 그림과 같습니다. 여기에서 특정 item Q를 구매한 user에게 어떤 item을 추천해주는 것이 좋을지를 고민한다면, 직관적으로 item Q가 item P와 비슷하게 user들과 관계를 가지고 있을 때 item P를 이 유저에게 추천하는 것이 좋을 것이라고 생각할 수 있습니다. 즉, item Q와 item P가 얼마나 가까운 관계인지 판단하는 것이 중요합니다. . . Node proximity Measurements . 노드 근접성(proximity) 측정에 대해 생각해보기 위해 아래의 3가지 케이스를 보겠습니다. A-A’은 B-B’보다 더 가까운 관계를 가지고 있다고 할 수 있습니다. 왜냐하면 A-A’ path에서 user을 한번만 거치는데 반해, B-B’path에서는 B-user-item-user-B’ 로 path의 길이가 더 길기 때문입니다. A-A’와 C-C’를 비교해보면 C-C’이 더 가까운 관계를 가지고 있다고 판단할 수 있는데 그 이유는 C-C’이 A-A’보다 더 많은 공통의 이웃(Common Neighbors)를 가지고 있기 때문입니다. C-C’은 A-A’의 shortest path가 2개있는 것으로도 볼 수 있습니다. . . Proximity on Graphs . 이전에 PageRank를 다시 떠올려보면, (1) rank는 node의 “importance”를 정의하며 (2) 그래프의 모든 node들에 균일 분포로 teleport 이동을 할 수 있는 알고리즘이었습니다. . 여기에 좀 더 아이디어를 덧붙여서 Personalized PageRank 알고리즘을 생각해 볼 수 있습니다. 그래프의 모든 노드들에 대해 균일 분포로 teleport 이동을 하는 것이 아닌, 그래프 노드들의 부분집합(subset) $ mathbf{S}$의 노드들로만 teleport 이동을 하도록 할 수 있습니다. 모든 노드들로 랜덤하게 teleport하지 않고 좀 더 연관성이 높은 노드들로 teleport할 수 있도록 하는 것입니다. item Q와 item P가 더 연관성이 높다는 것(Node proxmity ↑)을 어떻게 알 수 있을까요? 이는 Random Walks로 확인해볼 수 있습니다. . Random Walks . item Q가 우리가 알고싶은 item 노드들의 집합인 QUERY_NODES집합에 속해있다고 해봅시다. Bipartite User-Item Graph 상에서 QUERY_NODES 집합에 속해 있는 어떤 노드(item Q)에서 시작하여 랜덤하게 움직이면서 과정을 기록합니다. 이 과정을 기록한다는 것은 item↔user 사이를 계속 랜덤하게 움직이면서 방문(visit)하게 된 item 노드에는 +1 count를 하는 것을 의미합니다. 이렇게 랜덤하게 움직이면서 이동을 결정할 때마다 일정 확률 ALPHA 만큼 재시작을 하게되는데, 재시작시에는 QUERY_NODES집합에 속해 있는 하나의 노드로 이동해서 다시 랜덤하게 움직이기 시작합니다. (아래 pseudo code 참고) . . 이렇게 계속 Random Walks를 하다보면 item 노드의 visit 수가 높을수록 query item Q와 높은 관계성을 가진것으로 판단할 수 있습니다. . Benefits . 이와 같은 Random Walks를 통한 시뮬레이션과 visit 수로 노드들간의 근접성(proximity)을 판단하는데 좋은 이유는 다음과 같은 사항들을 고려하여 similarity를 나타낼 수 있는 방법이기 때문입니다. . Multiple connnections | Multiple paths | Direct and Indirect connections | Degree of the node | . PageRank Varients 정리 . PageRank와 이를 변형한 총 3가지 알고리즘들을 정리하면 다음과 같습니다. . PageRank Personalized PR Random Walk w/ Restarts . 모든 노드들에 같은 확률로 teleport 이동 | 특정 노드들로 특정 확률을 가지고 teleport 이동 | 항상 똑같은 1개의 노드로 이동 | . . . Original Lecture Video : CS224W: Machine Learning with Graphs 2021 Lecture 4.3 - Random Walk with Restarts .",
            "url": "https://cs224w-kor.github.io/blog/matrix/pagerank/recommendation/proximity/random%20walks/ppr/restarts/2022/07/13/lecture-0403.html",
            "relUrl": "/matrix/pagerank/recommendation/proximity/random%20walks/ppr/restarts/2022/07/13/lecture-0403.html",
            "date": " • Jul 13, 2022"
        }
        
    
  
    
        ,"post5": {
            "title": "Lecture 4.2 - PageRank, How to Solve?",
            "content": "이전의 강의에서 Powe iteration 방법으로 반복적인 매트릭스 곱 연산으로 $ mathbf{r}$을 구할 수 있음을 확인했습니다. 이 방법에 대해 조금 더 구체적으로 살펴보겠습니다. . Power Iteration Method . power iteration은 2가지 표현식이 있는데 하나는 벡터의 요소 관점에서의 업데이트 식(왼쪽)과 다른 하나는 매트릭스 관점의 업데이트 식(오른쪽)으로 나타낼 수 있습니다. . . 과정을 살펴보면 다음과 같습니다. . 처음 초기화로 모든 노드의 importance score를 똑같은 값으로 만들어 줍니다.(반복적인 연산으로 수렴을 보장하므로 사실 어떤 값으로 초기화하든 상관없습니다.) $ boldsymbol{r}^{(0)}=[1 / N, ldots ., 1 / N]^{T}$ | 반복적인 연산을 하면서 $ mathbf{r}$ 값을 업데이트합니다. $ boldsymbol{r}^{(t+1)}= boldsymbol{M} cdot boldsymbol{r}^{(t)}$ | 수렴조건 $ left| boldsymbol{r}^{( boldsymbol{t}+ mathbf{1})}- boldsymbol{r}^{(t)} right|_{1}&lt; varepsilon$ 을 만족할 때까지 2번 과정의 연산을 진행합니다. | 예시 그래프에서의 power iteration 과정은 다음과 같습니다. . . . Three Questions . Does this converge? 반복적인 연산과정을 통해 값이 수렴하는가? | Does it converge to what we want? 수렴한 값이 우리가 원하는 값인가? | Are results reasonable? 연산 결과가 합당한가?(말이 되는가?) | (어색한 한국어 번역보다 영어로된 질문에서 얻어가는 insight가 좋을 것 같습니다.) . Problems . PageRank에는 2가지의 문제가 있습니다. . Dead Ends | out-link를 가지지 않는 일부 페이지(노드)들에서 생기는 문제로 이런 페이지들에서 importance가 leak out 됩니다. leak out의 세어나가다 라는 뜻 그대로 importance flow의 흐름에서 값이 세어나가는 문제를 말합니다. . 아래의 예시에서 페이지 b에서 나가는 out-link가 없다보니 importance update를 한 결과가 $r_a = 0, r_b=0$이 됨을 확인할 수 있습니다. 이는 앞서 page rank $ mathbf{r}$ vector의 정의에서 약속한 모든 노드의 importance의 합이 1이 된다는 column stochastic 수학적 전제에서 벗어난 결과 입니다. . . Spider traps | 특정 페이지의 모든 out-link들이 다른페이지로 나가지 않아 결국 spider trap 페이지가 모든 importance 값을 독차지하게 됩니다. . 아래의 예시에서 a에서 walk를 시작하더라고 b로 이동한 후 b에서 빠져나올 수 없습니다. 이런 경우 importance update 결과 모든 importance를 페이지 b가 가지게 되어 $r_a = 0, r_b=1$이 됩니다. 이런 경우 페이지 a에 아무리 큰 웹 그래프가 연결되어 있다고 하더라도 이동할 수 없습니다. 사실 spider trap은 column stochastic을 만족하기 때문에 수학적으로 문제되진 않습니다. 하지만 우리가 원하지 않는 값에 수렴하는 문제로 볼 수 있습니다. . . Solutions . 위의 2가지 문제들 모두 Teleports로 해결할 수 있습니다. . . Dead Ends를 Teleports로 해결하기 | Dead Ends인 m 페이지에서 column stochastic을 만족하지 않고 모든 값이 0이 되지 않도록 자신을 포함한 그래프의 모든 노드들로 uniform random 하게 teleport 이동을 하도록 합니다. 이때 그래프의 노드가 총 3개이므로 m열의 행렬값을 $1/3$으로 채워 $ mathbf{M}$을 완성합니다. . . Spider Traps를 Teleports로 해결하기 | Spier Trap인 m 페이지에서 다른 노드로 빠져나갈 수 있도록 일정 확률 $1- beta$만큼 random 페이지로 점핑(teleport)할 수 있도록 합니다. 즉, 확률 $ beta$만큼은 원래 그래프의 out-links 중에 골라서(random) 이동하고 나머지 확률($1- beta$)로는 out-link와 상관없이 그래프의 모든 페이지들 중에 골라서 이동하여 거미줄, Spider trap에서 벗어나게 되는 것 입니다. 보통 $ beta$값으로는 0.8~0.9값을 사용하는 것이 일반적입니다. . . The Google Matrix . PageRank에서 생길 수 있는 2가지 문제를 Teleport로 해결한다면 PageRank Equation은 다음과 같이 바꿀 수 있습니다. 첫번째 항은 기존의 수식에 있던 부분으로 페이지 $i$의 out-link를 random하게 골라서 이동하는 것에다 확률 $ beta$값을 곱해 보통 0.8~0.9의 확률로 out-link를 통해 이동하게 합니다. . 두번째 항은 Teleport를로 out-link와 상관없이 그래프의 모든 페이지들중 하나로 랜덤하게 순간이동하는 것을 수식적으로 표현한 부분입니다. 그래프에 존재하는 모든 페이지의 수를 $N$이라고 할 때, 추가적으로 $1/N$의 확률로 페이지 $j$로 갈 수 있고 이는 앞서 확률 $ beta$를 제외한 나머지 확률, 약 0.2~0.1의 확률로 이동하는 것이므로 $1- beta$를 곱해줍니다. . rj=∑i→jβridi+(1−β)1Nr_{j}= sum_{i rightarrow j} beta frac{r_{i}}{d_{i}}+(1- beta) frac{1}{N}rj​=i→j∑​βdi​ri​​+(1−β)N1​ . (단, 위의 수식은 $ mathbf{M}$에 dead ends가 없다고 가정하며, 실제로 모든 dead ends를 없애거나 dead ends인 부분들에는 random teleport를 확률1로 따르게 하여 계속 다른 노드로 이동하게 할 수 있습니다.) . 구글 매트릭스는 이와 크게 다르지 않습니다. 단지 위의 PageRank equation을 행렬식으로 바꿔쓰면 구글 매트릭스가 됩니다. 각각의 항들이 의미하는 바는 위에서 설명된 것과 동일하며, 두번째 항의 $ left[ frac{1}{N} right]_{N times N}$는 행렬의 모든 원소가 $ frac{1}{N}$으로 채워진 $N times N$차원의 행렬을 말합니다. . G=βM+(1−β)[1N]N×NG= beta M+(1- beta) left[ frac{1}{N} right]_{N times N}G=βM+(1−β)[N1​]N×N​ . Random Teleports ($ beta=0.8$) . 아래의 $ beta=0.8$일 때 Random Teleports 예시에서 검은색 선들은 teleports를 적용하지 않았을 때의 그래프의 directed links를 표현하며 초록색 선들은 0.2확률의 teleports가 추가된 부분을 나타냅니다. Power iteration을 통해 계산되면 페이지 $y, a, m$이 각각 $7/33, 5/33, 21/33$으로 수렴하는 것을 알 수 있고 spider trap인 페이지 m이 모든 importance를 흡수하지 않는 것을 확인할 수 있습니다. . . Solving PageRank 정리 . PageRank $ mathbf{r} = mathbf{G} mathbf{r}$을 power iteration method로 풀 수 있다. | PageRank에서 생길 수 있는 문제들인 Dead Ends와 Spider Traps를 Random Uniform Teleportation으로 해결할 수 있다. | . . Original Lecture Video : CS224W: Machine Learning with Graphs 2021 Lecture 4.2 - PageRank: How to Solve? .",
            "url": "https://cs224w-kor.github.io/blog/matrix/pagerank/spider%20trap/dead%20ends/teleports/2022/07/13/lecture-0402.html",
            "relUrl": "/matrix/pagerank/spider%20trap/dead%20ends/teleports/2022/07/13/lecture-0402.html",
            "date": " • Jul 13, 2022"
        }
        
    
  
    
        ,"post6": {
            "title": "Lecture 4.1 - PageRank",
            "content": "Lecture 4. Graph as Matrix . 4강에서는 Graph를 매트릭스(선형대수) 관점으로 바라보는 것에 대해 이야기 합니다. . . 다음 3가지 키워드, Random walk(Node Importance), Matrix Factorization, Node embedding를 중심으로 공부합니다. 강의는 총 4파트로 나누어져 진행됩니다. . Lecture 4.1 - PageRank | Lecture 4.2 - PageRank, How to Solve? | Lecture 4.3 - Random Walk with Restarts | Lecture 4.4 - Matrix Factorization and Node Embeddings | . . The Web as a Directed Graph . . 웹을 거시적인 관점으로 보게되면, 하나의 웹 페이지 → Node로 하이퍼링크 → Edge로 생각하여 하나의 거대한 Graph로 볼 수 있습니다. . Side issue . 다이나믹하게 새로 페이지들이 생길 수 있습니다. | 다크웹과 같은 접근할 수 없는 페이지들도 있을 수 있습니다. | . 잠시 Side issue는 내려놓고, 새로 페이지들이 생기지도 않고 기존의 페이지들이 사라지지도 않는 Static pages 상황을 가정해봅시다. 아래의 그림에서처럼 페이지들은 하이퍼링크들로 서로 연결되어 있고, 유저는 페이지들에 달려있는 하이퍼 링크들로 이루어진 연결망을 기반으로 항해하듯이 Navigational 하게 page to page 이동을 하게 됩니다. (오늘날에는 post, comment, like 등의 기반의 transactional한 웹에서의 상호작용이 일어나지만 이는 우선 논외로 하겠습니다.) . . 위의 그림처럼 웹 그래프는 방향성이 있는 유향 그래프(Directed graph)임을 알 수 있습니다. 위키피디아와 같은 웹 사전 페이지들 간의 관계성이나 논문의 인용 관계 그래프 등에서 예시를 쉽게 찾아볼 수 있습니다. . . Ranking Nodes on the Graph . 웹을 하나의 거대한 유향 그래프로 생각할 때 한가지 중요한 insight가 있습니다. . 💡 모든 웹 페이지들이 똑같이 중요하지는 않다 . 바로 각 페이지의 중요성이 똑같지 않다는 이야기는 그래프에서 각 노드의 중요성(importance)가 다르다는 말로 바꿔 생각할 수 있습니다. 아래 사진을 보면 직관적으로 파란색 노드가 빨간색 노드보다 더 중요할 것 같다라고 생각할 수 있습니다. 왜 그렇게 보일까요? 아직 노드의 중요성에 대해 정의하지 않았지만 그래프에서 각 노드를 중심으로 뻗어있는 edge(link)의 수가 한눈에 비교되기 때문에 직관적으로 파악할 수 있는 것입니다. 이처럼 웹 그래프의 link structure를 가지고 우리는 각 페이지들(node)의 ranking을 매길 수 있습니다. . . Link Analysis Algorithms . 각 페이지들의 중요성(importance)를 파악하기 위해 Link Analysis가 필요합니다. . 본 수업에서 다룰 Link Analysis 알고리즘들은 아래 총 3개에 대해서 다룰 예정입니다. . PageRank | Personalized PageRank (PPR) | Random Walk with Restarts | . Links as Votes . 링크가 투표용지라고 생각해봅시다. 여기서 유향 그래프인 웹 그래프에서 링크는 2가지 종류가 있다는 것을 다시한번 생각해봐야 합니다. . in-comming links(in-links): 기준 페이지로 들어오는 방향의 링크 | out-going links(out-links): 기준 페이지에서 나가는 방향의 링크 | . 이렇게 방향까지 고려하여 링크를 투표라고 생각할 때, 엄밀히 말하자면 in-link를 투표라고 생각해야 할 것 입니다. 한 가지 더 생각해볼 문제는 모든 in-link들을 동등하게 생각할 수 있는가?라는 문제입니다. 어떤 링크들은 다른 링크들에 비해 좀 더 중요한 페이지로부터(from) 기준페이지로(to) 온 링크일 수도 있기 때문에 count에 차등을 둬야 하지 않을까라고 생각할 수도 있습니다. 이런 고민들은 결국 페이지들이 서로 연결되어 있어서 recursive한 문제로 볼 수 있습니다. . ➕ recursive한 문제란, 물리고 물리는 문제로 생각할 수 있습니다. A→B 링크에서 A가 중요한 페이지라는 사실을 기반으로 B가 중요해지고, 이어지는 B→C 링크에서 이 영향을 이어받아 C까지 중요한 페이지라고 판단하게 되기 때문입니다. PageRank . The “Flow” Model . 위에서 설명한 recursive한 특성을 기반으로 중요성이 흘러가는(flow) 모델을 생각해볼 수 있습니다. 중요성을 $r$이라는 변수로 두고 기준 노드 j의 importance가 어떻게 flow되는지 살펴보겠습니다. . j로 in-link되어있는 i, k 의 importance $r_i$, $r_k$를 각 노드의 out-link의 수만큼 나누어서 j로 전달됩니다. i 노드의 out-link는 총 3개 이므로 $ frac{r_i}{3}$, k노드의 out-link는 총 4개 이므로 $ frac{r_k}{4}$로 계산되어 두 값의 합이 $r_j$가 됩니다. | $r_j$는 j노드의 out-link를 통해 flow하게 되는데 out-link의 수, 즉 3으로 나누어져 $ frac{r_j}{3}$ 값이 각각의 다음 노드들로 $r_j$값이 전달되게 됩니다. | . 이처럼 importance가 높은 페이지로부터 in-link된 페이지는 영향을 받아 importance가 높아짐을 알 수 있습니다. 노드 $j$의 rank, $r_j$를 정의하면 다음과 같이 수식으로 나타낼 수 있습니다. (이때 $d_i$는 노드 i의 out-degree를 말합니다.) . rj=∑i→jridir_{j}= sum_{i rightarrow j} frac{r_{i}}{d_{i}}rj​=i→j∑​di​ri​​ . 다음과 같은 예시에서 각 기준 노드를 가지고 in-link들을 고려하여 “Flow equation”을 계산해보면 다음과 같다. . . 노드 y 노드 a 노드 m . y에서 오는 링크 + a에서 오는 링크 | y에서 오는 링크 + m에서 오는 링크 | a에서 오는 링크 | . $r_y = frac{r_y}{2} + frac{r_a}{2}$ | $r_a = frac{r_y}{2} + r_m$ | $r_m = frac{r_a}{2}$ | . ➕ 3 Unknowns, 3 Equations 이기 때문에 4번째 constraint로 $r_y + r_a + r_m =1$로 scale관련 constraint를 추가하여 Gaussian elimination을 사용하여 선형방정식으로 풀려고 하는 생각은 좋지 않다. 왜냐하면 importance는 이런식으로 scalable하지 않기 때문이다. (It’s not scalable) 좀 더 정교한 설계가 필요하다. Matrix Formulation . Stochastic Adjacency Matrix $ mathbf{M}$ . $ mathbf{M}$은 $(node의 수) times (node의 수)$차원의 매트릭스 입니다. | $i$→$j$ 링크에서 매트릭스 요소 $M_{ji}$는 $ frac{1}{d_i}$가 됩니다. ($d_i$를 노드 $i$의 out-degree라고 정의합니다.) . Mji=1diM_{ji} = frac{1}{d_i}Mji​=di​1​ 오른쪽 예시에서처럼 노드 $i$를 기준으로 총 3개의 out-link들이 있다면 각각의 값은 $1/3$이 됩니다. . | column 기준 stochastic : 열 방향의 모든 값들을 더하면 1이 되는 확률값이 됩니다. | . . Rank Vector $r$ . $ mathbf{r}$은 각 페이지의 entry 값을 가지는 $(node의 수) times 1$ 차원의 벡터입니다. | 각 페이지의 importance score를 $r_i$로 정의합니다. | 모든 노드의 importance score의 합은 1입니다. 따라서 이 또한 확률값으로 생각할 수 있습니다. | . ∑iri=1 sum_ir_i = 1i∑​ri​=1 . Flow Equations . 이전에 정의했던 노드의 rank 수식을 새롭게 정의한 매트릭스 $M$과 벡터 $r$로 다시 써보면 Flow Equation을 완성할 수 있습니다. . r=M⋅r mathbf{r}= mathbf{M} cdot mathbf{r}r=M⋅r . 앞서 살펴본 간단한 그래프 예시를 가져와서 flow equation을 매트릭스 연산으로 표현해보면 아래와 같습니다. (flow equation은 앞내용을 참고) . . . Connection to Random Walk . 다음과 같은 조건을 만족하며 랜덤하게 웹페이지들을 돌아다니고 있는 유저를 생각해보겠습니다. . 시점 $t$에 페이지 $i$에 있습니다. | 다음 시점 $t+1$에 페이지 $i$로부터 나가는 방향의 out-link들 중에 uniform하게 선택하여 서핑을 합니다. | 앞서 선택된 out-link를 통해 $i$와 연결된 $j$ 페이지에 도달합니다. | 이 과정(1~3)을 무한으로 반복합니다. | . 여기에서 우리는 시점 의 개념을 고려하여 새로운 개념 정의를 하나 할 수 있습니다. . p(t) mathbf{p(t)}p(t) . $ mathbf{p(t)}$는 확률 벡터(probability distribution)로, 이 벡터의 $i$번째 요소는 앞서 가정한 유저가 시점 $t$에 페이지 $i$에 있을 확률을 나타냅니다. . The Stationary Distribution . 앞서 정의한 $ mathbf{p(t)}$를 가지고 이 유저가 시점 $t+1$에 있을 확률분포는 다음과 같이 계산합니다. . p(t+1)=M⋅p(t) mathbf{p(t+1)}= mathbf{M} cdot mathbf{p(t)}p(t+1)=M⋅p(t) . 💡 만약에 유저가 웹 서핑을 계속하다가 $ mathbf{p(t+1)} = mathbf{p(t)}$ 같은 상황이 되면 어떨까요? . p(t+1)=M⋅p(t)=p(t) mathbf{p(t+1)}= mathbf{M} cdot mathbf{p(t)} = mathbf{p(t)}p(t+1)=M⋅p(t)=p(t) . 이러한 상황에서는 더 이상 유저가 특정 페이지에 있을 확률이 변하지 않고 유지되는 경우가 되며, 이를 stationary distribution of a random walk 라고 합니다. . 이러한 형태는 낮설지가 않은데, 앞서 rank vector $ mathbf{r}$가 매트릭스 $ mathbf{M}$과 flow equation을 구성할 때 이러한 꼴이었으며, 따라서 $ mathbf{r}$은 stationary distribution of a random walk 입니다. . Eigenvector Formulation . 이전 Lecture 2에서 잠시 배웠던 eigenvector와 eignvalue를 생각해보면 다음 수식을 떠올려볼 수 있습니다. . λc=Ac lambda mathbf{c} = mathbf{A} mathbf{c}λc=Ac . 여기에서 flow equation을 다시 위와 같은 꼴로 작성해보면, 아래와 같이 eigenvalue가 1이고 eigenvector가 $ mathbf{r}$인 수식으로 해석될 수 있습니다. . 1⋅r=M⋅r1 cdot mathbf{r}= mathbf{M} cdot mathbf{r}1⋅r=M⋅r . 따라서 $ mathbf{r}$은 매트릭스 $ mathbf{M}$의 principle eigenvector(eigenvalue 1)이며, 임의의 벡터 $ mathbf{u}$에서 시작해서 계속 매트릭스 $ mathbf{M}$을 곱하여 극한 $ mathbf{M}( mathbf{M}(…( mathbf{M}( mathbf{M} mathbf{u}))))$으로 도달하게되는 long-term distribution이 됩니다. 이러한 방식으로 $ mathbf{r}$을 구하는 방법을 Power iteration 이라고 합니다. . PageRank 정리 . 웹 구조에서 볼 수 있는 link들을 기반으로 node들의 importance를 측정할 수 있다. | 랜덤하게 웹 서핑하는 유저 모델은 stochastic advacency matrix $ mathbf{M}$으로 나타낼 수 있다. | PageRank 수식은 $ mathbf{r} = mathbf{M} mathbf{r}$ 이며, $ mathbf{r}$은 (1) 매트릭스 $ mathbf{M}$의 principle eigenvector, (2) stationary distribution of a random walk 2가지로 해석될 수 있다. | . . Original Lecture Video : CS224W: Machine Learning with Graphs 2021 Lecture 4.1 - PageRank .",
            "url": "https://cs224w-kor.github.io/blog/web/matrix/pagerank/rank/flow%20model/flow%20equations/eigenvector/stationary%20distribution/power%20iteration/2022/07/13/lecture-0401.html",
            "relUrl": "/web/matrix/pagerank/rank/flow%20model/flow%20equations/eigenvector/stationary%20distribution/power%20iteration/2022/07/13/lecture-0401.html",
            "date": " • Jul 13, 2022"
        }
        
    
  
    
        ,"post7": {
            "title": "Lecture N.1 - 강의 소제목(분할되어 있는 동영상 제목)",
            "content": "위에 title에 특수문자 :나 []를 사용하지 마세요! . .md 파일의 제목은 날짜-lecture-0N0n.md로 작성해주세요.(2022-07-13-lecture-0401) . Lecture N. 강의 제목 . N 강의의 첫번째 포스트에 강의에 대한 전반적인 소개를 작성합니다. &gt; 표시를 앞에 작성합니다. . Imgur로 이미지도 넣을 수 있습니다.(아래에 Imgur에 대해 설명있습니다.) 설명설명 . 강의가 세분화되어 있어서 나누어진 포스팅들을 모아서 한번에 보여주기 위해 링크를 답니다. . Lecture N.1 - 소제목1 | Lecture N.2 - 소제목2 | Lecture N.3 - 소제목3 | Lecture N.4 - 소제목4 | . . 제목 . 제목같은 경우는 자유롭게 작성하셔도 되나 저는 강의 자료 ppt를 기반으로 해서 섹션 제목(약 80% 정도)을 가져왔습니다. . 소제목 . #, ## 등 제목을 세분화하면 좋겠지만 너무 많이 제목을 쓰면 위에 TOC가 길어져서 보기 좋지 않을 것 같아 제목은 #만 쓰고 하위 제목으로는 **소제목**을 사용해서 작성했습니다. . 이미지 . 이미지는 깃헙에 직접업로드한 후 markdown으로 작성할 수 있으나 그러면 추후에 용량문제가 생긴다고 해서 Imgur를 이용해서 포스팅합니다. . 계정을 만들어야 합니다. | 계정 아이콘의 아래에 있는 Images를 누룹니다. . . | 업로드 하고자 하는 이미지를 드래그 앤 드랍을 합니다. | 업로드 된 이미지 위로 커서를 가져가면 편집 아이콘이 뜨고 이걸 눌러서 이미지 크기를 조절합니다. (이미지 크기를 조절할때마다 이미지 주소링크가 달라지기 때문에 글에 markdown 링크를 가져오기 전에 조정하는 것이 좋습니다. 여러개 이미지들을 눌러서 일일이 누르지 않고 연속적으로 편집할 수 있습니다.) . . 저는 개인적으로 가로 800픽셀이 넘어가지 않는 선에서 width만 조절하면 나머지 height는 비율에 맞게 저장 됩니다. 편집후 위에 있는 save를 눌러 저장해주세요. . | 크기 편집을 다 한 후 이미지를 클릭하면 markdown link를 복사할 수 있고 이를 md 파일에 붙여넣으면 됩니다. | 이때 `!`없이 링크가 복사되는데 md 파일에 가져올때 앞에 `!`를 붙여주세요. (이 부분은 노션에서 `callout`으로 작성하면 생기는 부분입니다. **raw text**만 인식되며 굵은 글씨, 코드 포맷 등 다 지원되지 않습니다. 수식은 지원됩니다. $a$) 표 . title 1 2 3 . 일반적인 | 작성 양식을 | 따릅니다. | :) | . 토글 . html을 이용하여 토글을 할 수 있습니다. 하지만 이는 노션의 callout과 마찬가지로 안에 내용은 raw text만 지원됩니다. . 토글된 제목 숨겨진 내용은 raw text만 됩니다. 더 다양한 작성 형식 . 은 2020-01-14-test-markdown-post.md 공식 예시글을 참고해주세요. 여기에 없는 다른 기능들은 보통 html 형식을 맞춰주면 적용이 되는 것 같습니다. . 작성 흐름 . 작성하는 흐름은 팁공유하는 차원으로 제가 하고 있는 방식을 말씀드리겠습니다. . 노션에 강의 내용 정리 | 노션에서 export를 markdown으로 해서 zip 파일로 받은 후 압축해제하면 안에 md 파일과 이미지들이 모아져 있는 폴더가 생깁니다. | blog repository에 다운받은 md 파일을 복붙해서 조금의 편집을 합니다.(위에 작성해야 하는 항목(title, description, tag 등), 수식, 인터넷 링크확인) | Imgur에 접속해서 2번에서 받은 이미지들을 한번에 업로드하고 편집한 후 markdown link들을 md 파일에 가져옵니다. | 포스팅 맨 아래 원래 동영상 링크를 첨부합니다. | 깃헙에 commit 합니다. fastpages에서 만들어놓은 action이 실행된 후 조금 시간이 흐른 후 포스팅이 올라옵니다. | 원하는 대로 포스팅이 제대로 올라왔는지 확인합니다. 이과정을 여러번 commit하면서 확인해야 할 수도 있습니다. | 꼭 이 순서대로 작성하실 필요는 없으며 처음에 좀 익숙해지는데 시간이 걸립니다. . 이모지 작성가능합니다. https://getemoji.com/ 제가 주로 이용하는 사이트입니다. | . 💡 이외에 혹시 잘 모르겠는 부분은 말씀해주세요! 😀 . . Original Lecture Video : 원래 유튜브 동영상 제목을 그대로 복붙하되 |은 제외하고 작성해주세요. 저 문자가 있을 경우 표로 인식됩니다. .",
            "url": "https://cs224w-kor.github.io/blog/tag1/tag2/keyword1/2022/07/01/template-0000.html",
            "relUrl": "/tag1/tag2/keyword1/2022/07/01/template-0000.html",
            "date": " • Jul 1, 2022"
        }
        
    
  
    
        ,"post8": {
            "title": "Fastpages Notebook Blog Post",
            "content": "About . This notebook is a demonstration of some of capabilities of fastpages with notebooks. . With fastpages you can save your jupyter notebooks into the _notebooks folder at the root of your repository, and they will be automatically be converted to Jekyll compliant blog posts! . Front Matter . The first cell in your Jupyter Notebook or markdown blog post contains front matter. Front matter is metadata that can turn on/off options in your Notebook. It is formatted like this: . # &quot;My Title&quot; &gt; &quot;Awesome summary&quot; - toc:true- branch: master - badges: true - comments: true - author: Hamel Husain &amp; Jeremy Howard - categories: [fastpages, jupyter] . Setting toc: true will automatically generate a table of contents | Setting badges: true will automatically include GitHub and Google Colab links to your notebook. | Setting comments: true will enable commenting on your blog post, powered by utterances. | . The title and description need to be enclosed in double quotes only if they include special characters such as a colon. More details and options for front matter can be viewed on the front matter section of the README. . Markdown Shortcuts . A #hide comment at the top of any code cell will hide both the input and output of that cell in your blog post. . A #hide_input comment at the top of any code cell will only hide the input of that cell. . The comment #hide_input was used to hide the code that produced this. . put a #collapse-hide flag at the top of any cell if you want to hide that cell by default, but give the reader the option to show it: . import pandas as pd import altair as alt . . put a #collapse-show flag at the top of any cell if you want to show that cell by default, but give the reader the option to hide it: . cars = &#39;https://vega.github.io/vega-datasets/data/cars.json&#39; movies = &#39;https://vega.github.io/vega-datasets/data/movies.json&#39; sp500 = &#39;https://vega.github.io/vega-datasets/data/sp500.csv&#39; stocks = &#39;https://vega.github.io/vega-datasets/data/stocks.csv&#39; flights = &#39;https://vega.github.io/vega-datasets/data/flights-5k.json&#39; . . place a #collapse-output flag at the top of any cell if you want to put the output under a collapsable element that is closed by default, but give the reader the option to open it: . print(&#39;The comment #collapse-output was used to collapse the output of this cell by default but you can expand it.&#39;) . The comment #collapse-output was used to collapse the output of this cell by default but you can expand it. . . Interactive Charts With Altair . Charts made with Altair remain interactive. Example charts taken from this repo, specifically this notebook. . Example 1: DropDown . # use specific hard-wired values as the initial selected values selection = alt.selection_single( name=&#39;Select&#39;, fields=[&#39;Major_Genre&#39;, &#39;MPAA_Rating&#39;], init={&#39;Major_Genre&#39;: &#39;Drama&#39;, &#39;MPAA_Rating&#39;: &#39;R&#39;}, bind={&#39;Major_Genre&#39;: alt.binding_select(options=genres), &#39;MPAA_Rating&#39;: alt.binding_radio(options=mpaa)} ) # scatter plot, modify opacity based on selection alt.Chart(df).mark_circle().add_selection( selection ).encode( x=&#39;Rotten_Tomatoes_Rating:Q&#39;, y=&#39;IMDB_Rating:Q&#39;, tooltip=&#39;Title:N&#39;, opacity=alt.condition(selection, alt.value(0.75), alt.value(0.05)) ) . Example 2: Tooltips . alt.Chart(df).mark_circle().add_selection( alt.selection_interval(bind=&#39;scales&#39;, encodings=[&#39;x&#39;]) ).encode( alt.X(&#39;Rotten_Tomatoes_Rating&#39;, type=&#39;quantitative&#39;), alt.Y(&#39;IMDB_Rating&#39;, type=&#39;quantitative&#39;, axis=alt.Axis(minExtent=30)), # y=alt.Y(&#39;IMDB_Rating:Q&#39;, ), # use min extent to stabilize axis title placement tooltip=[&#39;Title:N&#39;, &#39;Release_Date:N&#39;, &#39;IMDB_Rating:Q&#39;, &#39;Rotten_Tomatoes_Rating:Q&#39;] ).properties( width=500, height=400 ) . Example 3: More Tooltips . label = alt.selection_single( encodings=[&#39;x&#39;], # limit selection to x-axis value on=&#39;mouseover&#39;, # select on mouseover events nearest=True, # select data point nearest the cursor empty=&#39;none&#39; # empty selection includes no data points ) # define our base line chart of stock prices base = alt.Chart().mark_line().encode( alt.X(&#39;date:T&#39;), alt.Y(&#39;price:Q&#39;, scale=alt.Scale(type=&#39;log&#39;)), alt.Color(&#39;symbol:N&#39;) ) alt.layer( base, # base line chart # add a rule mark to serve as a guide line alt.Chart().mark_rule(color=&#39;#aaa&#39;).encode( x=&#39;date:T&#39; ).transform_filter(label), # add circle marks for selected time points, hide unselected points base.mark_circle().encode( opacity=alt.condition(label, alt.value(1), alt.value(0)) ).add_selection(label), # add white stroked text to provide a legible background for labels base.mark_text(align=&#39;left&#39;, dx=5, dy=-5, stroke=&#39;white&#39;, strokeWidth=2).encode( text=&#39;price:Q&#39; ).transform_filter(label), # add text labels for stock prices base.mark_text(align=&#39;left&#39;, dx=5, dy=-5).encode( text=&#39;price:Q&#39; ).transform_filter(label), data=stocks ).properties( width=500, height=400 ) . Data Tables . You can display tables per the usual way in your blog: . df[[&#39;Title&#39;, &#39;Worldwide_Gross&#39;, &#39;Production_Budget&#39;, &#39;Distributor&#39;, &#39;MPAA_Rating&#39;, &#39;IMDB_Rating&#39;, &#39;Rotten_Tomatoes_Rating&#39;]].head() . Title Worldwide_Gross Production_Budget Distributor MPAA_Rating IMDB_Rating Rotten_Tomatoes_Rating . 0 The Land Girls | 146083.0 | 8000000.0 | Gramercy | R | 6.1 | NaN | . 1 First Love, Last Rites | 10876.0 | 300000.0 | Strand | R | 6.9 | NaN | . 2 I Married a Strange Person | 203134.0 | 250000.0 | Lionsgate | None | 6.8 | NaN | . 3 Let&#39;s Talk About Sex | 373615.0 | 300000.0 | Fine Line | None | NaN | 13.0 | . 4 Slam | 1087521.0 | 1000000.0 | Trimark | R | 3.4 | 62.0 | . Images . Local Images . You can reference local images and they will be copied and rendered on your blog automatically. You can include these with the following markdown syntax: . ![](my_icons/fastai_logo.png) . . Remote Images . Remote images can be included with the following markdown syntax: . ![](https://image.flaticon.com/icons/svg/36/36686.svg) . . Animated Gifs . Animated Gifs work, too! . ![](https://upload.wikimedia.org/wikipedia/commons/7/71/ChessPawnSpecialMoves.gif) . . Captions . You can include captions with markdown images like this: . ![](https://www.fast.ai/images/fastai_paper/show_batch.png &quot;Credit: https://www.fast.ai/2020/02/13/fastai-A-Layered-API-for-Deep-Learning/&quot;) . . Other Elements . GitHub Flavored Emojis . Typing I give this post two :+1:! will render this: . I give this post two :+1:! . Tweetcards . Typing &gt; twitter: https://twitter.com/jakevdp/status/1204765621767901185?s=20 will render this: Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 . Youtube Videos . Typing &gt; youtube: https://youtu.be/XfoYk_Z5AkI will render this: . Boxes / Callouts . Typing &gt; Warning: There will be no second warning! will render this: . Warning: There will be no second warning! . Typing &gt; Important: Pay attention! It&#39;s important. will render this: . Important: Pay attention! It&#8217;s important. . Typing &gt; Tip: This is my tip. will render this: . Tip: This is my tip. . Typing &gt; Note: Take note of this. will render this: . Note: Take note of this. . Typing &gt; Note: A doc link to [an example website: fast.ai](https://www.fast.ai/) should also work fine. will render in the docs: . Note: A doc link to an example website: fast.ai should also work fine. . Footnotes . You can have footnotes in notebooks, however the syntax is different compared to markdown documents. This guide provides more detail about this syntax, which looks like this: . For example, here is a footnote {% fn 1 %}. And another {% fn 2 %} {{ &#39;This is the footnote.&#39; | fndetail: 1 }} {{ &#39;This is the other footnote. You can even have a [link](www.github.com)!&#39; | fndetail: 2 }} . For example, here is a footnote 1. . And another 2 . 1. This is the footnote.↩ . 2. This is the other footnote. You can even have a link!↩ .",
            "url": "https://cs224w-kor.github.io/blog/jupyter/2020/02/20/test.html",
            "relUrl": "/jupyter/2020/02/20/test.html",
            "date": " • Feb 20, 2020"
        }
        
    
  
    
        ,"post9": {
            "title": "An Example Markdown Post",
            "content": "Example Markdown Post . Basic setup . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a “level 1 heading” in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here’s a footnote 1. Here’s a horizontal rule: . . Lists . Here’s a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes …and… . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote. &#8617; . |",
            "url": "https://cs224w-kor.github.io/blog/markdown/2020/01/14/test-markdown-post.html",
            "relUrl": "/markdown/2020/01/14/test-markdown-post.html",
            "date": " • Jan 14, 2020"
        }
        
    
  

  
  

  
      ,"page1": {
          "title": "About Us",
          "content": "안녕하세요. 글또 그래프 스터디 모임입니다. . Contributers: . Alphabetical order . JungYeon Lee | person2 | person3 | person4 | person5 | person6 | person7 | .",
          "url": "https://cs224w-kor.github.io/blog/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

  
  

  
      ,"page10": {
          "title": "",
          "content": "Sitemap: {{ “sitemap.xml” | absolute_url }} | .",
          "url": "https://cs224w-kor.github.io/blog/robots.txt",
          "relUrl": "/robots.txt",
          "date": ""
      }
      
  

}