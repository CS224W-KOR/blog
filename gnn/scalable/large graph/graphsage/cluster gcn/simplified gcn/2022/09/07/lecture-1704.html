<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="twitter:card" content="summary_large_image" /><!-- Begin Jekyll SEO tag v2.8.0 -->
<title>Lecture 17.4 - Scaling up by Simplifying GNNs | CS224W-KOR</title>
<meta name="generator" content="Jekyll v4.1.1" />
<meta property="og:title" content="Lecture 17.4 - Scaling up by Simplifying GNNs" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="CS224W Lecture 17. Scaling Up Graph Neural Networks to Large Graphs" />
<meta property="og:description" content="CS224W Lecture 17. Scaling Up Graph Neural Networks to Large Graphs" />
<link rel="canonical" href="https://cs224w-kor.github.io/blog/gnn/scalable/large%20graph/graphsage/cluster%20gcn/simplified%20gcn/2022/09/07/lecture-1704.html" />
<meta property="og:url" content="https://cs224w-kor.github.io/blog/gnn/scalable/large%20graph/graphsage/cluster%20gcn/simplified%20gcn/2022/09/07/lecture-1704.html" />
<meta property="og:site_name" content="CS224W-KOR" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2022-09-07T00:00:00-05:00" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Lecture 17.4 - Scaling up by Simplifying GNNs" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"BlogPosting","dateModified":"2022-09-07T00:00:00-05:00","datePublished":"2022-09-07T00:00:00-05:00","description":"CS224W Lecture 17. Scaling Up Graph Neural Networks to Large Graphs","headline":"Lecture 17.4 - Scaling up by Simplifying GNNs","mainEntityOfPage":{"@type":"WebPage","@id":"https://cs224w-kor.github.io/blog/gnn/scalable/large%20graph/graphsage/cluster%20gcn/simplified%20gcn/2022/09/07/lecture-1704.html"},"url":"https://cs224w-kor.github.io/blog/gnn/scalable/large%20graph/graphsage/cluster%20gcn/simplified%20gcn/2022/09/07/lecture-1704.html"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/blog/assets/css/style.css"><link type="application/atom+xml" rel="alternate" href="https://cs224w-kor.github.io/blog/feed.xml" title="CS224W-KOR" /><link rel="shortcut icon" type="image/x-icon" href="/blog/images/favicon.ico"><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/Primer/15.2.0/primer.css" integrity="sha512-xTz2ys4coGAOz8vuV1NcQBkgVmKhsSEtjbqyMJbBHRplFuvKIUo6xhLHpAyPt9mfR6twHJgn9OgVLuqOvjeBhg==" crossorigin="anonymous" />
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.14.0/css/all.min.css" integrity="sha512-1PKOgIY59xJ8Co8+NE6FZ+LOAZKjy+KY8iq0G4B3CyeY6wYHN3yt9PW0XpSriVlkMXe40PTKnXrLnZ9+fkDaog==" crossorigin="anonymous" />
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.12.0/katex.min.css" integrity="sha512-h7nl+xz8wgDlNM4NqKEM4F1NkIRS17M9+uJwIGwuo8vGqIl4BhuCKdxjWEINm+xyrUjNCnK5dCrhM0sj+wTIXw==" crossorigin="anonymous" />
    <script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.12.0/katex.min.js" integrity="sha512-/CMIhXiDA3m2c9kzRyd97MTb3MC6OVnx4TElQ7fkkoRghwDf6gi41gaT1PwF270W6+J60uTmwgeRpNpJdRV6sg==" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.12.0/contrib/auto-render.min.js" integrity="sha512-Do7uJAaHZm5OLrIv/yN4w0iG1dbu01kzdMNnFfu/mAqgUk6Nniv2JYHcwH+cNwjqgLcqcuBBk+JRvprLVI8azg==" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js" integrity="sha512-0doc9hKxR3PYwso42RD1p5ySZpzzuDiOwMrdCEh2WdJZCjcmFKc/wEnL+z8fBQrnHoiNWbo+3fiGkOYXBdQp4A==" crossorigin="anonymous"></script>
    <script>
    document.addEventListener("DOMContentLoaded", function() {
        renderMathInElement( document.body, {
        delimiters: [
            {left: "$$", right: "$$", display: true},
            {left: "[%", right: "%]", display: true},
            {left: "$", right: "$", display: false}
        ]}
        );
    });
    </script>


<script>
function wrap_img(fn) {
    if (document.attachEvent ? document.readyState === "complete" : document.readyState !== "loading") {
        var elements = document.querySelectorAll(".post img");
        Array.prototype.forEach.call(elements, function(el, i) {
            if (el.getAttribute("title") && (el.className != "emoji")) {
                const caption = document.createElement('figcaption');
                var node = document.createTextNode(el.getAttribute("title"));
                caption.appendChild(node);
                const wrapper = document.createElement('figure');
                wrapper.className = 'image';
                el.parentNode.insertBefore(wrapper, el);
                el.parentNode.removeChild(el);
                wrapper.appendChild(el);
                wrapper.appendChild(caption);
            }
        });
    } else { document.addEventListener('DOMContentLoaded', fn); }
}
window.onload = wrap_img;
</script>

<script>
    document.addEventListener("DOMContentLoaded", function(){
    // add link icon to anchor tags
    var elem = document.querySelectorAll(".anchor-link")
    elem.forEach(e => (e.innerHTML = '<i class="fas fa-link fa-xs"></i>'));
    });
</script>
</head>
<body><header class="site-header">

  <div class="wrapper"><a class="site-title" rel="author" href="/blog/">CS224W-KOR</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/blog/about/">Questions</a><a class="page-link" href="/blog/search/">Search</a><a class="page-link" href="/blog/categories/">Tags</a></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">Lecture 17.4 - Scaling up by Simplifying GNNs</h1><p class="page-description">CS224W Lecture 17. Scaling Up Graph Neural Networks to Large Graphs</p><p class="post-meta post-meta-title"><time class="dt-published" datetime="2022-09-07T00:00:00-05:00" itemprop="datePublished">
        Sep 7, 2022
      </time>
       • <span class="read-time" title="Estimated read time">
    
    
      5 min read
    
</span></p>

    
      <p class="category-tags"><i class="fas fa-tags category-tags-icon"></i></i> 
      
        <a class="category-tags-link" href="/blog/categories/#GNN">GNN</a>
        &nbsp;
      
        <a class="category-tags-link" href="/blog/categories/#scalable">scalable</a>
        &nbsp;
      
        <a class="category-tags-link" href="/blog/categories/#large graph">large graph</a>
        &nbsp;
      
        <a class="category-tags-link" href="/blog/categories/#GraphSAGE">GraphSAGE</a>
        &nbsp;
      
        <a class="category-tags-link" href="/blog/categories/#Cluster GCN">Cluster GCN</a>
        &nbsp;
      
        <a class="category-tags-link" href="/blog/categories/#Simplified GCN">Simplified GCN</a>
        
      
      </p>
    

    </header>

  <div class="post-content e-content" itemprop="articleBody">
    <ul id="toc" class="section-nav">
<li class="toc-entry toc-h1"><a href="#roadmap-of-simplifying-gcn">Roadmap of Simplifying GCN</a></li>
<li class="toc-entry toc-h1"><a href="#recall-gcn-mean-pool">Recall: GCN (mean-pool)</a></li>
<li class="toc-entry toc-h1"><a href="#simplifying-gcn">Simplifying GCN</a></li>
<li class="toc-entry toc-h1"><a href="#simplified-gcn-summary">Simplified GCN: Summary</a></li>
<li class="toc-entry toc-h1"><a href="#comparison-with-other-methods--potential-issue-of-simplified-gcn">Comparison with Other Methods &amp; Potential Issue of Simplified GCN</a></li>
<li class="toc-entry toc-h1"><a href="#performance-of-simplified-gcn">Performance of Simplified GCN</a></li>
<li class="toc-entry toc-h1"><a href="#graph-homophily">Graph Homophily</a></li>
<li class="toc-entry toc-h1"><a href="#summary-simplified-gcn">Summary: Simplified GCN</a></li>
</ul><hr>

<blockquote>
  <p><strong>Lecture 17. Scaling Up Graph Neural Networks to Large Graphs</strong></p>

  <ul>
    <li><a href="https://cs224w-kor.github.io/blog/web/matrix/pagerank/rank/flow%20model/flow%20equations/eigenvector/stationary%20distribution/power%20iteration/2022/07/13/lecture-0401.html">Lecture 17.1 - Scaling up Graph Neural Networks</a></li>
    <li><a href="https://cs224w-kor.github.io/blog/matrix/pagerank/spider%20trap/dead%20ends/teleports/2022/07/13/lecture-0402.html">Lecture 17.2 - GraphSAGE Neighbor Sampling</a></li>
    <li><a href="https://cs224w-kor.github.io/blog/matrix/pagerank/recommendation/proximity/random%20walks/ppr/restarts/2022/07/13/lecture-0403.html">Lecture 17.3 - Cluster GCN: Scaling up GNNs</a></li>
    <li><a href="https://cs224w-kor.github.io/blog/matrix/node%20embedding/factorization/random%20walks/2022/07/13/lecture-0404.html">Lecture 17.4 - Scaling up by Simplifying GNNs</a></li>
  </ul>

</blockquote>

<hr>

<p>이때까지 SGD training에서 mini-batch를 구성하는 방식을 바꿔가며 큰 그래프를 학습할 수 있도록 만들었던 것에 반해, <strong>지금부터는 mini-batch는 랜덤 샘플링 그대로 두고, 반대로 GNN 구조를 바꾸어 큰 그래프를 학습하는 방법에 대해서 알아보도록 하겠습니다</strong>. 어떻게 보면 큰 그래프를 학습하려고 하는 목적은 같지만 이전에 배운 방법들에 대해 orthogonal 한 방법이라고 보시면 되겠습니다.</p>

<h1 id="roadmap-of-simplifying-gcn">
<a class="anchor" href="#roadmap-of-simplifying-gcn" aria-hidden="true"><span class="octicon octicon-link"></span></a>Roadmap of Simplifying GCN</h1>

<p>먼저 대표적인 GNN 모델 중 하나인 GCN으로부터 시작해서 모델을 simplify 해 나가 보겠습니다. Wu et al.에 따르면 <strong>단순하게 GCN에서 비선형 활성 함수를 제거함으로써 모델 디자인을 굉장히 scalable하게 만들 수 있다고 합니다</strong>. 더욱 놀라운 점은, 활성 함수 제거라는 simplification을 하더라도 모델의 성능에는 큰 차이가 없다는 것인데요, 이 놀라운 발견에 대해 이제부터 차차 설명 드리도록 하겠습니다.</p>

<blockquote>
  <p>💡 <strong>GCN에서 ReLU 활성 함수를 뺌으로써 구조를 단순화하자!</strong></p>
</blockquote>

<h1 id="recall-gcn-mean-pool">
<a class="anchor" href="#recall-gcn-mean-pool" aria-hidden="true"><span class="octicon octicon-link"></span></a>Recall: GCN (mean-pool)</h1>

<p>설명에 앞서 먼저 GCN을 되짚어 보도록 하죠. 이 부분은 귀가 닳도록 이전 강의들에서 반복한 내용이니 간단하게만 언급하고 넘어갈게요. 여기서 그래프 $G=(V,E)$가 주어졌고, 엣지 집합 $E$는 각 노드에 대한 self-loop을 포함한다고 가정해보겠습니다. 이 때, GCN layer $k$에 대한 이웃 노드 aggregation과 transform 과정은 아래와 같은 벡터식으로 표현됩니다.</p>

<p><img src="https://i.imgur.com/1hzYTQe.png" alt="Imgur"></p>

<p>Layer 0에서의 초기 노드 임베딩을 $h_v^{(0)} = X_v$로 두고 총 $K$개의 GCN layer을 거치며 반복적으로 노드 임베딩을 업데이트 하면, 마지막 layer에서 최종 노드 임베딩 $z_v = h_v^{(K)}$를 얻을 수 있습니다.</p>

<p>이처럼 벡터식으로 표현된 전 과정을 더욱 간단하게 행렬식으로도 표현할 수 있는데요, 우선 반복적으로 사용할 notation 부터 정리하고 가겠습니다.</p>

<ul>
  <li>
    <table>
      <tbody>
        <tr>
          <td>Layer $k$에서의 노드 임베딩 행렬 : $H^{(k)} = [h_1^{(k)}, …, h_{</td>
          <td>V</td>
          <td>}^{(k)}]^T$</td>
        </tr>
      </tbody>
    </table>
  </li>
  <li>Self-loop를 포함한 그래프의 인접 행렬 : $A$</li>
  <li>
    <table>
      <tbody>
        <tr>
          <td>$D$를 $D_{v,v} = Deg(v) =</td>
          <td>N(v)</td>
          <td>$의 대각선 행렬로 정의한다면,</td>
        </tr>
        <tr>
          <td>$D$의 역행렬인 $D^{-1}$ 또한 $D_{v,v}^{-1} = 1/</td>
          <td>N(v)</td>
          <td>$를 만족하는 대각선 행렬로 정의 가능</td>
        </tr>
      </tbody>
    </table>
  </li>
</ul>

<p>위와 같은 notation을 통해 GCN의 ‘Mean-pooling’ 부분을 아래와 같이 행렬식으로 변환할 수 있습니다.</p>

<p><img src="https://i.imgur.com/QslXoCE.png" alt="Imgur"></p>

<p>최종적으로 GCN의 벡터식과 행렬식을 이렇게 정리해 볼 수 있겠네요. 행렬식을 통해 계산하면 모든 노드에 대해 한번에 계산이 가능하다는 장점이 있습니다.</p>

<p><img src="https://i.imgur.com/o4D9n85.png" alt="Imgur"></p>

<h1 id="simplifying-gcn">
<a class="anchor" href="#simplifying-gcn" aria-hidden="true"><span class="octicon octicon-link"></span></a>Simplifying GCN</h1>

<p><strong>자, 이제 아까 말했듯이 GCN의 수식에서 ReLU 활성 함수를 빼보도록 하겠습니다.</strong> 활성 함수를 뺀 GCN 행렬식은 아래와 같은데요, 이를 길게 전개해서 layer $K$에서의 최종 노드 임베딩을 구하는 식으로 바꿔보겠습니다.</p>

<p><img src="https://i.imgur.com/Def5Hzm.png" alt="Imgur"></p>

<p>여기서 여러 선형 변환 행렬 $W_k$의 곱 또한 선형 변환 행렬인 $W$로 쓸 수 있다는 점에 주의하십시오. 단숨에 $K$개의 GCN layer을 통과한 최종 노드 임베딩을 얻기 위해서는, 우선 0번째 layer에서의 초기 노드 임베딩 $X$에 인접 행렬을 $K$번 곱함으로써 $K$-hop 까지의 이웃 노드 정보를 aggregate 해야 합니다. 다만 simplified 된 GCN의 경우 비선형 함수가 제외되어 각 GCN layer에서 행해지는 transformation을 하나의 선형 변환으로 표현할 수 있기 때문에 마지막에 한번 $W$를 곱해주기만 하면 최종 노드 임베딩을 구할 수 있는 것이죠. 최종 행렬식을 직관적으로 이해하는 것도 어렵지 않죠?</p>

<p><strong>이처럼 GCN에서 단순히 비선형 함수를 없애는 것 만으로도 모델 구조를 아주 단순화 시킬 수 있습니다.</strong> 특히, $\tilde{A}^KX$는 학습되지 않는 부분이기 때문에 처음에 먼저 계산해 놓고 그냥 필요할 때 효율적으로 사용하면 된답니다. 간단하게 $\tilde{X} = \tilde{A}^KX$라고 하고 다시 한번 식을 쓴다면 아래와 같습니다. 즉, 최종 노드 임베딩은 그저 미리 연산된 행렬 $\tilde{X}$에 선형 변환을 하여 쉽게 계산할 수 있다는 것이죠.</p>

<p><img src="https://i.imgur.com/g4Q1rKQ.png" alt="Imgur"></p>

<blockquote>
  <p>💡 <strong>ReLU를 뺀 GCN의 행렬식 전개 결과 →</strong> $H^{(K)} = \tilde{A}^KXW^T = \tilde{X}W^T$</p>
</blockquote>

<h1 id="simplified-gcn-summary">
<a class="anchor" href="#simplified-gcn-summary" aria-hidden="true"><span class="octicon octicon-link"></span></a>Simplified GCN: Summary</h1>

<p>우리는 그대로 $M$개의 노드를 랜덤 추출하여 mini-batch를 구성하는 SGD training을 통해 학습을 진행할 것이기 때문에 전 학습 과정을 아래와 같이 표현할 수 있습니다.</p>

<ol>
  <li>
    <p><strong>Pre-processing</strong></p>

    <p>먼저 미리 계산 가능한 $\tilde{X}=\tilde{A}^KX$를 구해 놓습니다. 이는 CPU 위에서 빠르게 계산할 수 있습니다.</p>
  </li>
  <li>
    <p><strong>Mini-batch training</strong></p>
    <ul>
      <li>전체 노드 중에 $M$개의 노드를 랜덤으로 샘플링 하여 mini-batch를 구성합니다. 각 mini-batch는 ${v_1, v_2, …, v_M}$으로 표현할 수 있겠네요.</li>
      <li>각 노드에 대한 최종 노드 임베딩을 $h_{v_i}^{(K)} = W\tilde{X}_{v_i}$ 를 통해 구합니다.</li>
      <li>나머지 과정은 동일합니다. 생성된 노드 임베딩을 갖고 적절한 loss 값을 계산하여 역전파 함으로써 모델 파라미터를 업데이트 하는 것이죠.</li>
    </ul>
  </li>
</ol>

<h1 id="comparison-with-other-methods--potential-issue-of-simplified-gcn">
<a class="anchor" href="#comparison-with-other-methods--potential-issue-of-simplified-gcn" aria-hidden="true"><span class="octicon octicon-link"></span></a>Comparison with Other Methods &amp; Potential Issue of Simplified GCN</h1>

<ul>
  <li>
    <p><strong>Simplified GCN vs. Neighbor Sampling</strong></p>

    <p>Neighbor Sampling처럼 computation graph를 일일이 만들어서 mini-batch를 구성하지 않아도 되기 때문에 <strong>Simplified GCN이 학습 시간 측면에서는 더 효율적</strong>입니다.</p>
  </li>
  <li>
    <p><strong>Simplified GCN vs. Cluster-GCN</strong></p>

    <p>Cluster-GCN에서 mini-batch가 특정 커뮤니티에 국한되어 전체 그래프를 대표하지 못하여 여러 커뮤니티를 굳이 굳이 붙여서 사용해야 하는데 반해, <strong>Simplified GCN에선 그냥 랜덤으로 노드를 샘플링해서 mini-batch를 구성하면 되므로 훨씬 간편합니다.</strong> 또한, 완전히 랜덤으로 mini-batch를 만들기 때문에 배치 마다의 편차도 작아져서 <strong>학습이 보다 안정적</strong>입니다.</p>
  </li>
</ul>

<p>앞에서 배운 두 방법과 Simplified GCN을 비교해보면 Simplified GCN이 여러 측면에서 훨씬 효율적이라는 것을 볼 수 있습니다. 그렇다면 그냥 일괄적으로 Simplified GCN을 사용하여 큰 그래프를 학습하면 되는 것 아닐까요?</p>

<p>하지만, <strong>Simplified GCN은 노드 임베딩을 만드는데 있어서 비선형성을 완전히 제거해 버렸기 때문에 표현력 측면에서는 다른 두 방법에 비해 크게 떨어진다는 단점</strong>이 있습니다.</p>

<h1 id="performance-of-simplified-gcn">
<a class="anchor" href="#performance-of-simplified-gcn" aria-hidden="true"><span class="octicon octicon-link"></span></a>Performance of Simplified GCN</h1>

<p>표현력이 떨어지는 단점에도 불구하고 <strong>GCN은 semi-supervised node classification 태스크에서 오리지널 GCN에 필적할만한 우수한 성능을 보이는데요</strong>, 대체 그 이유가 무엇일까요?</p>

<p><img src="https://i.imgur.com/2FaLLsC.png" alt="Imgur"></p>

<h1 id="graph-homophily">
<a class="anchor" href="#graph-homophily" aria-hidden="true"><span class="octicon octicon-link"></span></a>Graph Homophily</h1>

<p>이전 강의에서도 다루었듯이, <strong>homophily란 유유상종을 나타내는 말</strong>입니다. <strong>즉, 그래프 상에서 엣지로 연결된 노드들은 동일한 정답 라벨을 가질 가능성이 높다고 해석할 수 있겠네요.</strong> Graph homophily는 실생활의 그래프에서도 잘 드러나는데요, 예를 들어 논문-인용 네트워크에서 서로 인용 관계 (엣지)로 연결된 논문 노드들은 서로 비슷한 연구 주제를 다룰 가능성이 높고, 소셜 네트워크에서 서로 친구 관계인 사용자 노드끼리 영화 취향이 비슷할 가능성이 높은 것처럼 말이죠.</p>

<p>반면, Simplified GCN에서 $\tilde{A}^KX$를 계산하는 과정을 다시 생각해 볼까요? 이를 위해 $K$회 반복하여 $X\leftarrow \tilde{A}X$를 연산하면 되는데, 이 과정에서 반복적으로 이웃 노드 feature의 평균을 활용하여 타겟 노드 feature를 업데이트 하기 때문에 <strong>결국 엣지로 연결된 노드 끼리 비슷한 $\tilde{A}^KX$를 갖게 됩니다.</strong></p>

<p><img src="https://i.imgur.com/3CFq7SJ.png" alt="Imgur"></p>

<p>우리의 Simplified GCN 모델은 결국 $\tilde{A}^KX$에 단순 선형 변환을 거친 최종 노드 임베딩을 활용하여 downstream task에서 예측값을 만들게 되는데, 엣지로 연결된 노드끼리는 최종 노드 임베딩이 비슷하므로 결국 비슷한 예측값을 가지게 될 것입니다. <strong>따라서 graph homophily가 강하게 드러나는 그래프의 경우에는 node classification task에서 Simplified GCN의 예측이 정답 라벨과 비슷한 양상을 띠어 좋은 성능을 나타낼 수 밖에 없죠.</strong></p>

<h1 id="summary-simplified-gcn">
<a class="anchor" href="#summary-simplified-gcn" aria-hidden="true"><span class="octicon octicon-link"></span></a>Summary: Simplified GCN</h1>

<blockquote>
  <p>💡 <strong>1.  GNN 구조 : ReLU 활성 함수를 빼서 간소화함<br>2.  Mini-batch 구성 방식: 일반적인 랜덤 샘플링으로 구성함<br>3.  학습 방식: SGD training</strong></p>
</blockquote>

<ul>
  <li>Simplified GCN은 오리지널 GCN에서 ReLU 활성 함수를 빼서 노드 임베딩 생성 과정을 단순한 pre-processing 연산으로 바꿈으로써, 효율적인 임베딩 생성을 가능하게 함 (이 과정은 CPU에서 수행할 수 있을 정도로 가볍고 효율적임!)</li>
  <li>$\tilde{A}^KX$를 미리 계산해두면, scalable하게 mini-batch를 구성하여 SGD training을 할 수 있음</li>
  <li>Graph homophily가 잘 드러나는 그래프에 대해 node classification task에서 우수한 성능을 보임</li>
</ul>

<hr>

  </div><!-- from https://github.com/utterance/utterances
<script src="https://utteranc.es/client.js"
        repo="CS224W-KOR/blog"
        issue-term="title"
        label="blogpost-comment"
        theme="github-light"
        crossorigin="anonymous"
        async>
</script>-->

<div id="disqus_thread"></div>
<script>
    /**
    *  RECOMMENDED CONFIGURATION VARIABLES: EDIT AND UNCOMMENT THE SECTION BELOW TO INSERT DYNAMIC VALUES FROM YOUR PLATFORM OR CMS.
    *  LEARN WHY DEFINING THESE VARIABLES IS IMPORTANT: https://disqus.com/admin/universalcode/#configuration-variables    */
    /*
    var disqus_config = function () {
    this.page.url = PAGE_URL;  // Replace PAGE_URL with your page's canonical URL variable
    this.page.identifier = PAGE_IDENTIFIER; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
    };
    */
    (function() { // DON'T EDIT BELOW THIS LINE
    var d = document, s = d.createElement('script');
    s.src = 'https://cs224w-kor.disqus.com/embed.js';
    s.setAttribute('data-timestamp', +new Date());
    (d.head || d.body).appendChild(s);
    })();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
<a class="u-url" href="/blog/gnn/scalable/large%20graph/graphsage/cluster%20gcn/simplified%20gcn/2022/09/07/lecture-1704.html" hidden></a>
</article>

      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/blog/"></data>

  <div class="wrapper">

    <div class="footer-col-wrapper">
      <div class="footer-col">
        <p class="feed-subscribe">
          <a href="/blog/feed.xml">
            <svg class="svg-icon orange">
              <use xlink:href="/blog/assets/minima-social-icons.svg#rss"></use>
            </svg><span>Subscribe</span>
          </a>
        </p>
      </div>
      <div class="footer-col">
        <p>CS224W 강의를 공부하는 글또 그래프 스터디 블로그</p>
      </div>
    </div>

    <div class="social-links"><ul class="social-media-list"><li><a rel="me" href="https://github.com/cs224w-kor" target="_blank" title="cs224w-kor"><svg class="svg-icon grey"><use xlink:href="/blog/assets/minima-social-icons.svg#github"></use></svg></a></li></ul>
</div>

  </div>

</footer>
</body>

</html>
