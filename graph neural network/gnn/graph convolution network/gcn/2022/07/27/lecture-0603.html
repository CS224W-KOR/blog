<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="twitter:card" content="summary_large_image" /><!-- Begin Jekyll SEO tag v2.8.0 -->
<title>Lecture 6.3 - Deep Learning for Graphs | CS224W-KOR</title>
<meta name="generator" content="Jekyll v4.1.1" />
<meta property="og:title" content="Lecture 6.3 - Deep Learning for Graphs" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="CS224W Lecture 6. Graph Neural Networks (1) GNN Model" />
<meta property="og:description" content="CS224W Lecture 6. Graph Neural Networks (1) GNN Model" />
<link rel="canonical" href="https://cs224w-kor.github.io/blog/graph%20neural%20network/gnn/graph%20convolution%20network/gcn/2022/07/27/lecture-0603.html" />
<meta property="og:url" content="https://cs224w-kor.github.io/blog/graph%20neural%20network/gnn/graph%20convolution%20network/gcn/2022/07/27/lecture-0603.html" />
<meta property="og:site_name" content="CS224W-KOR" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2022-07-27T00:00:00-05:00" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Lecture 6.3 - Deep Learning for Graphs" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"BlogPosting","dateModified":"2022-07-27T00:00:00-05:00","datePublished":"2022-07-27T00:00:00-05:00","description":"CS224W Lecture 6. Graph Neural Networks (1) GNN Model","headline":"Lecture 6.3 - Deep Learning for Graphs","mainEntityOfPage":{"@type":"WebPage","@id":"https://cs224w-kor.github.io/blog/graph%20neural%20network/gnn/graph%20convolution%20network/gcn/2022/07/27/lecture-0603.html"},"url":"https://cs224w-kor.github.io/blog/graph%20neural%20network/gnn/graph%20convolution%20network/gcn/2022/07/27/lecture-0603.html"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/blog/assets/css/style.css"><link type="application/atom+xml" rel="alternate" href="https://cs224w-kor.github.io/blog/feed.xml" title="CS224W-KOR" /><link rel="shortcut icon" type="image/x-icon" href="/blog/images/favicon.ico"><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/Primer/15.2.0/primer.css" integrity="sha512-xTz2ys4coGAOz8vuV1NcQBkgVmKhsSEtjbqyMJbBHRplFuvKIUo6xhLHpAyPt9mfR6twHJgn9OgVLuqOvjeBhg==" crossorigin="anonymous" />
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.14.0/css/all.min.css" integrity="sha512-1PKOgIY59xJ8Co8+NE6FZ+LOAZKjy+KY8iq0G4B3CyeY6wYHN3yt9PW0XpSriVlkMXe40PTKnXrLnZ9+fkDaog==" crossorigin="anonymous" />
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.12.0/katex.min.css" integrity="sha512-h7nl+xz8wgDlNM4NqKEM4F1NkIRS17M9+uJwIGwuo8vGqIl4BhuCKdxjWEINm+xyrUjNCnK5dCrhM0sj+wTIXw==" crossorigin="anonymous" />
    <script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.12.0/katex.min.js" integrity="sha512-/CMIhXiDA3m2c9kzRyd97MTb3MC6OVnx4TElQ7fkkoRghwDf6gi41gaT1PwF270W6+J60uTmwgeRpNpJdRV6sg==" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.12.0/contrib/auto-render.min.js" integrity="sha512-Do7uJAaHZm5OLrIv/yN4w0iG1dbu01kzdMNnFfu/mAqgUk6Nniv2JYHcwH+cNwjqgLcqcuBBk+JRvprLVI8azg==" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js" integrity="sha512-0doc9hKxR3PYwso42RD1p5ySZpzzuDiOwMrdCEh2WdJZCjcmFKc/wEnL+z8fBQrnHoiNWbo+3fiGkOYXBdQp4A==" crossorigin="anonymous"></script>
    <script>
    document.addEventListener("DOMContentLoaded", function() {
        renderMathInElement( document.body, {
        delimiters: [
            {left: "$$", right: "$$", display: true},
            {left: "[%", right: "%]", display: true},
            {left: "$", right: "$", display: false}
        ]}
        );
    });
    </script>


<script>
function wrap_img(fn) {
    if (document.attachEvent ? document.readyState === "complete" : document.readyState !== "loading") {
        var elements = document.querySelectorAll(".post img");
        Array.prototype.forEach.call(elements, function(el, i) {
            if (el.getAttribute("title") && (el.className != "emoji")) {
                const caption = document.createElement('figcaption');
                var node = document.createTextNode(el.getAttribute("title"));
                caption.appendChild(node);
                const wrapper = document.createElement('figure');
                wrapper.className = 'image';
                el.parentNode.insertBefore(wrapper, el);
                el.parentNode.removeChild(el);
                wrapper.appendChild(el);
                wrapper.appendChild(caption);
            }
        });
    } else { document.addEventListener('DOMContentLoaded', fn); }
}
window.onload = wrap_img;
</script>

<script>
    document.addEventListener("DOMContentLoaded", function(){
    // add link icon to anchor tags
    var elem = document.querySelectorAll(".anchor-link")
    elem.forEach(e => (e.innerHTML = '<i class="fas fa-link fa-xs"></i>'));
    });
</script>
</head>
<body><header class="site-header">

  <div class="wrapper"><a class="site-title" rel="author" href="/blog/">CS224W-KOR</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/blog/about/">Questions</a><a class="page-link" href="/blog/search/">Search</a><a class="page-link" href="/blog/categories/">Tags</a></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">Lecture 6.3 - Deep Learning for Graphs</h1><p class="page-description">CS224W Lecture 6. Graph Neural Networks (1) GNN Model</p><p class="post-meta post-meta-title"><time class="dt-published" datetime="2022-07-27T00:00:00-05:00" itemprop="datePublished">
        Jul 27, 2022
      </time>
       • <span class="read-time" title="Estimated read time">
    
    
      8 min read
    
</span></p>

    
      <p class="category-tags"><i class="fas fa-tags category-tags-icon"></i></i> 
      
        <a class="category-tags-link" href="/blog/categories/#Graph Neural Network">Graph Neural Network</a>
        &nbsp;
      
        <a class="category-tags-link" href="/blog/categories/#GNN">GNN</a>
        &nbsp;
      
        <a class="category-tags-link" href="/blog/categories/#Graph Convolution Network">Graph Convolution Network</a>
        &nbsp;
      
        <a class="category-tags-link" href="/blog/categories/#GCN">GCN</a>
        
      
      </p>
    

    </header>

  <div class="post-content e-content" itemprop="articleBody">
    <ul id="toc" class="section-nav">
<li class="toc-entry toc-h1"><a href="#setup">Setup</a></li>
<li class="toc-entry toc-h1"><a href="#naive-approach">Naive Approach</a></li>
<li class="toc-entry toc-h1"><a href="#from-images-to-graphs">From Images to Graphs</a></li>
<li class="toc-entry toc-h1"><a href="#idea-aggregate-neighbors">Idea: Aggregate Neighbors</a></li>
<li class="toc-entry toc-h1"><a href="#deep-model-many-layers">Deep Model: Many Layers</a></li>
<li class="toc-entry toc-h1"><a href="#the-math-deep-encoder">The Math: Deep Encoder</a></li>
<li class="toc-entry toc-h1"><a href="#matrix-formulation">Matrix Formulation</a></li>
<li class="toc-entry toc-h1"><a href="#how-to-train-a-gnn">How to train a GNN</a></li>
<li class="toc-entry toc-h1"><a href="#model-design-overview">Model Design: Overview</a></li>
<li class="toc-entry toc-h1"><a href="#summary">Summary</a></li>
</ul><hr>
<blockquote>
  <p><strong>Lecture 6. Graph Neural Networks (1) GNN Model</strong></p>

  <ul>
    <li><a href="https://cs224w-kor.github.io/blog/graph%20neural%20network/gnn/graph%20convolution%20network/gcn/2022/07/27/lecture-0601.html">Lecture 6.1 - Introduction to Graph Neural Networks</a></li>
    <li><a href="https://cs224w-kor.github.io/blog/graph%20neural%20network/gnn/graph%20convolution%20network/gcn/2022/07/27/lecture-0602.html">Lecture 6.2 - Basics of Deep Learning</a></li>
    <li><a href="https://cs224w-kor.github.io/blog/graph%20neural%20network/gnn/graph%20convolution%20network/gcn/2022/07/27/lecture-0603.html">Lecture 6.3 - Deep Learning for Graphs</a></li>
  </ul>

</blockquote>

<hr>

<p>이제 본격적으로 이번 강의의 메인 주제였던 딥러닝을 활용한 그래프 데이터 임베딩 방법에 대해 공부해 봅시다. 참고로 Deep Encoder은 그래프 뉴럴 네트워크(GNN)으로도 부르기 때문에 혼재해서 사용하더라도 헷갈리지 않으시길 바랍니다.</p>

<h1 id="setup">
<a class="anchor" href="#setup" aria-hidden="true"><span class="octicon octicon-link"></span></a>Setup</h1>

<p>들어가기에 앞서 반복적으로 활용할 notation에 대해 간략히 설명하고 시작하겠습니다. 앞으로 아래 기호를 쭉 사용하여 설명을 할 예정이니 잘 알아두시기 바랍니다.</p>

<ul>
  <li>$V$: 노드 집합</li>
  <li>$A$: 인접 행렬 (Adjacency matrix)</li>
  <li>$X \in \mathbb{R}^{m\times |V|}$: 노드 features</li>
  <li>$v$: 노드 집합에 포함된 한 노드,   $N(v)$: $v$의 이웃 노드</li>
</ul>

<p>그래프 구조는 엣지의 방향성 및 가중 여부에 따라 여러 종류로 분류할 수 있지만, 이해를 위해 여기서는 가장 간단한 undirected &amp; unweighted 그래프로 설명하겠습니다. 따라서 인접 행렬은 0과 1로 이루어진, 대각 방향으로 symmetric한 행렬이라고 생각하실 수 있습니다.</p>

<p>또한, 이전의 Shallow Encoder과는 달리 <strong>이제 노드 feature도 함께 고려하여 노드 임베딩을 학습</strong>한다는 점에 유의하시기 바랍니다.</p>

<ul>
  <li>사실 대부분의 그래프 데이터셋은 노드 feature을 포함하고 있지만, 만에 하나 없는 경우라면 다음과 같은 벡터/값을 노드 feature로 사용하기도 합니다.
    <ul>
      <li>노드의 one-hot 인코딩 벡터</li>
      <li>상수 벡터 [1, 1, …, 1]</li>
      <li>노드 차수(degree)</li>
    </ul>
  </li>
</ul>

<h1 id="naive-approach">
<a class="anchor" href="#naive-approach" aria-hidden="true"><span class="octicon octicon-link"></span></a>Naive Approach</h1>

<p>딥러닝 모델을 활용하여 그래프 및 노드를 임베딩 하기 위해 가장 쉽게 생각할 수 있는 방법은 단순하게 그래프의 구조적인 특성을 나타내는 인접 행렬과 노드 feature 행렬을 그냥 이어 붙여서 딥러닝 모델에 던져주는 것입니다.</p>

<p><img src="https://i.imgur.com/Htd160v.png?1" alt="Imgur"></p>

<p>위와 같은 undirected 그래프의 각 노드가 2차원의 feature를 각각 가지고 있다면, 단순히 두 행렬을 이어 붙여서 만든 7차원의 벡터를 뉴럴 네트워크에 전달하면 각 노드를 간단히 임베딩 할 수 있을 것입니다. 하지만 이러한 방법에는 다음과 같은 문제가 존재합니다.</p>

<ul>
  <li>
<strong>$O(|V|)$ 파라미터가 필요함</strong>
<br>노드 feature이 $d$차원이라고 가정하면, 각 노드가 뉴럴 네트워크에 입력되는 차원이 $|V|+d$ 이겠죠? 따라서 그래프의 노드 갯수에 비례하여 모델의 파라미터가 증가합니다.</li>
  <li>
<strong>다른 사이즈의 그래프에는 적용할 수 없음</strong>
<br>위와 같은 그래프에 대해 뉴럴 네트워크를 기껏 학습시켜 놓았는데, 100개의 노드로 구성된 새로운 그래프가 인풋으로 들어온다면, 인풋 차원이 맞지 않아 학습시킨 임베딩 모델을 활용할 수 없을 것입니다.</li>
  <li>
<strong>노드 순서가 바뀌면 동일 노드의 임베딩이 달라질 수 있음</strong>
<br>위 그래프에서 노드 순서를 A→B→C→D→E에서 B→E→A→C→D 등으로 바꾼다면, 이에 따라 인접 행렬도 바뀌게 됩니다. 이렇게 된다면 같은 A 노드를 임베딩 하기 위해 인풋으로 활용되는 7차원의 벡터가 달라지기 때문에 임베딩 결과 값도 달라질 것입니다.</li>
</ul>

<h1 id="from-images-to-graphs">
<a class="anchor" href="#from-images-to-graphs" aria-hidden="true"><span class="octicon octicon-link"></span></a>From Images to Graphs</h1>

<p>그렇다면 기존 CNN 모델에서 아이디어를 차용해보는 건 어떨까요?</p>

<p><img src="https://i.imgur.com/5WHwb3y.png?1" alt="Imgur"></p>

<p><img src="https://i.imgur.com/H798kmS.png?1" alt="Imgur"></p>

<p>이미지를 다루는 CNN은 위와 같이 고정된 사이즈의 convolution 필터를 사용하여 이미지를 주욱 훑습니다. 여기서의 <strong>convolution 필터는 붉은 원으로 표시된 타겟 픽셀의 주위 픽셀 정보를 축약하는 역할</strong>을 합니다. 사실 이미지가 특수한 형태의 그래프로 해석될 수 있음을 생각해보면, 그래프 데이터에서도 <strong>타겟 노드의 임베딩을 만들기 위해 주변 노드의 정보를 사용한다</strong>는 아이디어는 나쁘지 않아 보입니다.</p>

<p>하지만 그래프에서는 CNN에서와 같이 고정된 크기의 필터(?)를 사용할 수 없습니다. 어떤 노드는 한두개의 이웃 노드를 가지지만 또 어떤 노드는 수백수천개의 이웃 노드를 가질 수 있기 때문이죠.</p>

<p><img src="https://i.imgur.com/hj1IEmg.png?1" alt="Imgur"></p>

<p><strong>그렇다면 그래프를 임베딩 할 때 타겟 노드의 이웃 노드에서 정보를 전달받아 이를 활용하여 타겟 노드의 임베딩을 업데이트 하되, 타겟 노드마다 이웃 노드의 갯수가 다를 수 있는 점을 고려하여 각기 다른 computation graph를 갖도록 하는 것이 좋겠습니다!</strong></p>

<p>다음과 같은 아이디어를 근간으로 Graph Convolutional Network가 등장하게 되었습니다. 남은 강의에서는 이 GCN을 디테일하게 설명하고 있습니다.</p>

<h1 id="idea-aggregate-neighbors">
<a class="anchor" href="#idea-aggregate-neighbors" aria-hidden="true"><span class="octicon octicon-link"></span></a>Idea: Aggregate Neighbors</h1>

<blockquote>
  <p><strong>주요 Idea: 이웃 노드 정보를 가지고 타겟 노드 임베딩을 생성하자!</strong></p>

</blockquote>

<p><img src="https://i.imgur.com/aY7jSuv.png?1" alt="Imgur"></p>

<p>왼쪽과 같은 그래프에서 우리가 임베딩하고 싶은 타겟 노드가 노란색의 A 노드라고 생각해 봅시다. 그렇다면 A 노드의 이웃 노드, 그리고 그 이웃 노드들의 이웃 노드를 가지고 오른쪽과 같은 computation graph가 생깁니다.</p>

<p>타겟 노드 A는 직속 이웃인 노드 B, C, D로부터 메시지를 전달 받고, 모든 메시지를 합친 후 어떠한 변환을 거쳐 본인의 임베딩으로 활용합니다. 우측 그림에서 회색 박스로 표시된 뉴럴 네트워크가 바로 이러한  <strong>1) 메시지 변환, 2) 이웃 노드로부터 온 메시지를 통합</strong>하는 두 과정을 수행하게 됩니다. 이 뉴럴 네트워크 내의 모델 파라미터가 최종적인 우리의 학습 대상이 되는 것입니다.</p>

<p>여기서 또 눈여겨 보아야 할 점이 있습니다. 여지껏 우리는 노란색 A 노드를 타겟 노드로 한 computation graph만 보았는데, 그렇다면 B, C, D 등 다른 타겟 노드에 대해서도 동일한 computation graph를 가질까요?</p>

<p><img src="https://i.imgur.com/25YNtNb.png?1" alt="Imgur"></p>

<p>아닙니다. 노드마다 이웃 노드의 갯수와 종류가 다르기 때문에 당연히 <strong>노드마다 서로 다른 computation graph를 가지게 됩니다.</strong></p>

<h1 id="deep-model-many-layers">
<a class="anchor" href="#deep-model-many-layers" aria-hidden="true"><span class="octicon octicon-link"></span></a>Deep Model: Many Layers</h1>

<p><img src="https://i.imgur.com/Q3jIV12.png?1" alt="Imgur"></p>

<p><strong>Layer의 수</strong></p>

<p>Deep Encoder은 여러 layer로 구성할 수 있는데, 한 layer이 직속 이웃 노드에 대한 정보를 aggregate하는 것이기 때문에 <strong>layer을 두 개 쌓는다면</strong> 직속 이웃 노드에 대한 이웃 노드, 즉 <strong>타겟 노드로부터 2-hop 떨어진 노드의 정보까지 활용</strong>하겠다는 의미로 해석할 수 있습니다. 위 그림에서도 잘 나타나 있는데, 2개의 layer로 구성된 모델을 사용하기 때문에 타겟 노드 A로부터 2-hop 떨어진 노드 E, F의 정보도 임베딩 생성에 활용되는 것을 볼 수 있습니다.</p>

<p><strong>노드 임베딩 초기화</strong></p>

<p>또한, 보통 Layer-0에서 <strong>최초 노드 임베딩으로 노드 feature을 사용</strong>합니다. 모든 노드 임베딩은 layer을 거칠수록 이웃 노드의 정보를 변환하고 합친 후 업데이트 됩니다. 결국 모든 layer을 거치고 나면 최종 노드 임베딩이 생성되어 우리가 downstream task를 위해 사용하게 되는 것이죠.</p>

<p><strong>Aggregator 함수</strong></p>

<p>여기서 타겟 노드 A가 이웃 노드 B, C, D의 메시지를 aggregation 할 때, 노드 B, C, D의 순서와 관계 없이 aggregate된 메시지는 동일해야 합니다. 즉, 메시지를 aggregate하는 함수는 <strong>permutation-invariant</strong>한 속성을 가져야 한다는 말입니다. 일반적인 GNN은 주로 <strong>합, 평균, 맥스 풀링</strong>등의 aggregator를 활용합니다.</p>

<h1 id="the-math-deep-encoder">
<a class="anchor" href="#the-math-deep-encoder" aria-hidden="true"><span class="octicon octicon-link"></span></a>The Math: Deep Encoder</h1>

<p><img src="https://i.imgur.com/49N2oH5.png?1" alt="Imgur"></p>

<p>자, 이제 가장 기본적인 GNN 형태를 정의하고 이를 수식으로 나타내어 알고리즘의 원리를 자세히 들여다보는 시간을 갖겠습니다. 우리의 GNN은 타겟 노드의 이웃 노드 임베딩을 전달받아 이를 평균냄으로써 메시지를 aggregate 합니다. 그 후, 뉴럴 네트워크를 통해 어떠한 변환을 거치고 이를 활용하여 타겟 노드 임베딩을 업데이트 합니다. 이를 수식으로 나타내면 아래와 같습니다.<br></p>

<p><img src="https://i.imgur.com/CoK7fSk.png?2" alt="Imgur"></p>

<p>식이 처음엔 되게 복잡해 보이지만, 하나씩 뜯어보면 사실 아주 간단합니다. 식 전반에 나타나는 $h_{v}^{(l)}$ 은 $l$번째 layer에서 노드 $v$의 임베딩을 나타낸다고 보시면 됩니다.</p>

<ol>
  <li>
<strong>초록색 수식 블럭</strong> : 첫 노드 임베딩을 노드 feature로 초기화합니다.</li>
  <li>
<strong>파란색 수식 블럭</strong> : $l+1$번째 layer에서의 노드 임베딩을 만들기 위해, 먼저 타겟 노드 $v$의 이웃 노드에 대해  $l$ 번째 layer에서의 노드 임베딩 평균을 구합니다. 그 후, 이웃 노드의 평균 임베딩에 어떠한 transformation $W_{l}$ 을 가합니다.</li>
  <li>
<strong>빨간색 수식 블럭</strong> : 타겟 노드의 임베딩을 업데이트할 때 이웃 노드 뿐 아니라, 이전 layer에서 갖고 있던 자기 자신의 임베딩도 활용합니다. 같은 방법으로 $h_{v}^{(l)}$에 어떠한 transformation $B_{l}$ 을 가합니다.</li>
  <li>
<strong>노란색 수식 블럭</strong> : 최종적으로 비선형 함수를 적용해서 $l+1$번째 layer에서의 타겟 노드 임베딩을 구합니다.</li>
  <li>
<strong>보라색 수식 블럭</strong> : 노드 임베딩 업데이트 과정을 $L$번 반복합니다. 이 때 최종적으로 형성된 노드 임베딩은 본인으로부터 L-hop 떨어진 노드의 정보까지 활용하여 만든 것입니다.</li>
</ol>

<h1 id="matrix-formulation">
<a class="anchor" href="#matrix-formulation" aria-hidden="true"><span class="octicon octicon-link"></span></a>Matrix Formulation</h1>

<p>이전에 Random Walk를 행렬 형태로 표현했듯이, 벡터식으로 다뤘던 GNN도 행렬식으로 다시 표현해보겠습니다.</p>

<p>모든 노드 임베딩 벡터를 한데 모아 노드 임베딩 행렬을 만든다면, 이는 아래와 같이 표현할 수 있습니다.</p>

<p>$H^{L} = {[h_{1}^{(l)}  …  h_{|V|}^{(l)}]}^T$</p>

<p>그렇다면 이웃 노드의 임베딩을 합산하는 과정은 그래프의 인접 행렬을 사용하여 아래와 같이 간단하게 나타낼 수 있습니다.</p>

<p>$\sum_{u\in N_{v}} h_{u}^{(l)} = A_{v:}H^{(l)}$</p>

<p>만약 대각 행렬을 이렇게 정의한다면, 이 대각 행렬의 역행렬은 다음과 같기 때문에,</p>

<p>$D_{v,v} = Deg(v) = |N(v)|$ ,     $D_{v,v}^{-1} = 1/|N(v)|$</p>

<p>이를 활용하면 이웃 노드의 임베딩을 평균내는 연산을 행렬식으로 간단하게 표현할 수 있습니다.</p>

<p><img src="https://i.imgur.com/c7BISSb.png?1" alt="Imgur"></p>

<p>따라서 최종적으로 노드 임베딩 업데이트 함수를 행렬식으로 나타내면 아래와 같습니다.</p>

<p><img src="https://i.imgur.com/b0ih789.png?1" alt="Imgur"></p>

<p>벡터식으로 이해하기도 어려웠는데 왜 굳이 사서 행렬식으로 변환하냐고요? 사실 행렬식이 가지는 구현상의 이점이 있기 때문입니다. <strong>행렬식을 사용한다면 각 노드에 대한 임베딩을 따로 따로 업데이트 하지 않고 하나의 행렬로써 한번에 업데이트 할 수 있으며, 이 과정에서 $\tilde{A}$가 희소 행렬이기 때문에 보다 더 효율적인 행렬 연산이 가능하기 때문에 구현할 때는 행렬식이 더 선호됩니다.</strong></p>

<h1 id="how-to-train-a-gnn">
<a class="anchor" href="#how-to-train-a-gnn" aria-hidden="true"><span class="octicon octicon-link"></span></a>How to train a GNN</h1>

<p>오늘 강의의 마지막 부분으로 이렇게 구성한 GNN을 어떻게 학습시켜야 하는지 알아보겠습니다. 시작하기에 앞서 학습의 대상이 되는 파라미터가 무엇인지 짚고 넘어가보죠.</p>

<p><img src="https://i.imgur.com/urFvswH.png?1" alt="Imgur"></p>

<p>다음 식에서 <strong>학습되는 파라미터는 $W_{l}$와 $B_{l}$</strong> 입니다. 딸린 subscript를 봐도 알 수 있듯이, 두 파라미터 행렬은 모두 <strong>layer마다 따로 존재하며, 한 layer 내에서는 공유</strong>됩니다. 이를 간단하게 그림으로 표현하면 아래와 같습니다.</p>

<p><img src="https://i.imgur.com/sYIgPuk.png?1" alt="Imgur"></p>

<p>GNN을 학습하는 방법은 여느 딥러닝 학습과 마찬가지로 크게 <strong>지도 학습, 비지도 학습 세팅</strong>으로 나뉩니다. 이를 하나씩 살펴보도록 합시다.</p>

<ol>
  <li>
<strong>지도 학습 세팅</strong>
    <ul>
      <li>노드 label $y$가 존재하는 상황</li>
      <li>정답 노드 label $y$를 활용하여  $min_{\theta}\mathcal{L}(y,f(z_v))$를 풂, 이 때 task에 맞게 L2 loss 혹은 Cross entropy loss 등을 loss 함수를 사용함</li>
      <li>
        <p>예시) 각 노드가 safe한지 혹은 toxic한지 분류하는 node classification
 → 분류 문제이므로 cross-entropy loss를 사용하고, <strong>노드의 정답 클래스 label을 활용하여 직접 모델 학습 가능</strong></p>

        <aside>
  💡 아래 loss 식에서 $\sigma(z_v^T\theta)$는 학습된 노드 임베딩 $z_v^T$를 가지고 모델이 예측한 노드 $v$의 클래스 확률을 나타냅니다.
        
  </aside>
      </li>
    </ul>

    <p><img src="https://i.imgur.com/HQ38X60.png?2" alt="Imgur"></p>

    <p><img src="https://i.imgur.com/qQFObHg.png?1" alt="Imgur"></p>
  </li>
  <li>
<strong>비지도 학습 세팅</strong>
    <ul>
      <li>노드 label이 존재하지 않는 상황</li>
      <li>
        <p><strong>3강에서 공부했던 그래프 상 노드 similarity를 supervision으로 활용</strong></p>

        <p><img src="https://i.imgur.com/wwOybOk.png?1" alt="Imgur"></p>

        <p>임의의 supervision 시그널을 만들어 비지도 학습 세팅을 지도 학습 세팅으로 바꾸기 위해서  원본 그래프에서의 노드 similarity를 바탕으로 label을 지정해줍니다. 여기서 노드 similarity는 3강에서 다뤘던 Random Walk 등을 활용할 수 있습니다.</p>

        <p>간단하게 DeepWalk로 노드 similarity를 정의하는 경우를 생각해볼까요? 만약 두 노드 $u$와 $v$가 랜덤 워크 상에서 co-occur한다면 두 노드는 ‘similar’하다고 말할 수 있으며, $y_{u,v} = 1$로 임의의 label을 붙입니다. 또한 여기서 $DEC(z_u,z_v)$는 학습된 두 노드의 임베딩을 내적함으로써 임베딩 공간에서의 노드 similarity를 측정합니다. Loss 함수로 cross-entropy를 사용함으로써 그래프에서 노드 similarity를 최대한 잘 보존하도록 노드 임베딩을 학습할 수 있게 됩니다.</p>

        <aside>
  💡 원본 그래프에서 similar한 노드는 → similar한 임베딩을 갖도록 합니다
  </aside>

        <p><img src="https://i.imgur.com/UyyVLbR.png?1" alt="Imgur"></p>
      </li>
    </ul>
  </li>
</ol>

<h1 id="model-design-overview">
<a class="anchor" href="#model-design-overview" aria-hidden="true"><span class="octicon octicon-link"></span></a>Model Design: Overview</h1>

<p>자 그럼, 오늘 배웠던 내용을 한번 쭉 훑어 정리하고 포스트를 마무리하도록 하겠습니다. 그래프를 위한 Deep Encoder, a.k.a. GNN 모델을 만들기 위해서 아래와 같은 단계를 따라가야 합니다.</p>

<ol>
  <li><strong>이웃 노드 임베딩을 aggregate하는 함수를 정함</strong></li>
  <li><strong>Task의 특성에 맞추어 loss 함수를 정의함</strong></li>
</ol>

<p><img src="https://i.imgur.com/hK8yNIW.png?1" alt="Imgur"></p>

<p><img src="https://i.imgur.com/IVjcjvq.png?1" alt="Imgur"></p>

<ol>
  <li><strong>여러 computation graph에 대해 GNN 모델을 학습시킴</strong></li>
  <li><strong>학습된 모델을 갖고 노드에 대한 임베딩을 생성할 수 있음. 이 때, 모든 노드에 대해 뉴럴 네트워크의 파라미터가 공유되기 때문에 학습에 사용되지 않은 노드에 대해서도 임베딩을 생성할 수 있음 (Inductive Capability)</strong></li>
</ol>

<p><img src="https://i.imgur.com/9VE7ZON.png?1" alt="Imgur"></p>

<p><br></p>
<aside>
💡 Inductive Capability
<br>
1. 새로운 그래프: 예를 들어 분자 그래프에서, 화합물 A에 대해 학습된 GNN 모델이 화합물 B 그래프에서 임베딩을 만드는데 활용될 수 있음
<br>2. 새로운 노드: 시간에 따라 evolving 하는 그래프에서 새로운 노드가 추가될 때마다 바로바로 임베딩을 생성할 수 있음

</aside>

<h1 id="summary">
<a class="anchor" href="#summary" aria-hidden="true"><span class="octicon octicon-link"></span></a>Summary</h1>

<ul>
  <li>Deep Encoder (GNN)의 핵심 아이디어: 이웃 노드의 정보를 aggregate 함으로써 노드 임베딩을 생성하자!</li>
  <li>모델 내 Aggregator과 Transformation 함수를 각각 어떻게 정의하느냐에 따라 모델 구조가 달라질 수 있습니다.</li>
  <li>다음 강의에선 GNN variant의 하나인 GraphSAGE를 다룰 것입니다.</li>
</ul>

<hr>

  </div><!-- from https://github.com/utterance/utterances
<script src="https://utteranc.es/client.js"
        repo="CS224W-KOR/blog"
        issue-term="title"
        label="blogpost-comment"
        theme="github-light"
        crossorigin="anonymous"
        async>
</script>-->

<div id="disqus_thread"></div>
<script>
    /**
    *  RECOMMENDED CONFIGURATION VARIABLES: EDIT AND UNCOMMENT THE SECTION BELOW TO INSERT DYNAMIC VALUES FROM YOUR PLATFORM OR CMS.
    *  LEARN WHY DEFINING THESE VARIABLES IS IMPORTANT: https://disqus.com/admin/universalcode/#configuration-variables    */
    /*
    var disqus_config = function () {
    this.page.url = PAGE_URL;  // Replace PAGE_URL with your page's canonical URL variable
    this.page.identifier = PAGE_IDENTIFIER; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
    };
    */
    (function() { // DON'T EDIT BELOW THIS LINE
    var d = document, s = d.createElement('script');
    s.src = 'https://cs224w-kor.disqus.com/embed.js';
    s.setAttribute('data-timestamp', +new Date());
    (d.head || d.body).appendChild(s);
    })();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
<a class="u-url" href="/blog/graph%20neural%20network/gnn/graph%20convolution%20network/gcn/2022/07/27/lecture-0603.html" hidden></a>
</article>

      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/blog/"></data>

  <div class="wrapper">

    <div class="footer-col-wrapper">
      <div class="footer-col">
        <p class="feed-subscribe">
          <a href="/blog/feed.xml">
            <svg class="svg-icon orange">
              <use xlink:href="/blog/assets/minima-social-icons.svg#rss"></use>
            </svg><span>Subscribe</span>
          </a>
        </p>
      </div>
      <div class="footer-col">
        <p>CS224W 강의를 공부하는 글또 그래프 스터디 블로그</p>
      </div>
    </div>

    <div class="social-links"><ul class="social-media-list"><li><a rel="me" href="https://github.com/cs224w-kor" target="_blank" title="cs224w-kor"><svg class="svg-icon grey"><use xlink:href="/blog/assets/minima-social-icons.svg#github"></use></svg></a></li></ul>
</div>

  </div>

</footer>
</body>

</html>
