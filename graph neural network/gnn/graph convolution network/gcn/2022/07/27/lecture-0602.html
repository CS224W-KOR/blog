<!DOCTYPE html>
<html lang="en"><head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="twitter:card" content="summary_large_image" /><!-- Begin Jekyll SEO tag v2.8.0 -->
<title>Lecture 6.2 - Basics of Deep Learning | CS224W-KOR</title>
<meta name="generator" content="Jekyll v4.1.1" />
<meta property="og:title" content="Lecture 6.2 - Basics of Deep Learning" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="CS224W Lecture 6. Graph Neural Networks (1) GNN Model" />
<meta property="og:description" content="CS224W Lecture 6. Graph Neural Networks (1) GNN Model" />
<link rel="canonical" href="https://cs224w-kor.github.io/blog/graph%20neural%20network/gnn/graph%20convolution%20network/gcn/2022/07/27/lecture-0602.html" />
<meta property="og:url" content="https://cs224w-kor.github.io/blog/graph%20neural%20network/gnn/graph%20convolution%20network/gcn/2022/07/27/lecture-0602.html" />
<meta property="og:site_name" content="CS224W-KOR" />
<meta property="og:type" content="article" />
<meta property="article:published_time" content="2022-07-27T00:00:00-05:00" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Lecture 6.2 - Basics of Deep Learning" />
<script type="application/ld+json">
{"@context":"https://schema.org","@type":"BlogPosting","dateModified":"2022-07-27T00:00:00-05:00","datePublished":"2022-07-27T00:00:00-05:00","description":"CS224W Lecture 6. Graph Neural Networks (1) GNN Model","headline":"Lecture 6.2 - Basics of Deep Learning","mainEntityOfPage":{"@type":"WebPage","@id":"https://cs224w-kor.github.io/blog/graph%20neural%20network/gnn/graph%20convolution%20network/gcn/2022/07/27/lecture-0602.html"},"url":"https://cs224w-kor.github.io/blog/graph%20neural%20network/gnn/graph%20convolution%20network/gcn/2022/07/27/lecture-0602.html"}</script>
<!-- End Jekyll SEO tag -->
<link rel="stylesheet" href="/blog/assets/css/style.css"><link type="application/atom+xml" rel="alternate" href="https://cs224w-kor.github.io/blog/feed.xml" title="CS224W-KOR" /><link rel="shortcut icon" type="image/x-icon" href="/blog/images/favicon.ico"><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/Primer/15.2.0/primer.css" integrity="sha512-xTz2ys4coGAOz8vuV1NcQBkgVmKhsSEtjbqyMJbBHRplFuvKIUo6xhLHpAyPt9mfR6twHJgn9OgVLuqOvjeBhg==" crossorigin="anonymous" />
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.14.0/css/all.min.css" integrity="sha512-1PKOgIY59xJ8Co8+NE6FZ+LOAZKjy+KY8iq0G4B3CyeY6wYHN3yt9PW0XpSriVlkMXe40PTKnXrLnZ9+fkDaog==" crossorigin="anonymous" />
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.12.0/katex.min.css" integrity="sha512-h7nl+xz8wgDlNM4NqKEM4F1NkIRS17M9+uJwIGwuo8vGqIl4BhuCKdxjWEINm+xyrUjNCnK5dCrhM0sj+wTIXw==" crossorigin="anonymous" />
    <script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.12.0/katex.min.js" integrity="sha512-/CMIhXiDA3m2c9kzRyd97MTb3MC6OVnx4TElQ7fkkoRghwDf6gi41gaT1PwF270W6+J60uTmwgeRpNpJdRV6sg==" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/KaTeX/0.12.0/contrib/auto-render.min.js" integrity="sha512-Do7uJAaHZm5OLrIv/yN4w0iG1dbu01kzdMNnFfu/mAqgUk6Nniv2JYHcwH+cNwjqgLcqcuBBk+JRvprLVI8azg==" crossorigin="anonymous"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js" integrity="sha512-0doc9hKxR3PYwso42RD1p5ySZpzzuDiOwMrdCEh2WdJZCjcmFKc/wEnL+z8fBQrnHoiNWbo+3fiGkOYXBdQp4A==" crossorigin="anonymous"></script>
    <script>
    document.addEventListener("DOMContentLoaded", function() {
        renderMathInElement( document.body, {
        delimiters: [
            {left: "$$", right: "$$", display: true},
            {left: "[%", right: "%]", display: true},
            {left: "$", right: "$", display: false}
        ]}
        );
    });
    </script>


<script>
function wrap_img(fn) {
    if (document.attachEvent ? document.readyState === "complete" : document.readyState !== "loading") {
        var elements = document.querySelectorAll(".post img");
        Array.prototype.forEach.call(elements, function(el, i) {
            if (el.getAttribute("title") && (el.className != "emoji")) {
                const caption = document.createElement('figcaption');
                var node = document.createTextNode(el.getAttribute("title"));
                caption.appendChild(node);
                const wrapper = document.createElement('figure');
                wrapper.className = 'image';
                el.parentNode.insertBefore(wrapper, el);
                el.parentNode.removeChild(el);
                wrapper.appendChild(el);
                wrapper.appendChild(caption);
            }
        });
    } else { document.addEventListener('DOMContentLoaded', fn); }
}
window.onload = wrap_img;
</script>

<script>
    document.addEventListener("DOMContentLoaded", function(){
    // add link icon to anchor tags
    var elem = document.querySelectorAll(".anchor-link")
    elem.forEach(e => (e.innerHTML = '<i class="fas fa-link fa-xs"></i>'));
    });
</script>
</head>
<body><header class="site-header">

  <div class="wrapper"><a class="site-title" rel="author" href="/blog/">CS224W-KOR</a><nav class="site-nav">
        <input type="checkbox" id="nav-trigger" class="nav-trigger" />
        <label for="nav-trigger">
          <span class="menu-icon">
            <svg viewBox="0 0 18 15" width="18px" height="15px">
              <path d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.032C17.335,0,18,0.665,18,1.484L18,1.484z M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.032C17.335,6.031,18,6.696,18,7.516L18,7.516z M18,13.516C18,14.335,17.335,15,16.516,15H1.484 C0.665,15,0,14.335,0,13.516l0,0c0-0.82,0.665-1.483,1.484-1.483h15.032C17.335,12.031,18,12.695,18,13.516L18,13.516z"/>
            </svg>
          </span>
        </label>

        <div class="trigger"><a class="page-link" href="/blog/about/">Questions</a><a class="page-link" href="/blog/search/">Search</a><a class="page-link" href="/blog/categories/">Tags</a></div>
      </nav></div>
</header>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <article class="post h-entry" itemscope itemtype="http://schema.org/BlogPosting">

  <header class="post-header">
    <h1 class="post-title p-name" itemprop="name headline">Lecture 6.2 - Basics of Deep Learning</h1><p class="page-description">CS224W Lecture 6. Graph Neural Networks (1) GNN Model</p><p class="post-meta post-meta-title"><time class="dt-published" datetime="2022-07-27T00:00:00-05:00" itemprop="datePublished">
        Jul 27, 2022
      </time>
       • <span class="read-time" title="Estimated read time">
    
    
      4 min read
    
</span></p>

    
      <p class="category-tags"><i class="fas fa-tags category-tags-icon"></i></i> 
      
        <a class="category-tags-link" href="/blog/categories/#Graph Neural Network">Graph Neural Network</a>
        &nbsp;
      
        <a class="category-tags-link" href="/blog/categories/#GNN">GNN</a>
        &nbsp;
      
        <a class="category-tags-link" href="/blog/categories/#Graph Convolution Network">Graph Convolution Network</a>
        &nbsp;
      
        <a class="category-tags-link" href="/blog/categories/#GCN">GCN</a>
        
      
      </p>
    

    </header>

  <div class="post-content e-content" itemprop="articleBody">
    <ul id="toc" class="section-nav">
<li class="toc-entry toc-h1"><a href="#machine-learning-as-optimization">Machine Learning as Optimization</a></li>
<li class="toc-entry toc-h1"><a href="#gradient-descent">Gradient Descent</a></li>
<li class="toc-entry toc-h1"><a href="#stochastic-gradient-descent-sgd">Stochastic Gradient Descent (SGD)</a></li>
<li class="toc-entry toc-h1"><a href="#back-propagation">Back-propagation</a></li>
<li class="toc-entry toc-h1"><a href="#non-linearity">Non-linearity</a></li>
<li class="toc-entry toc-h1"><a href="#multi-layer-perceptron-mlp">Multi-layer Perceptron (MLP)</a></li>
<li class="toc-entry toc-h1"><a href="#summary">Summary</a></li>
</ul><hr>
<blockquote>
  <p><strong>Lecture 6. Graph Neural Networks (1) GNN Model</strong></p>

  <ul>
    <li><a href="https://cs224w-kor.github.io/blog/graph%20neural%20network/gnn/graph%20convolution%20network/gcn/2022/07/27/lecture-0601.html">Lecture 6.1 - Introduction to Graph Neural Networks</a></li>
    <li><a href="https://cs224w-kor.github.io/blog/graph%20neural%20network/gnn/graph%20convolution%20network/gcn/2022/07/27/lecture-0602.html">Lecture 6.2 - Basics of Deep Learning</a></li>
    <li><a href="https://cs224w-kor.github.io/blog/graph%20neural%20network/gnn/graph%20convolution%20network/gcn/2022/07/27/lecture-0603.html">Lecture 6.3 - Deep Learning for Graphs</a></li>
  </ul>

</blockquote>

<hr>

<p>이번 파트는 본격적으로 그래프 데이터를 위한 딥러닝을 설명하기에 앞서 딥러닝에 익숙하지 않은 사람들을 위해 <strong>딥러닝의 기본 개념</strong>을 간단하게 설명하는 부분입니다. 많은 내용을 커버하긴 하지만 딥러닝 초심자들은 CS231n과 같은 딥러닝 강의를 먼저 수강하고 오심을 추천드립니다!</p>

<h1 id="machine-learning-as-optimization">
<a class="anchor" href="#machine-learning-as-optimization" aria-hidden="true"><span class="octicon octicon-link"></span></a>Machine Learning as Optimization</h1>

<p>먼저 간단한 지도 학습 문제를 생각해 봅시다. 지도 학습(Supervised Learning)이란 데이터에 대한 ground truth label이 존재하는 경우를 일컫는데, 다시 말해 input $x$가 주어졌을 때, label $y$를 예측하는 문제라고 할 수 있습니다. 이러한 task는 아래와 같은 식을 통해 최적화 문제로 바꾸어 해결할 수 있습니다.</p>

<p><img src="https://i.imgur.com/JouDoxA.png" alt="Imgur"></p>

<p>위 식을 이해하기 위해 먼저 구성 요소에 대해 하나씩 짚어보겠습니다.</p>

<blockquote>
  <p><strong>$\theta$ : 우리가 최적화(학습) 하려는 파라미터들</strong></p>

</blockquote>

<p>최종적으로 우리가 학습하고자 하는 파라미터 값입니다. 예를 들어 앞의 Shallow Encoder에서는 학습으로 결정되는 $|V|\times d$ 사이즈의 임베딩 look-up table이 $\theta$에 해당하겠죠.</p>

<blockquote>
  <p><strong>$\mathcal{L}$ : Loss 함수</strong></p>

</blockquote>

<p>Loss 함수는 <strong>ground truth label $y$와 머신러닝 모델이 예측한 label $f(x)$의 차이</strong>를 계산하는 metric 입니다. 회귀(Regression) 문제에서 주로 사용되는 L2 loss와 분류(Classification) 문제에서 주로 사용되는 Cross entropy loss 이외에도 L1 loss, Huber loss, Hinge loss 등 다양한 loss 함수가 존재합니다. 대표적인 loss 함수인 L2 loss와 Cross entropy loss의 수식은 각각 다음과 같습니다.</p>

<p><strong>L2 loss</strong></p>

<p><img src="https://i.imgur.com/Qhw8Ggq.png?1" alt="Imgur"></p>

<p><strong>Cross entropy loss</strong></p>

<p><img src="https://i.imgur.com/klLFvXV.png?1" alt="Imgur"></p>

<p><img src="https://i.imgur.com/QxFbShn.png?1" alt="Imgur"></p>

<p>결국 우리가 원하는 것은 모델이 예측한 label이 정답 ground truth label에 가까워지는 것이기 때문에, 이 <strong>loss 함수 값이 작으면 작을수록 우리의 모델이 더 정확한 예측을 한다는 의미</strong>입니다.</p>

<p><img src="https://i.imgur.com/JouDoxA.png" alt="Imgur"></p>

<p>그럼 이제 위에서 공부한 각 구성 요소를 바탕으로 다시 이 최적화 식을 해석해 봅시다. 결국 우리가 풀고자 하는 머신러닝 문제는, <strong>정답 label $y$와 모델이 예측한 label $f(x)$의 차이를 나타내는 loss 함수를  최소화 하는 방향으로 모델 파라미터 $\theta$를 최적화하는 문제</strong>로 해석할 수 있습니다.</p>

<h1 id="gradient-descent">
<a class="anchor" href="#gradient-descent" aria-hidden="true"><span class="octicon octicon-link"></span></a>Gradient Descent</h1>

<p>지금까진 두루뭉술하게만 보였던 머신러닝 문제를 동일한 의미인 최적화 문제로 재정의했습니다. 그렇다면 이 최적화 문제를 어떻게 해결해야 할까요?</p>

<p><img src="https://i.imgur.com/OLVfqS6.png?1" alt="Imgur"></p>

<p>(출처: <a href="https://ieeexplore.ieee.org/abstract/document/9298092">https://ieeexplore.ieee.org/abstract/document/9298092</a>)</p>

<p>우리는 일반적으로 Loss 함수로 convex function(볼록 함수)를 활용합니다. 이 loss 함수의 값이 작아지는 방향으로 모델 파라미터 <strong>$\theta$</strong> 를 업데이트 하기 위해,  <strong>$\theta$에 대한 $\mathcal{L}$의 기울기를 구한 후, 기울기가 작아지는 방향으로 $\theta$를 업데이트</strong> 해줍니다.</p>

<p>(위 그림에서 Cost를 $\mathcal{L}$, Weights를 $\theta$로 보시면 됩니다!)<br></p>

<p><img src="https://i.imgur.com/Jp8GQ7W.png?1" alt="Imgur"></p>

<p>이를 다시 <strong>Gradient 벡터</strong>라는 개념으로 정리해서 이야기 해보겠습니다. Gradient 벡터란 위의 식과 같이 $\theta$에 대한 $\mathcal{L}$의 편미분, 즉 기울기 값으로 구성된 벡터로써, 가장 빠르게 $\mathcal{L}$이 증가하는 방향을 나타내는 방향 도함수 벡터입니다.</p>
<aside>
💡 Gradient is the directional derivative in the direction of largest increase

</aside>

<p><br>일단 Gradient를 구했으면 이제 할 일은 <strong>모델 파라미터 $\theta$를 gradient의 반대방향으로, 즉 $\mathcal{L}$이 작아지는 방향으로, 반복적으로 업데이트</strong> 하는 것입니다.</p>

<p><img src="https://i.imgur.com/83JjnS5.png?1" alt="Imgur"></p>

<p>위 식에서 $\eta$는 learning rate로, 한번 파라미터를 업데이트 시 얼마나 변경할 것인지 보폭을 나타내는 값입니다. 이는 학습 과정 동안 동일하게 유지할 수도 있고, 목적에 따라 계속 변하게 할 수도 있습니다. 이론적으로 모델 파라미터의 업데이트는 loss 함수의 local minimum에 도달하여 gradient가 0이 될 때까지 진행하는 것이 맞지만, 실전에서는 검증 데이터셋에서의 성능이 더 이상 증가하지 않는 기점에서 모델 파라미터 업데이트를 중단합니다.</p>

<h1 id="stochastic-gradient-descent-sgd">
<a class="anchor" href="#stochastic-gradient-descent-sgd" aria-hidden="true"><span class="octicon octicon-link"></span></a>Stochastic Gradient Descent (SGD)</h1>

<p><strong>Gradient Descent의 문제점</strong></p>

<p>Gradient descent 방법으로 최적화 문제를 푸는 것은 이론적으론 무결하지만 현실적으로는 어렵습니다. 앞서 언급했듯이, Gradient 벡터를 계산하기 위해서는 전체 데이터에 대한 loss 값을 구해야 하기 때문에 몇십억개의 데이터를 갖는 오늘날의 데이터셋에 적용하기에는 계산적으로 무리가 있습니다.</p>

<p><img src="https://i.imgur.com/tuiCaRj.png?1" alt="Imgur"></p>

<p>(출처: <a href="https://www.slideshare.net/w0ong/ss-82372826">https://www.slideshare.net/w0ong/ss-82372826</a>)</p>

<p>따라서 <strong>전체 데이터셋을 작은 미니 배치로 나누어 모델 파라미터를 업데이트하는 SGD 방법</strong>이 등장하게 되었습니다. 미니 배치 마다 gradient를 구하고 모델 파라미터를 업데이트하는 것이 전체 데이터셋을 활용한 모델 업데이트 정도를 근사할 수 있기 때문이죠.</p>

<aside>
💡 SGD is unbiased estimator of full gradient

</aside>

<p><br>오늘날의 최적화 optimizer은 SGD의 여러 발전된 형태로써, Adam, Adagrad, Adadelta, RMSprop 등이 있습니다.</p>

<h1 id="back-propagation">
<a class="anchor" href="#back-propagation" aria-hidden="true"><span class="octicon octicon-link"></span></a>Back-propagation</h1>

<p>지금까지 머신러닝 문제를 최적화 문제로 재정의하고, 최적화 문제를 풀기 위해 gradient를 활용해 모델 파라미터를 업데이트 하는 방법에 대해서 배웠습니다. 잘 따라오고 계신가요?</p>

<p>이제부터는 실질적으로 머신러닝 모델이 주어졌을 때, gradient를 계산하는 방법에 대해 알아보겠습니다. 오늘날의 머신러닝/딥러닝 모델은 아주 복잡한 형태를 가지지만, 일단 이해의 편의를 돕기 위해 가장 간단한 linear function를 예시로 설명하겠습니다.</p>

<p><strong>Case 1</strong></p>

<p><img src="https://i.imgur.com/XaAFccg.png?1" alt="Imgur"></p>

<p>첫번째로 우리의 머신러닝 모델이 단순한 선형 변환 함수인 경우를 다뤄보겠습니다.</p>

<p><img src="https://i.imgur.com/WCvz3lZ.png?1" alt="Imgur"></p>

<p>선형 변환 함수가 벡터 형태이든 행렬 형태이든 관계 없이 gradient는 이렇게 간단히 구할 수 있습니다.</p>

<p><strong>Case 2</strong></p>

<p><img src="https://i.imgur.com/fPKG6mn.png?1" alt="Imgur"></p>

<p><img src="https://i.imgur.com/iC6m3Mb.png?1" alt="Imgur"></p>

<p>두번째로 조금 더 복잡한 형태의 모델로 확장해보겠습니다. (물론 아직 엄청 단순한 형태긴 하지만..)  이 경우에는 $W_{1}$과 $W_{2}$에 대해 모두 gradient를 구해야 합니다. 여기서 우리가 고등학교 때 배운 합성함수 미분에서의 연쇄법칙이 사용됩니다.</p>

<aside>
💡 Chain Rule (연쇄법칙)

![Imgur](https://i.imgur.com/IqvyTZ1.png?1)

</aside>

<p><br></p>

<p><img src="https://i.imgur.com/rLUT8NV.png?1" alt="Imgur"></p>

<p><img src="https://i.imgur.com/QO3D2Tl.png?1" alt="Imgur"></p>

<p>연쇄법칙에 따라 gradient가 $\mathcal{L}$ → $f(x)$ → $W_{2}$ → $W_{1}$ 을 따라 차례로 거꾸로 흐르면서 계산됩니다. 이렇게 말단에서부터 앞쪽까지 gradient가 흘러오기 때문에 역전파(back-propagation)이라는 이름이 붙었다고 합니다.</p>

<h1 id="non-linearity">
<a class="anchor" href="#non-linearity" aria-hidden="true"><span class="octicon octicon-link"></span></a>Non-linearity</h1>

<p>지금까지 예시로써 다뤄본 두 케이스는 사실 모두 선형 모델로써, 비선형적인 데이터를 잘 모델링할 수 없습니다. 따라서 오늘날의 머신러닝 모델은 비선형적인 활성 함수(Activation function)를 도입함으로써 이러한 문제를 해결합니다. 대표적인 비선형 함수로는 <strong>ReLU, Sigmoid</strong> 등이 있습니다.</p>

<p><img src="https://i.imgur.com/9F5rpJS.png?1" alt="Imgur"></p>

<h1 id="multi-layer-perceptron-mlp">
<a class="anchor" href="#multi-layer-perceptron-mlp" aria-hidden="true"><span class="octicon octicon-link"></span></a>Multi-layer Perceptron (MLP)</h1>

<p><img src="https://i.imgur.com/p2cEKZr.png?1" alt="Imgur"></p>

<p><strong>MLP란 한 layer마다 선형 변환과 비선형 변환이 합쳐진 가장 기본적인 형태의 머신러닝 모델</strong>이라고 볼 수 있습니다. 위 식은 MLP 한 layer을 나타내는데, layer $l$ 의 인풋으로 들어온 $x^{(l)}$에 $W_{l}$이 곱해져  선형 변환 된 후 bias 항이 더해집니다. 최종적으로 비선형 함수를 거친 아웃풋이 layer $l+1$의 인풋으로 전달됩니다. 이를 그림으로 나타내면 다음과 같습니다.</p>

<p><img src="https://i.imgur.com/54tlQil.png?1" alt="Imgur"></p>

<h1 id="summary">
<a class="anchor" href="#summary" aria-hidden="true"><span class="octicon octicon-link"></span></a>Summary</h1>

<p>지금까지 간단하게 배운 딥러닝의 기본 개념을 정리해보고, 본격적인 오늘 강의의 주제로 넘어가겠습니다.</p>

<ul>
  <li>머신러닝 문제는 최적화 문제로 풀 수 있습니다.</li>
</ul>

<p><img src="https://i.imgur.com/rp5kAPn.png?1" alt="Imgur"></p>

<ul>
  <li>모델 $f$는 간단한 선형 함수, MLP, 또는 다른 형태의 신경망일 수 있습니다. (e.g., 추후에 다룰 GNN도 가능합니다)</li>
  <li>먼저 전체 데이터셋을 미니배치로 나누어 인풋 $x$으로 사용합니다.</li>
  <li>순전파(Forward Propagation): $x$가 주어졌을 때 loss 함수 값 $\mathcal{L}$ 구하기</li>
  <li>역전파(Backward Propagation): 연쇄법칙으로 gradient $\nabla_{\theta}\mathcal{L}$ 구하기</li>
  <li>SGD를 활용하여 반복적으로 모델 파라미터 $\theta$를 최적화합니다.</li>
</ul>

<hr>

  </div><!-- from https://github.com/utterance/utterances
<script src="https://utteranc.es/client.js"
        repo="CS224W-KOR/blog"
        issue-term="title"
        label="blogpost-comment"
        theme="github-light"
        crossorigin="anonymous"
        async>
</script>-->

<div id="disqus_thread"></div>
<script>
    /**
    *  RECOMMENDED CONFIGURATION VARIABLES: EDIT AND UNCOMMENT THE SECTION BELOW TO INSERT DYNAMIC VALUES FROM YOUR PLATFORM OR CMS.
    *  LEARN WHY DEFINING THESE VARIABLES IS IMPORTANT: https://disqus.com/admin/universalcode/#configuration-variables    */
    /*
    var disqus_config = function () {
    this.page.url = PAGE_URL;  // Replace PAGE_URL with your page's canonical URL variable
    this.page.identifier = PAGE_IDENTIFIER; // Replace PAGE_IDENTIFIER with your page's unique identifier variable
    };
    */
    (function() { // DON'T EDIT BELOW THIS LINE
    var d = document, s = d.createElement('script');
    s.src = 'https://cs224w-kor.disqus.com/embed.js';
    s.setAttribute('data-timestamp', +new Date());
    (d.head || d.body).appendChild(s);
    })();
</script>
<noscript>Please enable JavaScript to view the <a href="https://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
<a class="u-url" href="/blog/graph%20neural%20network/gnn/graph%20convolution%20network/gcn/2022/07/27/lecture-0602.html" hidden></a>
</article>

      </div>
    </main><footer class="site-footer h-card">
  <data class="u-url" href="/blog/"></data>

  <div class="wrapper">

    <div class="footer-col-wrapper">
      <div class="footer-col">
        <p class="feed-subscribe">
          <a href="/blog/feed.xml">
            <svg class="svg-icon orange">
              <use xlink:href="/blog/assets/minima-social-icons.svg#rss"></use>
            </svg><span>Subscribe</span>
          </a>
        </p>
      </div>
      <div class="footer-col">
        <p>CS224W 강의를 공부하는 글또 그래프 스터디 블로그</p>
      </div>
    </div>

    <div class="social-links"><ul class="social-media-list"><li><a rel="me" href="https://github.com/cs224w-kor" target="_blank" title="cs224w-kor"><svg class="svg-icon grey"><use xlink:href="/blog/assets/minima-social-icons.svg#github"></use></svg></a></li></ul>
</div>

  </div>

</footer>
</body>

</html>
